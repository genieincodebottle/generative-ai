{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6140902,"sourceType":"datasetVersion","datasetId":3475059},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318},{"sourceId":11384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6216}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Install required libraries**\n\n\n**huggingface_hub** : \n\n    The huggingface_hub library is a Python package by Hugging Face, designed for easy access to NLP resources like pre-trained models and datasets hosted on the Hugging Face Hub. It simplifies tasks like model access, sharing, fine-tuning, and dataset downloading.\n\n**transformers** : \n\n    The Transformers library, developed by Hugging Face, is a popular open-source Python library for natural language processing (NLP). It provides easy access to a wide range of pre-trained models for various NLP tasks, such as text classification, named entity recognition, text generation, and more. The library is built on top of PyTorch and TensorFlow, allowing users to easily implement state-of-the-art models like BERT, GPT, RoBERTa, and many others. With Transformers, users can fine-tune these pre-trained models on custom datasets, perform inference on new data, and even create their own models. It's widely used in both research and industry for tasks ranging from sentiment analysis to machine translation.\n\n**accelerate** : \n\n    The Accelerate library is a Python package developed by Hugging Face. It's designed to optimize the training and inference performance of deep learning models, particularly those built with the Transformers library. Accelerate leverages mixed precision training, distributed training, and efficient data loading techniques to speed up the training process and improve resource utilization. By using Accelerate, users can train large-scale models faster and more efficiently, making it a valuable tool for researchers and practitioners working in natural language processing (NLP) and other deep learning domains.\n\n**bitsandbytes** : \n\n    bitsandbytes enables accessible large language models via k-bit quantization for PyTorch. bitsandbytes provides three main features for dramatically reducing memory consumption for inference and training:\n\n    8-bit optimizers uses block-wise quantization to maintain 32-bit performance at a small fraction of the memory cost.\n    LLM.Int() or 8-bit quantization enables large language model inference with only half the required memory and without any performance degradation. This method is based on vector-wise quantization to quantize most features to 8-bits and separately treating outliers with 16-bit matrix multiplication.\n    QLoRA or 4-bit quantization enables large language model training with several memory-saving techniques that donâ€™t compromise performance. This method quantizes a model to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allow training.","metadata":{}},{"cell_type":"code","source":"!pip install -q -U huggingface_hub\n!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Access Huggingface token at the notebook to connect Kaggle notebook to Huggingface","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\naccess_token = UserSecretsClient().get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token=access_token)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:52:42.087171Z","iopub.execute_input":"2024-03-09T11:52:42.087877Z","iopub.status.idle":"2024-03-09T11:52:42.848429Z","shell.execute_reply.started":"2024-03-09T11:52:42.087835Z","shell.execute_reply":"2024-03-09T11:52:42.847511Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Create instance of Gemma LLM using Huggingface transformer library\n\n\n**AutoTokenizer: ** In the Hugging Face library, AutoTokenizer is a class designed to provide an easy way to instantiate tokenizers for various pre-trained models without needing to know the exact tokenizer class name or import statement for a specific model architecture. It's part of the transformers package.\n\nHere's how AutoTokenizer works:\n\n    Automatic Selection: When you instantiate AutoTokenizer with the name of a pre-trained model, it automatically selects the appropriate tokenizer for that model.\n\n    Model-Agnostic: You can use AutoTokenizer with any model architecture supported by Hugging Face, whether it's BERT, GPT, RoBERTa, etc.\n\n    Usage: You can instantiate AutoTokenizer with the model name, and then use the tokenizer as you would with any other tokenizer object.\n\n      \n**AutoModelForCausalLM:** It is a class provided by the Hugging Face Transformers library. It's designed to automatically select and instantiate a pre-trained language model for causal language modeling (LM) tasks.\n\nHere's what you need to know about AutoModelForCausalLM:\n\n    Automatic Model Selection: Similar to AutoTokenizer, AutoModelForCausalLM automatically selects the appropriate pre-trained model architecture for causal language modeling tasks based on the model name provided.\n\n    Causal Language Modeling: Causal language modeling is a type of language modeling task where the model predicts the next token in a sequence given the preceding tokens. It's commonly used for tasks like text generation.\n\n    Usage: You can instantiate AutoModelForCausalLM with the name of the pre-trained model you want to use, and then use the model for tasks such as generating text.\n    \n\n**BitsAndBytesConfig:** [BitsAndBytesConfig](https://huggingface.co/blog/4bit-transformers-bitsandbytes)","metadata":{}},{"cell_type":"markdown","source":"\n[](https://huggingface.co/blog/4bit-transformers-bitsandbytes)","metadata":{}},{"cell_type":"code","source":"%%time\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Check what type of Device enabled (GPU or CPU)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)\n# Load the model\nquantization_config = BitsAndBytesConfig(\n                                        load_in_4bit=True,\n                                        bnb_4bit_use_double_quant=True,\n                                        bnb_4bit_quant_type=\"nf4\",\n                                        bnb_4bit_compute_dtype=torch.bfloat16,)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\nmodel = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\", quantization_config=quantization_config, low_cpu_mem_usage=True)# Use the model","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:52:44.558793Z","iopub.execute_input":"2024-03-09T11:52:44.559623Z","iopub.status.idle":"2024-03-09T11:53:21.011081Z","shell.execute_reply.started":"2024-03-09T11:52:44.559591Z","shell.execute_reply":"2024-03-09T11:53:21.009890Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65d23be7885248f98eabe28d93211d3c"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 10.7 s, sys: 5.47 s, total: 16.2 s\nWall time: 36.4 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Generate basic response from LLM","metadata":{}},{"cell_type":"code","source":"%%time\ninput_text = \"What is the best thing about Kaggle?\"\n# Encode input text to PyTorch tensors\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n\noutputs = model.generate(**input_ids, do_sample=True, max_new_tokens=100, temperature=0.5)\n\nprint(tokenizer.decode(outputs[0]))","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:54:08.144930Z","iopub.execute_input":"2024-03-09T11:54:08.145995Z","iopub.status.idle":"2024-03-09T11:54:29.224343Z","shell.execute_reply.started":"2024-03-09T11:54:08.145960Z","shell.execute_reply":"2024-03-09T11:54:29.223252Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-03-09 11:54:11.768203: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-09 11:54:11.768340: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-09 11:54:11.969685: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"<bos>What is the best thing about Kaggle?\n\n**Answer:**\n\n**The best thing about Kaggle is its vast and diverse dataset of labeled and unlabeled data, covering a wide range of domains and industries.** This allows users to explore different problems, learn new skills, and build robust data analytics models.\n\nHere are some of the other key benefits of Kaggle:\n\n* **Community engagement:** Kaggle is a vibrant community of data scientists, analysts, and entrepreneurs who share knowledge, resources, and insights.\n* **Collaboration tools\nCPU times: user 14.3 s, sys: 1.2 s, total: 15.5 s\nWall time: 21.1 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"****Import the Kaggle Winning Solution CSV file","metadata":{}},{"cell_type":"markdown","source":"# Add Kaggle winning solution dataset at the notebook to generate summary of solution writeup\n\n[Kaggle Winning Solution Dataset](https://www.kaggle.com/datasets/thedrcat/kaggle-winning-solutions-methods)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv('/kaggle/input/kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv')\ndata.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:56:49.559502Z","iopub.execute_input":"2024-03-09T11:56:49.560551Z","iopub.status.idle":"2024-03-09T11:56:50.894489Z","shell.execute_reply.started":"2024-03-09T11:56:49.560514Z","shell.execute_reply":"2024-03-09T11:56:50.893298Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                                link  place  \\\n0  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n1  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n\n                              competition_name     prize   team      kind  \\\n0  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n1  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n\n                    metric  year      nm  \\\n0  PostProcessorKernelDesc  2023  406306   \n1  PostProcessorKernelDesc  2023  406306   \n\n                                             writeup  num_tokens  \\\n0  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n1  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n\n                                             methods       cleaned_methods  \n0  ['EfficientNet-B0', 'Data Augmentation', 'Norm...  Replace augmentation  \n1  ['EfficientNet-B0', 'Data Augmentation', 'Norm...    Finger tree rotate  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link</th>\n      <th>place</th>\n      <th>competition_name</th>\n      <th>prize</th>\n      <th>team</th>\n      <th>kind</th>\n      <th>metric</th>\n      <th>year</th>\n      <th>nm</th>\n      <th>writeup</th>\n      <th>num_tokens</th>\n      <th>methods</th>\n      <th>cleaned_methods</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Replace augmentation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n      <td>2</td>\n      <td>Google - Isolated Sign Language Recognition</td>\n      <td>$100,000</td>\n      <td>1,165</td>\n      <td>Research</td>\n      <td>PostProcessorKernelDesc</td>\n      <td>2023</td>\n      <td>406306</td>\n      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n      <td>2914</td>\n      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n      <td>Finger tree rotate</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Check the first row and writup field text to understand the input text","metadata":{}},{"cell_type":"code","source":"data['writeup'][1]","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:56:52.729866Z","iopub.execute_input":"2024-03-09T11:56:52.730741Z","iopub.status.idle":"2024-03-09T11:56:52.740713Z","shell.execute_reply.started":"2024-03-09T11:56:52.730701Z","shell.execute_reply":"2024-03-09T11:56:52.739525Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\\n<p>We used only competition data.</p>\\n<h2>1. Data Preprocessing</h2>\\n<h3>1.1 CNN Preprocessing</h3>\\n<ul>\\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\\n<li>During training, we applied various augmentations.</li>\\n<li>We implemented standard normalization.</li>\\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\\n<li>We interpolated the time axis to a size of 160 using \\'nearest\\' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode=\\'nearest\\')</code>.</li>\\n<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\\n</ul>\\n<h3>1.2 Transformer Preprocessing</h3>\\n<ul>\\n<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\\n<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\\n<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\\n<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\\n<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\\n</ul>\\n<p>$$<br>\\n  Motion_{future} = position_{t+1} - position_{t}<br>\\n$$<br>\\n$$<br>\\n  Motion_{history} = position_{t} - position_{t-1}<br>\\n$$</p>\\n<ul>\\n<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\\n<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\\n<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\\n</ul>\\n<h2>2. Augmentation</h2>\\n<h3>2.1 Common Augmentations</h3>\\n<blockquote>\\n  <p>These augmentations are used in both CNN training and transformer training</p>\\n</blockquote>\\n<ol>\\n<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\\n<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\\n<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\\n<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\\n</ol>\\n<h3>2.2 CNN Specific Augmentations</h3>\\n<ul>\\n<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\\n<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\\n<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\\n</ul>\\n<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \\ntime_m = torchaudio.transforms.TimeMasking()       \\n</code></pre>\\n<h3>2.3 Augmented Sample Example</h3>\\n<p>Before augmentation:</p>\\n<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\\n<p>After augmentation:</p>\\n<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\\n<h2>3. Training</h2>\\n<h3>3.1 CNN Training</h3>\\n<ul>\\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\\n<li>Onecycle scheduler with 0.1 warmup.</li>\\n<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\\n<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\\n</ul>\\n<h3>3.2 Transformer Training</h3>\\n<ul>\\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\\n<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\\n<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\\n<li>A 3-layer model was initialized with 4-layer model\\'s first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\\n</ul>\\n<h3>3.3 Hyperparameter Tuning</h3>\\n<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\\n<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\\n<ul>\\n<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\\n<li><p>Learning rate (2e-3 - 3e-3)</p></li>\\n<li><p>Drop out (0.1 - 0.25)</p></li>\\n<li><p>Num of epochs (170-185)</p></li>\\n<li><p>Loss weights powers (0.75 - 2)</p></li>\\n<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\\n<li><p>Label smoothing (0.5 - 0.7)</p></li>\\n</ul>\\n<h2>4. Submissions, Conversion and Ensemble</h2>\\n<ol>\\n<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\\n<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don\\'t know why).</p></li>\\n<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\\n</ol>\\n<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\\n<ol>\\n<li>Efficientnet-B0, fold 0</li>\\n<li>BERT, full data train</li>\\n<li>DeBERTa, full data train</li>\\n</ol>\\n<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\\n<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\\n<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\\nWe spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\\n<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\\n<p>Then we rewrote the depthwise-conv like this:</p>\\n<pre><code>     ():\\n        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\\n         i  (self.kernel_size):\\n             j  (self.kernel_size):\\n                 i ==   j == :\\n                    \\n                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\\n         self.bias   :\\n            out = out + self.bias\\n         out\\n</code></pre>\\n<p>The running time of this is 1.24 ms.</p>\\n<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\\n<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\\n<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\\n<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\\n<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Create context from the loaded dataset to summarize the writeup. \n\nHere first row is taken to create context. You can uncomment next notebook cell to create a seperate 'context' field in the pandas dataframe in case you want to run for loop to generate summary of multiple solutions writeup.\n\nTo summarize the Kaggle Winning Solution writeup, you need other details as well other  than writup like provided in the below cell so that you get meaningful summarization.\n\nAlso you can remove hyperlinks or any other unwanted texts that may not be useful for the summarization after doing pre-processing on the context.","metadata":{}},{"cell_type":"code","source":"context = \"Competition Name: \" + data['competition_name'][1] + \\\n    \",\\nYear: \" + data['year'][1].astype(str) + \\\n    \",\\nPlace: \" + data['place'][1].astype(str) + \\\n    \",\\nMethods Used: \" + data['methods'][1] + \\\n    \",\\nSolution: \" + data['writeup'][1]\n\n\nprint(context)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:58:04.868899Z","iopub.execute_input":"2024-03-09T11:58:04.869652Z","iopub.status.idle":"2024-03-09T11:58:04.877299Z","shell.execute_reply.started":"2024-03-09T11:58:04.869617Z","shell.execute_reply":"2024-03-09T11:58:04.875906Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Competition Name: Google - Isolated Sign Language Recognition,\nYear: 2023,\nPlace: 2,\nMethods Used: ['EfficientNet-B0', 'Data Augmentation', 'Normalization', 'Interpolation', 'BERT', 'DeBERTa', 'Mixup', 'Replace augmentation', 'Time and frequence masking', 'Random affine', 'Random interpolation', 'Flip pose', 'Finger tree rotate', 'Onecycle scheduler', 'Weighted CrossEntropyLoss', 'Hypercolumn', 'Ranger optimizer', 'Transformer', 'Knowledge distillation', 'Optuna', 'Label smoothing'],\nSolution: <h2>TLDR</h2>\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\n<p>We used only competition data.</p>\n<h2>1. Data Preprocessing</h2>\n<h3>1.1 CNN Preprocessing</h3>\n<ul>\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\n<li>During training, we applied various augmentations.</li>\n<li>We implemented standard normalization.</li>\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\n<li>We interpolated the time axis to a size of 160 using 'nearest' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest')</code>.</li>\n<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\n</ul>\n<h3>1.2 Transformer Preprocessing</h3>\n<ul>\n<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\n<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\n<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\n<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\n<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\n</ul>\n<p>$$<br>\n  Motion_{future} = position_{t+1} - position_{t}<br>\n$$<br>\n$$<br>\n  Motion_{history} = position_{t} - position_{t-1}<br>\n$$</p>\n<ul>\n<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\n<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\n<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\n</ul>\n<h2>2. Augmentation</h2>\n<h3>2.1 Common Augmentations</h3>\n<blockquote>\n  <p>These augmentations are used in both CNN training and transformer training</p>\n</blockquote>\n<ol>\n<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\n<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\n<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\n<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\n</ol>\n<h3>2.2 CNN Specific Augmentations</h3>\n<ul>\n<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\n<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\n<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\n</ul>\n<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \ntime_m = torchaudio.transforms.TimeMasking()       \n</code></pre>\n<h3>2.3 Augmented Sample Example</h3>\n<p>Before augmentation:</p>\n<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\n<p>After augmentation:</p>\n<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\n<h2>3. Training</h2>\n<h3>3.1 CNN Training</h3>\n<ul>\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n<li>Onecycle scheduler with 0.1 warmup.</li>\n<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\n<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\n</ul>\n<h3>3.2 Transformer Training</h3>\n<ul>\n<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\n<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\n<li>A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\n</ul>\n<h3>3.3 Hyperparameter Tuning</h3>\n<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\n<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\n<ul>\n<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\n<li><p>Learning rate (2e-3 - 3e-3)</p></li>\n<li><p>Drop out (0.1 - 0.25)</p></li>\n<li><p>Num of epochs (170-185)</p></li>\n<li><p>Loss weights powers (0.75 - 2)</p></li>\n<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\n<li><p>Label smoothing (0.5 - 0.7)</p></li>\n</ul>\n<h2>4. Submissions, Conversion and Ensemble</h2>\n<ol>\n<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\n<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don't know why).</p></li>\n<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\n</ol>\n<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\n<ol>\n<li>Efficientnet-B0, fold 0</li>\n<li>BERT, full data train</li>\n<li>DeBERTa, full data train</li>\n</ol>\n<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\n<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\n<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\nWe spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\n<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\n<p>Then we rewrote the depthwise-conv like this:</p>\n<pre><code>     ():\n        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\n         i  (self.kernel_size):\n             j  (self.kernel_size):\n                 i ==   j == :\n                    \n                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\n         self.bias   :\n            out = out + self.bias\n         out\n</code></pre>\n<p>The running time of this is 1.24 ms.</p>\n<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\n<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\n<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\n<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\n<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Uncomment below cell if you want to create a separate context field in the dataframe. It helps to work on mutliple rows easily without hardcoding row number.","metadata":{}},{"cell_type":"code","source":"#data['context'] = (\"Competition Name: \" + data['competition_name'] + \\\n#    \",\\nYear: \" + data['year'].astype(str) + \\\n#    \",\\nPlace: \" + data['place'].astype(str) + \\\n#    \",\\nMethods Used: \" + data['methods'] + \\\n#    \",\\nSolution: \" + data['writeup'])\n\n#context = data['context'][1]\n#print(context)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:58:30.757289Z","iopub.execute_input":"2024-03-09T11:58:30.757686Z","iopub.status.idle":"2024-03-09T11:58:30.762615Z","shell.execute_reply.started":"2024-03-09T11:58:30.757656Z","shell.execute_reply":"2024-03-09T11:58:30.760968Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Function to generate summary based on the chat prompt template. \n\nSince we are utilizing Gemma 2b-it, an instruction-tuned version or chat-oriented variant of a Language Model (LM), it is well-suited for conversational tasks or when provided with chat prompt templates.\n","metadata":{}},{"cell_type":"code","source":"def generate_summary():\n    prompt_template = f\"\"\"Provide summary of following context in 500 words. \n\n    Provide only useful information: \n    \n    Context: {context}\"\"\"\n\n\n    messages = [\n        {\"role\": \"user\", \"content\": prompt_template},\n    ]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    input_ids = tokenizer.encode(prompt, add_special_tokens=True, return_tensors=\"pt\").to('cuda')\n    \n    outputs = model.generate(input_ids, max_new_tokens=900)\n    \n    response = tokenizer.decode(outputs[0])\n    return response","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:58:48.379209Z","iopub.execute_input":"2024-03-09T11:58:48.379605Z","iopub.status.idle":"2024-03-09T11:58:48.386691Z","shell.execute_reply.started":"2024-03-09T11:58:48.379576Z","shell.execute_reply":"2024-03-09T11:58:48.385628Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Regex funtion to post-process the output.\n\nYou can use any other methods to post-process the output if you find that better.","metadata":{}},{"cell_type":"code","source":"import re\ndef extract_content(text):\n    # Find the index of '<start_of_turn>model'\n    index = text.find('<start_of_turn>model')\n\n    # Extract the content after '<start_of_turn>model'\n    if index != -1:\n        content_after_model = text[index + len('<start_of_turn>model'):].strip()\n    else:\n        return \"Content not found after '<start_of_turn>model'\"\n    return content_after_model","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:58:51.684795Z","iopub.execute_input":"2024-03-09T11:58:51.685511Z","iopub.status.idle":"2024-03-09T11:58:51.691348Z","shell.execute_reply.started":"2024-03-09T11:58:51.685476Z","shell.execute_reply":"2024-03-09T11:58:51.690249Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Generate Summary after post-process the model generated output","metadata":{}},{"cell_type":"code","source":"summary = generate_summary()\nfinal_summary = extract_content(summary)\nprint(final_summary)","metadata":{"execution":{"iopub.status.busy":"2024-03-09T11:59:03.374489Z","iopub.execute_input":"2024-03-09T11:59:03.375218Z","iopub.status.idle":"2024-03-09T11:59:36.500905Z","shell.execute_reply.started":"2024-03-09T11:59:03.375185Z","shell.execute_reply":"2024-03-09T11:59:36.499898Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Sure, here's a summary of the context in 500 words:\n\nThe context describes the development and training of an EfficientNet-B0 model for isolated sign language recognition. The model is trained on a single fold from 8 randomly split folds, using a combination of common and augmentation augmentations.\n\n**Data Preprocessing:**\n\n* 80 lip and 21 hand points are extracted from the image.\n* Various augmentations and normalizations are applied to the extracted features.\n* Motion features are included, including future and history motion.\n* Finger tree rotation is used to generate features for the fingers.\n\n**Training:**\n\n* EfficientNet-B0 is trained on one fold with a random split.\n* A onecycle scheduler with 0.1 warmup is used.\n* Weighted CrossEntropyLoss is used to improve the model's performance.\n* A hypercolumn is implemented for EfficientNet.\n* BERT and DeBERTa are trained on the full dataset.\n\n**Ensemble:**\n\n* An ensemble of models is created by aggregating the outputs of the individual models.\n* The ensemble weights are calculated based on the local fold 0 score.\n* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n\n**Results:**\n\n* EfficientNet-B0 achieves a leaderboard score of approximately 0.8.\n* Transformers improve the score to 0.81.\n* The final ensemble includes EfficientNet-B0, BERT, and DeBERTa.\n\n**Key Takeaways:**\n\n* The model uses a combination of common and augmentation augmentations.\n* The ensemble weights are calculated based on the local fold 0 score.\n* EfficientNet-B0 is a highly effective model for isolated sign language recognition.\n\n**Additional Notes:**\n\n* The model is trained on a single fold, which may not be representative of the entire dataset.\n* The model is rewritten in Keras to improve performance.\n* The running time of the DepthwiseConv2D operation is significantly higher than the other operations.<eos>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Beautify the output by applying Markdown display library","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown as md\nmd(final_summary)","metadata":{},"execution_count":null,"outputs":[]}]}