{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpPz7G_N6gpp"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1xZ_QyAFtg2pGv_USPAfdfHRiVu1a1cFb?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "<div align=\"center\">\n",
        "  \n",
        "  🦙 [Prompt Engineering with Llama 3.1](https://colab.research.google.com/drive/1BKhDfx45YEpMMisDnn5V_Rvlh_gHImGe?usp=sharing)&nbsp;&nbsp;\n",
        "  🤖 [Prompt Engineering with OpenAI](https://colab.research.google.com/drive/10QNvN9rfRF0HohHVzREjPbYd57t3bYMh?usp=sharing)\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxOWz9dpsYj"
      },
      "source": [
        "# 📚 Prompt Engineering (with gemini-pro)\n",
        "\n",
        "[Prompt Engineering](https://www.promptingguide.ai/introduction) is the **art** and **science** of crafting effective inputs for Large Language Models (LLMs) to produce desired outputs. It's a crucial skill in GenAI related tasks, allowing users to harness the full potential of Large Language Models (LLMs) for various tasks.\n",
        "\n",
        "------------\n",
        "\n",
        "### 📖 Fundamentals of Prompt Design\n",
        "\n",
        "  * #### ✨ Clarity\n",
        "    * Be specific and unambiguous in your instructions.\n",
        "\n",
        "  * #### 🏛️ Context\n",
        "    * Provide relevant background information.\n",
        "\n",
        "  * #### 🚧 Constraints\n",
        "    * Set boundaries for the AI's response.\n",
        "\n",
        "  * #### 🧪 Examples\n",
        "    * Include sample inputs and outputs when possible.\n",
        "\n",
        "  * #### 🗂️ Format\n",
        "    * Specify the desired structure of the response.\n",
        "\n",
        "------------\n",
        "<br>\n",
        "\n",
        "<img width=\"700\" src=\"https://raw.githubusercontent.com/genieincodebottle/generative-ai/main/images/Prompt_engineering.png\">\n",
        "\n",
        "### 📌 Important Prompt Techniques you should know\n",
        "\n",
        "  1. #### 🧠 Chain of Thought (CoT)\n",
        "    * A strategy to enhance reasoning by articulating intermediate steps.\n",
        "\n",
        "  2. #### 🚀 Zero-Shot Chain of Thought (Zero-Shot-CoT)\n",
        "    * Applying CoT without prior examples or training on similar tasks.\n",
        "\n",
        "  3. #### 🎯 Few-Shot Chain of Thought  (Few-Shot-CoT)\n",
        "    * Using a few examples to guide the reasoning process.\n",
        "\n",
        "  4. #### 🤔 ReAct (Reasoning and Acting)\n",
        "    * Combining reasoning with action to improve responses.\n",
        "\n",
        "  5. #### 🌳 Tree of Thoughts (ToT)\n",
        "    * Organizing thoughts hierarchically for better decision-making.\n",
        "\n",
        "  6. #### 🔄 Self-Consistency\n",
        "    * Ensuring responses are stable and consistent across queries.\n",
        "\n",
        "  7. #### 📄 Hypothetical Document Embeddings (HyDE)\n",
        "    * Leveraging embeddings to represent potential documents for reasoning.\n",
        "\n",
        "  8. #### 🏗️ Least-to-Most Prompting\n",
        "    * Starting with simpler prompts and gradually increasing complexity.\n",
        "\n",
        "  9. #### 🔗 Prompt Chaining\n",
        "    * Connecting multiple prompts to create a coherent narrative.\n",
        "\n",
        "  10. #### 📊 Graph Prompting\n",
        "    * Using graph structures to represent complex relationships.\n",
        "\n",
        "  11. #### 🔄 Recursive Prompting\n",
        "    * Iteratively refining prompts to enhance results.\n",
        "\n",
        "  12. #### 💡 Generated Knowledge\n",
        "    * Utilizing generated content as a basis for further reasoning.\n",
        "\n",
        "  13. #### ⚙️ Automatic Reasoning and Tool-Use (ART)\n",
        "    * Automating reasoning processes and tool interactions.\n",
        "\n",
        "  14. #### 🛠️ Automatic Prompt Engineer (APE)\n",
        "    * Tools to automatically generate and refine prompts.\n",
        "\n",
        "  15. #### ✨ Additional Prompt Techniques\n",
        "    * **Reflexion**: Reflecting on past responses to improve future prompts.\n",
        "    \n",
        "    * **Prompt Ensembling**: Combining multiple prompts for enhanced results.\n",
        "    \n",
        "    * **Directional Stimulus Prompting**: Guiding responses with targeted prompts.\n",
        "\n",
        "------------\n",
        "<br>\n",
        "\n",
        "### 📈 Prompt Optimization Techniques\n",
        "\n",
        "1. **Iterative refinement:** Start with a basic prompt and gradually improve it based on the results.\n",
        "\n",
        "2. [**A/B testing:**](https://www.oracle.com/in/cx/marketing/what-is-ab-testing/#:~:text=A%2FB%20testing%3F-,A%2FB%20testing%20definition,based%20on%20your%20key%20metrics.) Compare different versions of a prompt to see which performs better.\n",
        "\n",
        "3. **Prompt libraries:** Create and maintain a collection of effective prompts for reuse.\n",
        "\n",
        "4. **Collaborative prompting:** Combine insights from multiple team members to create stronger prompts.\n",
        "------------\n",
        "<br>\n",
        "\n",
        "### ⚖️ Ethical Considerations in Prompt Engineering\n",
        "\n",
        "1. **Bias mitigation:** Be aware of and actively work to reduce biases in prompts and outputs.\n",
        "\n",
        "2. **Content safety:** Implement safeguards against generating harmful or inappropriate content.\n",
        "\n",
        "4. **Data privacy:** Avoid including sensitive information in prompts.\n",
        "------------\n",
        "<br>\n",
        "\n",
        "### ✅ Evaluating Prompt Effectiveness\n",
        "\n",
        "1. **Relevance:** Does the output address the intended task or question?\n",
        "\n",
        "2. **Accuracy:** Is the information provided correct and up-to-date?\n",
        "\n",
        "3. **Coherence:** Is the response well-structured and logical?\n",
        "\n",
        "4. **Creativity:** For open-ended tasks, does the output demonstrate originality?\n",
        "\n",
        "5. **Efficiency:** Does the prompt produce the desired result with minimal back-and-forth?\n",
        "\n",
        "------------\n",
        "<br>\n",
        "\n",
        "### ⚙️ Setup\n",
        "\n",
        "* **[LLM](https://deepmind.google/technologies/gemini/pro/):** Google's free gemini-pro api endpoint ([Google's API Key](https://console.cloud.google.com/apis/credentials))\n",
        "\n",
        "* **[LLM Framework](https://python.langchain.com/v0.2/docs/introduction/):** LangChain\n",
        "------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      },
      "source": [
        "# 📦 Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ShxTNxM5gqtr",
        "outputId": "49aa6bcc-954d-4c61-dd98-fb20a67a1261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-community==0.2.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      },
      "source": [
        "# 📥 Import related libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "import json\n",
        "from typing import List, Dict, Union\n",
        "\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6UeDlrgqI2A"
      },
      "source": [
        "# 🔑 Provide Google API Key\n",
        "\n",
        "It can be used both for Gemini Pro LLM  & Google Embedding Model. You can create Google API key using following link\n",
        "\n",
        "1. [Google Gemini-Pro API Key](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "2. [YouTube Video explaining Google API Key](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "156a2923-7fb3-4d81-e087-8a9bf209ab6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAL52x3u4o8_"
      },
      "source": [
        "# 🤖 Create instance of Google's free gemini-pro\n",
        "\n",
        "Using Langchain's **\"langchain-google-genai\"** library\n",
        "\n",
        "## 🔬 For Gemini Pro, we can experiment with following LLM Parameters\n",
        "\n",
        "* `temperature`\n",
        "\n",
        "  This parameter controls the randomness of the model's output. A lower temperature (closer to 0) makes the output more deterministic and focused, while a higher temperature (closer to 1) makes it more random and creative.\n",
        "  * **Range:** Usually 0 to 1\n",
        "  * **Default:** Often around 0.7, but this can vary\n",
        "  * **Use cases:** Lower for factual or precise tasks, higher for creative tasks\n",
        "\n",
        "* `max_output_tokens`\n",
        "\n",
        "  This sets the maximum length of the generated text in tokens. A token is generally a word or a part of a word.\n",
        "  * **Range:** Model-dependent, but often up to several thousand\n",
        "  * **Default:** Varies, but often around 1024 or 2048\n",
        "  * **Use cases:** Adjust based on the desired length of the response\n",
        "\n",
        "* `top_p (nucleus sampling)`\n",
        "  \n",
        "  This parameter sets a probability threshold for token selection. The model will only consider tokens whose cumulative probability exceeds this threshold.\n",
        "  * **Range:** 0 to 1\n",
        "  * **Default:** Often around 0.9\n",
        "  * **Use cases:** Lower values make output more focused, higher values allow for more diversity\n",
        "\n",
        "* `top_k`\n",
        "  \n",
        "  This parameter limits the number of highest probability tokens to consider at each step of generation.\n",
        "  * **Range:** Positive integers, often up to 100 or more\n",
        "  * **Default:** Often around 40\n",
        "  * **Use cases:** Lower values for more predictable output, higher for more variety\n",
        "--------------------\n",
        "<br>\n",
        "\n",
        "## 📋 General best practices\n",
        "\n",
        "* Experiment with different combinations to find what works best for your specific use case.\n",
        "\n",
        "* Avoid adjusting all parameters at once; change one at a time to understand its impact.\n",
        "\n",
        "* For factual or critical tasks, prioritize lower temperature or `top_p` values.\n",
        "\n",
        "* For creative tasks, higher `temperature` or `top_p` values can be beneficial.\n",
        "Consider the trade-off between creativity and accuracy when adjusting these parameters.\n",
        "\n",
        "* Document successful parameter combinations for different types of tasks.\n",
        "Regularly review and update your parameter choices as model capabilities evolve.\n",
        "\n",
        "--------------------\n",
        "<br>\n",
        "\n",
        "## 🛡️ Safety Settings\n",
        "\n",
        "* Gemini Pro likely categorizes potential harms into these four main categories:\n",
        "\n",
        "  * Hate Speech\n",
        "\n",
        "  * Dangerous Content\n",
        "\n",
        "  * Sexually Explicit Content\n",
        "\n",
        "  * Harassment\n",
        "\n",
        "* **HarmBlockThreshold:**\n",
        "  This is probably an enum in Gemini Pro's API that defines different levels of content filtering. While I don't know the exact levels, they might include:\n",
        "\n",
        "  * BLOCK_NONE: No filtering\n",
        "\n",
        "  * BLOCK_LOW: Minimal filtering\n",
        "\n",
        "  * BLOCK_MEDIUM: Moderate filtering\n",
        "\n",
        "  * BLOCK_HIGH: Strict filtering\n",
        "\n",
        "* **Implementation:**\n",
        "\n",
        "  * When set to BLOCK_NONE for all categories, Gemini Pro likely bypasses its content filtering mechanisms for these specific harm types.\n",
        "\n",
        "  * This doesn't mean the model will necessarily produce harmful content, but rather that it won't actively filter or block content in these categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ye3dDZF0IJzP"
      },
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-pro\",\n",
        "    temperature=0.1,  # Experiment with temperature setting\n",
        "    # top_k=40, # Experiment with top_k setting\n",
        "    # top_p=0.95, # Experiment with top_p setting\n",
        "    safety_settings={\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve-OpUwT7oQD"
      },
      "source": [
        "# ❓What is LangChain Chat Prompt Template ?\n",
        "\n",
        "* Here’s a Python code example using LangChain to create a chat prompt template, which we will utilize across all prompt technique implementations.\n",
        "\n",
        "* For Gemini Pro, we need to make slight adjustments since SystemMessage is not supported in Gemini Pro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjwoJ_pR7nD1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "\n",
        "# Create a chat prompt template using predefined messages\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(\n",
        "            content=\"You are a helpful assistant that explains problems step-by-step.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"Solve this problem step by step: {problem}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Note: The Google's Gemini Pro model does not support SystemMessage,\n",
        "# which can result in an error:\n",
        "# ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 Developer instruction is not enabled for models/gemini-pro\n",
        "\n",
        "# To ensure compatibility with the Gemini Pro model, we replace the SystemMessage\n",
        "# with an additional HumanMessage. This allows the model to function correctly\n",
        "# without throwing an error.\n",
        "\n",
        "# Fixed code to ensure compatibility with Gemini Pro\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are a helpful assistant that explains problems step-by-step.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"Solve this problem step by step: {problem}\"),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C48uwYpp47IM"
      },
      "source": [
        "# 📌 Prompt Techniques with Code\n",
        "\n",
        "Let's dive deep... :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQUYayVAM3CZ"
      },
      "source": [
        "# 🧠 Chain of Thought (CoT)\n",
        "\n",
        "Chain of Thought is a method that encourages language models to show their reasoning process step-by-step.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Prompt the model to break down complex reasoning into explicit intermediate steps.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Provide a question or problem\n",
        "  * Ask the model to think through the solution step-by-step\n",
        "  * Generate a final answer based on the reasoning chain\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves accuracy on complex tasks\n",
        "  * Enhances transparency of the model's decision-making\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Mathematical problem-solving\n",
        "  * Logical reasoning\n",
        "  * Multi-step analysis tasks\n",
        "  \n",
        "* 🚀 **Implementation:**\n",
        "  * Use prompts like \"Let's approach this step-by-step:\" or \"Think through this carefully:\"\n",
        "  * Can include examples of step-by-step reasoning (few-shot approach)\n",
        "  \n",
        "* 🔄 **Variations:**\n",
        "  * Zero-shot CoT: No examples provided\n",
        "  * Few-shot CoT: Includes examples of desired reasoning\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Ensuring coherence across reasoning steps\n",
        "  * Balancing detail with conciseness\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Problem: [Task description]\n",
        "  * Let's solve this step-by-step:\n",
        "\n",
        "    [First reasoning step]\n",
        "    [Second reasoning step]\n",
        "    ...\n",
        "    Therefore, the answer is [final conclusion]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "V_LFCcf7IJ19",
        "outputId": "79230b74-70a8-46cb-86c4-c3ec3ce59903"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Chain of Thought Response:**\n",
              "**Step 1: Understand the problem**\n",
              "\n",
              "Read the problem carefully and make sure you understand what it is asking you to do. If there are any unfamiliar terms, look them up or ask for help.\n",
              "\n",
              "**Step 2: Plan your solution**\n",
              "\n",
              "Once you understand the problem, start to think about how you are going to solve it. What steps do you need to take? What information do you need?\n",
              "\n",
              "**Step 3: Solve the problem**\n",
              "\n",
              "Follow the steps you planned in step 2 to solve the problem. Show all your work so that you can check your answer later.\n",
              "\n",
              "**Step 4: Check your answer**\n",
              "\n",
              "Once you have solved the problem, check your answer to make sure it is correct. You can do this by plugging your answer back into the original problem or by using a different method to solve the problem.\n",
              "\n",
              "**Step 5: Evaluate your solution**\n",
              "\n",
              "Once you have checked your answer, take a moment to evaluate your solution. Is there a better way to solve the problem? Could you have used a different method?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are a helpful assistant that explains problems step-by-step.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"Solve this problem step by step: {problem}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def cot_response(problem):\n",
        "    messages = cot_prompt.format_prompt(problem=problem).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "problem = \"If a train travels 120 km in 2 hours, what is its average speed in km/h?\"\n",
        "response = cot_response(problem)\n",
        "display(Markdown(f\"**Chain of Thought Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jV8Y-KdSqVA"
      },
      "source": [
        "# 🚀 Zero-Shot Chain of Thought (Zero-Shot-CoT)\n",
        "\n",
        "Zero-Shot-CoT is a variation of Chain of Thought prompting that doesn't rely on examples.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Encourages step-by-step reasoning without providing sample solutions.\n",
        "\n",
        "* 📝 **Prompt structure:** Typically includes phrases like \"Let's approach this step-by-step\" or \"Let's think about this logically.\"\n",
        "\n",
        "* 🌟 **Advantage:** Flexibility across various tasks without task-specific examples.\n",
        "\n",
        "* ⚖️ **Challenge:** Relies heavily on the model's inherent reasoning capabilities.\n",
        "\n",
        "* 💼 **Applications:** Problem-solving, analysis, and decision-making across diverse domains.\n",
        "\n",
        "* 🚀 **Implementation:** Often uses a two-stage process - reasoning generation followed by answer extraction.\n",
        "\n",
        "* 📝 **Example prompt:** \"Let's solve this problem step-by-step: [insert problem]\"\n",
        "\n",
        "* 📈 **Effectiveness:** Can significantly improve performance on complex tasks compared to direct questioning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2pSXe7DcIJ4b",
        "outputId": "2b4f9bd7-98cc-4488-91d9-4f8fe1417575"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Zero-Shot Chain of Thought Response:**\n",
              "**Step 1: Understand the problem**\n",
              "\n",
              "* Read the problem carefully and make sure you understand what it is asking for.\n",
              "* Identify the given information and what you need to find.\n",
              "\n",
              "**Step 2: Plan your approach**\n",
              "\n",
              "* Consider different ways to solve the problem.\n",
              "* Choose a method that you are comfortable with and that is likely to be successful.\n",
              "\n",
              "**Step 3: Execute your plan**\n",
              "\n",
              "* Carry out the steps of your chosen method.\n",
              "* Show your work and explain your reasoning at each stage.\n",
              "\n",
              "**Step 4: Check your solution**\n",
              "\n",
              "* Make sure your solution makes sense and answers the question.\n",
              "* Check for errors by going back over your steps or using a different method."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "zero_shot_cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"Approach problems step-by-step, explaining your reasoning at each stage.\"\n",
        "        ),\n",
        "        HumanMessage(content=\"Q: {question}\\nA: Let's approach this step-by-step:\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def zero_shot_cot_response(question):\n",
        "    messages = zero_shot_cot_prompt.format_prompt(question=question).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "question = \"A store has 100 apples. If 20% of the apples are rotten, how many good apples are left?\"\n",
        "response = zero_shot_cot_response(question)\n",
        "display(Markdown(f\"**Zero-Shot Chain of Thought Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEmvcu_STSwD"
      },
      "source": [
        "# 🎯 Few-Shot Chain of Thought (Few-Shot-CoT)\n",
        "\n",
        "Few-Shot-CoT is a prompting technique that provides examples of step-by-step reasoning before asking the model to solve a new problem.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Uses 1-5 examples of reasoning chains to guide the model's approach to new problems.\n",
        "\n",
        "* 📝 **Structure:** Includes example problems, their step-by-step solutions, and then a new problem to solve.\n",
        "\n",
        "* 🌟 **Advantage:** Improves performance by demonstrating the desired reasoning process.\n",
        "\n",
        "* 💼 **Applications:** Complex problem-solving, mathematical reasoning, logical deductions.\n",
        "\n",
        "* 🚀 **Implementation:** Carefully select diverse, relevant examples that showcase the desired reasoning style.\n",
        "\n",
        "* ⚖️ **Challenges:** Choosing appropriate examples and avoiding biasing the model.\n",
        "\n",
        "* 📝 **Example:**\n",
        "  * [Example problem 1]\n",
        "    Step 1: [Reasoning]\n",
        "    Step 2: [Reasoning]\n",
        "  * Answer: [Solution]\n",
        "    Now, solve this new problem using the same approach: [New problem]\n",
        "\n",
        "* 📈 **Effectiveness:** Often outperforms zero-shot techniques, especially on complex tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "uL1a363BIJ7F",
        "outputId": "2fc715b4-0502-493e-f810-dae78f3d2c6f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Few-Shot Chain of Thought Response:**\n",
              "**Q: A train leaves Chicago at 10:00 AM and travels at a speed of 70 miles per hour. Another train leaves St. Louis at 11:00 AM and travels at a speed of 80 miles per hour. If the distance between Chicago and St. Louis is 300 miles, at what time will the two trains meet?**\n",
              "\n",
              "**A: Let's solve it step-by-step:**\n",
              "\n",
              "**1) Calculate the distance traveled by the first train:**\n",
              "- Time traveled = 11:00 AM - 10:00 AM = 1 hour\n",
              "- Distance traveled = Speed x Time = 70 mph x 1 hour = 70 miles\n",
              "\n",
              "**2) Calculate the distance remaining for the first train:**\n",
              "- Distance remaining = Total distance - Distance traveled = 300 miles - 70 miles = 230 miles\n",
              "\n",
              "**3) Calculate the time it will take the first train to travel the remaining distance:**\n",
              "- Time = Distance / Speed = 230 miles / 70 mph = 3.28 hours\n",
              "\n",
              "**4) Convert the time to minutes:**\n",
              "- Time = 3.28 hours x 60 minutes/hour = 196.8 minutes\n",
              "\n",
              "**5) Calculate the meeting time:**\n",
              "- Meeting time = Departure time of second train + Time taken by first train to travel remaining distance\n",
              "- Meeting time = 11:00 AM + 196.8 minutes\n",
              "- Meeting time = 1:16 PM\n",
              "\n",
              "**Therefore, the two trains will meet at 1:16 PM.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "few_shot_cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(content=\"You are an expert at solving problems step-by-step.\"),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Here are some examples of solving problems step-by-step:\n",
        "\n",
        "    Q: What is 17 x 23?\n",
        "    A: Let's break it down:\n",
        "    1) First, let's multiply 17 by 20: 17 x 20 = 340\n",
        "    2) Now, let's multiply 17 by 3: 17 x 3 = 51\n",
        "    3) Finally, we add these results: 340 + 51 = 391\n",
        "    Therefore, 17 x 23 = 391\n",
        "\n",
        "    Q: How many seconds are in a day?\n",
        "    A: Let's calculate step-by-step:\n",
        "    1) There are 24 hours in a day\n",
        "    2) Each hour has 60 minutes\n",
        "    3) Each minute has 60 seconds\n",
        "    4) So, we calculate: 24 x 60 x 60\n",
        "    5) 24 x 60 = 1,440\n",
        "    6) 1,440 x 60 = 86,400\n",
        "    Therefore, there are 86,400 seconds in a day.\n",
        "\n",
        "    Now, solve this problem step-by-step:\n",
        "    Q: {question}\n",
        "    A: Let's break it down:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def few_shot_cot_response(question):\n",
        "    messages = few_shot_cot_prompt.format_prompt(question=question).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "question = \"What is the area of a circle with radius 5cm?\"\n",
        "response = few_shot_cot_response(question)\n",
        "display(Markdown(f\"**Few-Shot Chain of Thought Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhd6nHXzUVXj"
      },
      "source": [
        "# 🤔 ReACT (Reasoning and Acting)\n",
        "\n",
        "ReAct is an advanced prompting method that combines reasoning and acting in language models.\n",
        "\n",
        "## Key points\n",
        "\n",
        "* 🔑 **Core concept:** Interleaves thought generation with action execution.\n",
        "\n",
        "* 🛠️ **Components:** Thought, Action, Observation cycle.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * **Thought:** Model reasons about the current state\n",
        "  * **Action:** Decides on and executes an action\n",
        "  * **Observation:** Receives feedback from the environment\n",
        "\n",
        "* 💼 **Applications:** Task-solving, information retrieval, decision-making.\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves problem-solving abilities\n",
        "  * Enhances model's interaction with external tools/data\n",
        "\n",
        "* 🚀 **Implementation:** Uses specific prompts to guide the model through the Thought-Action-Observation cycle.\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * **Thought:** [Reasoning about the task]\n",
        "  * **Action:** [Chosen action, e.g., 'Search for X']\n",
        "  * **Observation:** [Result of the action]\n",
        "  * **Thought:** [Next step based on observation]\n",
        "\n",
        "* 📈 **Use cases:** Web navigation, complex multi-step tasks, interactive problem-solving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "id": "f6jozW4CIJ9p",
        "outputId": "a7cc47a6-fc61-4b98-dc06-8a9368c2bd6f"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**ReAct Response:**\n",
              "**Task:** Determine the area of a triangle\n",
              "\n",
              "**1) Analyze the task and identify key components**\n",
              "\n",
              "* The task is to determine the area of a triangle.\n",
              "* The key components of a triangle are its base and height.\n",
              "\n",
              "**2) Determine what information or actions are needed**\n",
              "\n",
              "* We need to know the base and height of the triangle.\n",
              "\n",
              "**3) If information is needed, state what you need to know**\n",
              "\n",
              "* We need to know the base and height of the triangle.\n",
              "\n",
              "**4) If an action is needed, describe the action**\n",
              "\n",
              "* We need to multiply the base and height of the triangle by 1/2.\n",
              "\n",
              "**5) Repeat steps 3-4 until the task is complete**\n",
              "\n",
              "* We have all the information we need, so we can proceed to the next step.\n",
              "\n",
              "**6) Provide the final answer or solution**\n",
              "\n",
              "* The area of a triangle is: A = (1/2) * base * height"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "react_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI assistant capable of reasoning and acting. Approach tasks step-by-step, explaining your thought process and actions.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Task: {task}\n",
        "\n",
        "    Think through this task step by step:\n",
        "    1) Analyze the task and identify key components\n",
        "    2) Determine what information or actions are needed\n",
        "    3) If information is needed, state what you need to know\n",
        "    4) If an action is needed, describe the action\n",
        "    5) Repeat steps 3-4 until the task is complete\n",
        "    6) Provide the final answer or solution\n",
        "\n",
        "    Your response:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def react_response(task):\n",
        "    messages = react_prompt.format_prompt(task=task).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "task = \"Calculate the total cost of a shopping trip where you buy 3 apples at $0.50 each and 2 loaves of bread at $2.25 each. Don't forget to add 8% sales tax.\"\n",
        "response = react_response(task)\n",
        "display(Markdown(f\"**ReAct Response:**\\n{response}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b667A5ROU6Mx"
      },
      "source": [
        "# 🌳 Tree of Thoughts (ToT)\n",
        "\n",
        "Tree of Thoughts is an advanced prompting method that explores multiple reasoning paths simultaneously.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Generates and evaluates multiple \"thoughts\" at each step of reasoning.\n",
        "\n",
        "* 📊 **Structure:** Creates a tree-like structure of potential solution paths.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Generate multiple initial thoughts\n",
        "  * Evaluate and expand promising thoughts\n",
        "  * Prune less promising branches\n",
        "  * Iterate until reaching a solution\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Explores diverse problem-solving approaches\n",
        "  * Reduces chances of getting stuck in suboptimal reasoning paths\n",
        "\n",
        "* 💼 **Applications:** Complex problem-solving, strategic planning, creative tasks.\n",
        "\n",
        "* 🚀 **Implementation:** Requires careful prompting to guide thought generation and evaluation.\n",
        "\n",
        "* 🔑 **Key components:**\n",
        "  * Thought generator\n",
        "  * State evaluator\n",
        "  * Search algorithm (e.g., breadth-first, depth-first)\n",
        "\n",
        "* ⚖️**Challenges:** Balancing exploration breadth with computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "rDfnsXpKIJ_0",
        "outputId": "5d435a43-72e3-4cbb-90fd-ebddbe0b0194"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Tree of Thoughts Response:**\n",
              "**Problem:** Design a system to manage and track employee performance.\n",
              "\n",
              "**Approach 1:**\n",
              "\n",
              "**Description:** Implement a traditional performance management system using spreadsheets or a basic database. Managers set goals for employees, track progress, and provide feedback through regular performance reviews.\n",
              "\n",
              "**Advantages:**\n",
              "- Low cost and easy to implement\n",
              "- Provides a structured framework for performance evaluation\n",
              "- Can be tailored to specific team or organizational needs\n",
              "\n",
              "**Disadvantages:**\n",
              "- Manual and time-consuming\n",
              "- Prone to biases and subjectivity\n",
              "- Lacks real-time data and insights\n",
              "\n",
              "**Approach 2:**\n",
              "\n",
              "**Description:** Utilize a cloud-based performance management software. The software automates goal setting, progress tracking, feedback, and reporting. It also provides analytics and dashboards for data-driven decision-making.\n",
              "\n",
              "**Advantages:**\n",
              "- Streamlines the performance management process\n",
              "- Enhances accuracy and objectivity\n",
              "- Provides real-time visibility and insights\n",
              "- Facilitates collaboration and communication\n",
              "\n",
              "**Disadvantages:**\n",
              "- Can be expensive to implement and maintain\n",
              "- Requires employee buy-in and training\n",
              "- May not be suitable for all organizations\n",
              "\n",
              "**Approach 3:**\n",
              "\n",
              "**Description:** Adopt a continuous performance management system. Employees set their own goals, receive ongoing feedback, and have regular check-ins with their managers. The focus is on development and improvement rather than formal evaluations.\n",
              "\n",
              "**Advantages:**\n",
              "- Empowers employees and promotes self-motivation\n",
              "- Encourages a culture of feedback and learning\n",
              "- Adapts to changing business needs\n",
              "- Improves employee engagement and retention\n",
              "\n",
              "**Disadvantages:**\n",
              "- Requires a high level of trust and commitment\n",
              "- Can be challenging to implement in large organizations\n",
              "- May not provide the necessary structure for certain roles\n",
              "\n",
              "**Evaluation:**\n",
              "\n",
              "Based on the evaluation, Approach 2 (cloud-based performance management software) is the best solution. It offers a comprehensive and efficient approach that addresses the limitations of Approach 1. Approach 3 is also a viable option for organizations that prioritize employee development and empowerment. However, Approach 1 may be suitable for smaller teams or organizations with limited resources.\n",
              "\n",
              "**Best Solution:**\n",
              "\n",
              "Approach 2: Cloud-based performance management software\n",
              "\n",
              "This solution provides a balance of automation, objectivity, and data-driven insights. It streamlines the performance management process, reduces biases, and facilitates collaboration. It is also scalable and adaptable to changing organizational needs."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI that can explore multiple solution paths for complex problems.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Explore multiple solution paths for this problem:\n",
        "    {problem}\n",
        "\n",
        "    Generate three different approaches, then evaluate and choose the best one:\n",
        "\n",
        "    Approach 1:\n",
        "    [Generate first approach]\n",
        "\n",
        "    Approach 2:\n",
        "    [Generate second approach]\n",
        "\n",
        "    Approach 3:\n",
        "    [Generate third approach]\n",
        "\n",
        "    Evaluation:\n",
        "    [Evaluate the three approaches]\n",
        "\n",
        "    Best Solution:\n",
        "    [Choose and explain the best solution]\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def tot_response(problem):\n",
        "    messages = tot_prompt.format_prompt(problem=problem).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "problem = (\n",
        "    \"Design a sustainable urban transportation system for a city of 1 million people.\"\n",
        ")\n",
        "response = tot_response(problem)\n",
        "\n",
        "# Display the response\n",
        "display(Markdown(f\"**Tree of Thoughts Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivh8e2yfu6iX"
      },
      "source": [
        "# 🔄 Self-Consistency\n",
        "\n",
        "Self-Consistency is a method to improve the reliability of language model outputs.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Generate multiple independent solutions and select the most consistent one.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Prompt the model multiple times for the same task\n",
        "  * Collect various reasoning paths and answers\n",
        "  * Choose the most common or consistent answer\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves accuracy, especially for complex tasks\n",
        "  * Reduces impact of occasional errors or biases\n",
        "\n",
        "* 💼 **Applications:** Mathematical problem-solving, logical reasoning, decision-making.\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Use temperature settings to introduce variability\n",
        "  * Prompt for full reasoning chains, not just final answers\n",
        "\n",
        "* **Evaluation:** Can use voting mechanisms or more sophisticated consistency measures.\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Increased computational cost\n",
        "  * Handling genuinely ambiguous problems\n",
        "\n",
        "* 🔄 **Variations:** Can be combined with other techniques like Chain of Thought or Tree of Thoughts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZM68icXIKCg",
        "outputId": "3babf5a5-5db9-43b6-95bf-6292d0634e91"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Self-Consistency Response:**\n",
              "**Problem:** Find the area of a triangle with a base of 10 cm and a height of 8 cm.\n",
              "\n",
              "**Method 1: Using the formula**\n",
              "\n",
              "The area of a triangle is given by the formula:\n",
              "\n",
              "```\n",
              "Area = (1/2) * base * height\n",
              "```\n",
              "\n",
              "Substituting the given values, we get:\n",
              "\n",
              "```\n",
              "Area = (1/2) * 10 cm * 8 cm = 40 cm²\n",
              "```\n",
              "\n",
              "**Method 2: Using the Pythagorean theorem**\n",
              "\n",
              "We can use the Pythagorean theorem to find the length of the hypotenuse of the triangle:\n",
              "\n",
              "```\n",
              "hypotenuse² = base² + height²\n",
              "```\n",
              "\n",
              "Substituting the given values, we get:\n",
              "\n",
              "```\n",
              "hypotenuse² = 10 cm² + 8 cm² = 184 cm²\n",
              "hypotenuse = √184 cm² = 13.57 cm\n",
              "```\n",
              "\n",
              "Now we can use the formula for the area of a triangle again:\n",
              "\n",
              "```\n",
              "Area = (1/2) * base * height\n",
              "```\n",
              "\n",
              "Substituting the given values and the length of the hypotenuse, we get:\n",
              "\n",
              "```\n",
              "Area = (1/2) * 10 cm * 8 cm = 40 cm²\n",
              "```\n",
              "\n",
              "**Method 3: Using geometry**\n",
              "\n",
              "We can divide the triangle into two right triangles, each with a base of 10 cm and a height of 4 cm. The area of each right triangle is:\n",
              "\n",
              "```\n",
              "Area = (1/2) * base * height\n",
              "```\n",
              "\n",
              "Substituting the given values, we get:\n",
              "\n",
              "```\n",
              "Area of each right triangle = (1/2) * 10 cm * 4 cm = 20 cm²\n",
              "```\n",
              "\n",
              "Therefore, the area of the entire triangle is:\n",
              "\n",
              "```\n",
              "Area = 2 * Area of each right triangle = 2 * 20 cm² = 40 cm²\n",
              "```\n",
              "\n",
              "**Problem:** Find the area of a triangle with base b and height h.\n",
              "\n",
              "**Method 1: Formula**\n",
              "\n",
              "* Area = (1/2) * b * h\n",
              "\n",
              "**Method 2: Heron's Formula**\n",
              "\n",
              "* s = (a + b + c) / 2\n",
              "* Area = √(s * (s - a) * (s - b) * (s - c))\n",
              "* where a, b, and c are the lengths of the triangle's sides (in this case, a = b, b = b, and c = h)\n",
              "\n",
              "**Method 3: Calculus**\n",
              "\n",
              "* Divide the triangle into infinitesimally small strips of width dx.\n",
              "* The area of each strip is given by dA = (1/2) * b * dx.\n",
              "* The total area is found by integrating from x = 0 to x = h:\n",
              "```\n",
              "Area = ∫[0, h] (1/2) * b * dx\n",
              "     = (1/2) * b * [x]_0^h\n",
              "     = (1/2) * b * h\n",
              "```\n",
              "\n",
              "**Problem:** Find the area of a circle with a radius of 5 cm.\n",
              "\n",
              "**Method 1: Using the formula for the area of a circle**\n",
              "\n",
              "```\n",
              "A = πr²\n",
              "A = π(5 cm)²\n",
              "A = 25π cm²\n",
              "```\n",
              "\n",
              "**Method 2: Using the circumference of the circle**\n",
              "\n",
              "```\n",
              "C = 2πr\n",
              "A = (C/2π)r\n",
              "A = (10π cm / 2π)5 cm\n",
              "A = 25 cm²\n",
              "```\n",
              "\n",
              "**Method 3: Using the Pythagorean theorem**\n",
              "\n",
              "```\n",
              "r² + h² = d²\n",
              "h = √(d² - r²)\n",
              "A = ½bh\n",
              "A = ½(d)(h)\n",
              "A = ½(10 cm)(√(10² cm² - 5² cm²))\n",
              "A = 25 cm²\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "self_consistency_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI that can solve problems using multiple approaches.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"Solve this problem using three different methods: {problem}\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def self_consistency_response(problem, n_solutions=3):\n",
        "    messages = self_consistency_prompt.format_prompt(problem=problem).to_messages()\n",
        "    solutions = [llm.invoke(messages).content for _ in range(n_solutions)]\n",
        "    # In a real scenario, you'd implement logic to choose the most consistent answer\n",
        "    return \"\\n\\n\".join(solutions)\n",
        "\n",
        "\n",
        "problem = \"What is the volume of a cube with side length 4cm?\"\n",
        "response = self_consistency_response(problem)\n",
        "display(Markdown(f\"**Self-Consistency Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb0lYapgvXHY"
      },
      "source": [
        "# 📄 Hypothetical Document Embeddings (HyDE)\n",
        "\n",
        "HyDE is a method to improve retrieval and question-answering tasks using language models.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Generate hypothetical documents to enhance retrieval of relevant information.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Create a hypothetical answer or document for a given query\n",
        "  * Embed this hypothetical document\n",
        "  * Use the embedding to retrieve similar real documents\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves retrieval accuracy, especially for complex queries\n",
        "  * Bridges the gap between query and document language\n",
        "\n",
        "* 💼 **Applications:** Information retrieval, question-answering systems, search engines.\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Prompt the model to generate a plausible answer or document\n",
        "  * Use embedding models to convert text to vector representations\n",
        "  * Employ similarity search to find matching real documents\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Quality of hypothetical document affects retrieval performance\n",
        "  * Computational overhead of generating and embedding hypothetical documents\n",
        "\n",
        "* 🔄 **Variations:**\n",
        "  * Multi-HyDE: Generate multiple hypothetical documents\n",
        "  * Iterative HyDE: Refine hypothetical documents based on retrieved results\n",
        "\n",
        "* ✔️ **Effectiveness:** Often outperforms traditional keyword-based retrieval methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "1deQWbLdIKE8",
        "outputId": "970248e3-3bb6-41a5-93cd-2874c60dfc18"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**HyDE Response:**\n",
              "**Using Multi-Modal Models in a Chain**\n",
              "\n",
              "LangChain makes it easy to use multiple LLM models in a single application. To do this, you can use the `chain` function to connect multiple models together. For example, the following code shows how to use a text-to-image model and an image-to-text model in a chain:\n",
              "\n",
              "```python\n",
              "from langchain import chain\n",
              "\n",
              "text_to_image_model = load_text_to_image_model()\n",
              "image_to_text_model = load_image_to_text_model()\n",
              "\n",
              "chain = chain(text_to_image_model, image_to_text_model)\n",
              "```\n",
              "\n",
              "Once you have created a chain, you can use it to process data. For example, the following code shows how to use the chain to generate an image from a text prompt:\n",
              "\n",
              "```python\n",
              "prompt = \"A beautiful landscape with a river and mountains\"\n",
              "image = chain.process(prompt)\n",
              "```\n",
              "\n",
              "**Turning a Chain into a REST API**\n",
              "\n",
              "LangServe makes it easy to deploy a LangChain application as a REST API. To do this, you can use the `serve` function to create a REST API endpoint that exposes the chain's functionality. For example, the following code shows how to create a REST API endpoint that exposes the chain created in the previous example:\n",
              "\n",
              "```python\n",
              "from langserve import serve\n",
              "\n",
              "api = serve(chain)\n",
              "```\n",
              "\n",
              "Once you have created a REST API endpoint, you can use it to process data from a web application or other client. For example, the following code shows how to use the REST API endpoint to generate an image from a text prompt:\n",
              "\n",
              "```python\n",
              "import requests\n",
              "\n",
              "prompt = \"A beautiful landscape with a river and mountains\"\n",
              "url = \"http://localhost:8000/generate\"\n",
              "response = requests.post(url, json={\"prompt\": prompt})\n",
              "image = response.json()[\"image\"]\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "system = \"\"\"You are an expert about a set of software for building LLM-powered applications called LangChain, LangGraph, LangServe, and LangSmith.\n",
        "\n",
        "        LangChain is a Python framework that provides a large set of integrations that can easily be composed to build LLM applications.\n",
        "        LangGraph is a Python package built on top of LangChain that makes it easy to build stateful, multi-actor LLM applications.\n",
        "        LangServe is a Python package built on top of LangChain that makes it easy to deploy a LangChain application as a REST API.\n",
        "        LangSmith is a platform that makes it easy to trace and test LLM applications.\n",
        "\n",
        "        Answer the user question as best you can. Answer as though you were writing a tutorial that addressed the user question.\"\"\"\n",
        "\n",
        "hyde_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def hyde_response(question):\n",
        "    messages = hyde_prompt.format_prompt(question=question).to_messages()\n",
        "    response = llm.invoke(messages)\n",
        "    return response.content\n",
        "\n",
        "\n",
        "question = \"how to use multi-modal models in a chain and turn chain into a rest api\"\n",
        "\n",
        "response = hyde_response(question)\n",
        "display(Markdown(f\"**HyDE Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmDb3TZ5v6DC"
      },
      "source": [
        "# 🏗️ Least-to-Most Prompting\n",
        "\n",
        "Least-to-Most Prompting is a method for breaking down complex problems into simpler, manageable sub-problems.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Solve a complex task by addressing easier sub-tasks first.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Decompose the main problem into sub-problems\n",
        "  * Solve sub-problems in order of increasing difficulty\n",
        "  * Use solutions from earlier steps to inform later ones\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves handling of complex, multi-step problems\n",
        "  * Enhances model's problem-solving capabilities\n",
        "\n",
        "* 💼 **Applications:** Multi-step reasoning, complex math problems, algorithmic tasks.\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Craft prompts that guide the model to identify sub-problems\n",
        "  * Use intermediate results as context for subsequent steps\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Effective problem decomposition\n",
        "  * Maintaining coherence across sub-problems\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  1. What's the first step to solve [problem]?\n",
        "  2. Given [previous result], what's the next step?\n",
        "  3. Using all previous information, solve [final step].\n",
        "\n",
        "* ✔️ **Effectiveness:** Often leads to more accurate solutions for complex tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "B18N5YbqIKJB",
        "outputId": "9e0f11cb-d1be-4085-85e0-16819a3e8d32"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Least-to-Most Prompting Response:**\n",
              "**Task:** Develop a machine learning model to predict stock prices\n",
              "\n",
              "**Steps:**\n",
              "\n",
              "1) **Gather data:** Collect historical stock prices, financial data, and other relevant information that can be used to train the model.\n",
              "2) **Clean and prepare data:** Remove any duplicate or missing data points, and preprocess the data to ensure it is in a suitable format for modeling.\n",
              "3) **Choose a machine learning algorithm:** Select an appropriate machine learning algorithm, such as a linear regression model or a neural network, to predict stock prices.\n",
              "4) **Train the model:** Use the training data to fit the chosen machine learning algorithm and determine its parameters.\n",
              "5) **Validate the model:** Evaluate the performance of the model on a separate validation dataset to assess its accuracy and reliability.\n",
              "6) **Deploy the model:** Once the model is validated, deploy it to make predictions on new data, such as predicting future stock prices.\n",
              "7) **Monitor and maintain the model:** Regularly monitor the model's performance and make adjustments as needed to ensure its continued accuracy and effectiveness."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ltm_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI that can break down complex problems into simpler sub-problems.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Break down this complex task into smaller, manageable steps:\n",
        "    Task: {task}\n",
        "\n",
        "    Steps:\n",
        "    1)\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def ltm_response(task):\n",
        "    messages = ltm_prompt.format_prompt(task=task).to_messages()\n",
        "    steps = llm.invoke(messages).content\n",
        "\n",
        "    # Now solve each step\n",
        "    solve_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(content=\"You are an AI that can solve problems step by step.\"),\n",
        "            HumanMessage(\n",
        "                content=f\"\"\"Solve each step of this task:\n",
        "        Task: {task}\n",
        "\n",
        "        Steps:\n",
        "        {steps}\n",
        "\n",
        "        Solutions:\"\"\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    solve_messages = solve_prompt.format_prompt().to_messages()\n",
        "    return llm.invoke(solve_messages).content\n",
        "\n",
        "\n",
        "task = \"Develop a machine learning model to predict stock prices\"\n",
        "response = ltm_response(task)\n",
        "display(Markdown(f\"**Least-to-Most Prompting Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXPjR6y3wbm8"
      },
      "source": [
        "# 🔗 Prompt Chaining\n",
        "\n",
        "Prompt Chaining is a method of breaking down complex tasks into a series of simpler, interconnected prompts.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Use the output of one prompt as input for the next in a sequence.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Divide the main task into smaller, manageable sub-tasks\n",
        "  * Create a series of prompts, each addressing a sub-task\n",
        "  * Pass results from each step to inform subsequent prompts\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Handles complex, multi-stage problems more effectively\n",
        "  * Improves overall task performance and accuracy\n",
        "\n",
        "* 💼 **Applications:** Data analysis, content creation, multi-step reasoning tasks.\n",
        "\n",
        "* 🚀**Implementation:**\n",
        "  * Design a logical sequence of prompts\n",
        "  * Ensure each prompt builds on previous results\n",
        "  * Manage context and token limits across the chain\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Error propagation through the chain\n",
        "  * Maintaining coherence across multiple prompts\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Step 1: [Initial prompt]\n",
        "  * Step 2: Given [result from Step 1], now [next sub-task]\n",
        "  * Step 3: Using [results from Steps 1 and 2], [final sub-task]\n",
        "\n",
        "* 🔄 **Variations:** Can be combined with other techniques like CoT or ReAct for enhanced performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "O75bD7rywb0Y",
        "outputId": "fb7abf21-a41d-4199-ebb8-63bebeee7436"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Prompt Chaining Response:**\n",
              "1. AI encompasses computational techniques that empower machines to perform tasks requiring human intelligence.\n",
              "2. Machine learning, deep learning, natural language processing, and computer vision are prominent AI approaches.\n",
              "3. AI has widespread applications in various domains, offering benefits like efficiency, automation, and personalization."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def prompt_chaining(topic):\n",
        "    overview_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=\"You are an AI that can generate brief overviews of topics.\"\n",
        "            ),\n",
        "            HumanMessage(content=\"Generate a brief overview of {topic}:\"),\n",
        "        ]\n",
        "    )\n",
        "    overview_messages = overview_prompt.format_prompt(topic=topic).to_messages()\n",
        "    overview = llm.invoke(overview_messages).content\n",
        "\n",
        "    key_points_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=\"You are an AI that can extract key points from overviews.\"\n",
        "            ),\n",
        "            HumanMessage(\n",
        "                content=f\"\"\"Based on this overview, list 3 key points:\n",
        "        {overview}\n",
        "\n",
        "        Key points:\"\"\"\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    key_points_messages = key_points_prompt.format_prompt().to_messages()\n",
        "    return llm.invoke(key_points_messages).content\n",
        "\n",
        "\n",
        "topic = \"quantum computing\"\n",
        "response = prompt_chaining(topic)\n",
        "display(Markdown(f\"**Prompt Chaining Response:**\\n{response}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKNT2Y-Fwtsg"
      },
      "source": [
        "# 📊 Graph Prompting\n",
        "**Note:** This is a simplified version without usinga graph database\n",
        "\n",
        "Graph Prompting is an advanced method that uses graph structures to guide complex reasoning tasks.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Represent problems as interconnected nodes in a graph, with prompts guiding traversal and reasoning.\n",
        "\n",
        "* 📊 **Structure:**\n",
        "  * Nodes represent concepts, sub-tasks, or decision points\n",
        "  * Edges represent relationships or transitions between nodes\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Define the problem as a graph\n",
        "  * Guide the model through the graph using targeted prompts\n",
        "  * Aggregate information from traversed nodes to form a solution\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Handles complex, interconnected problems\n",
        "  * Allows for non-linear reasoning paths\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Multi-step decision making\n",
        "  * Knowledge graph navigation\n",
        "  * Solving problems with multiple dependencies\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Design the graph structure based on the problem domain\n",
        "  * Craft prompts for node exploration and edge traversal\n",
        "  * Develop strategies for information aggregation across nodes\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Designing effective graph structures\n",
        "  * Managing context across multiple graph traversals\n",
        "\n",
        "* 🔄 **Variations:**\n",
        "  * Dynamic graph prompting: Adjust the graph structure based on intermediate results\n",
        "  * Hierarchical graph prompting: Use nested graphs for multi-level reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "M7CwC26Zwt7I",
        "outputId": "eb25106b-91b8-468e-b629-8ab8dc217df2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Graph Prompting Response:**\n",
              "Red Planet"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "system = \"\"\"You are an AI that can reason over graph-structured knowledge.\"\"\"\n",
        "\n",
        "graph_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\"Given the following graph structure:\n",
        "                    Earth - neighboring planet -> Mars\n",
        "                    Mars - nickname -> Red Planet\n",
        "\n",
        "                    Answer the following question:\n",
        "                    {question}\n",
        "\n",
        "                    Answer:\"\"\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def graph_prompting(question):\n",
        "    messages = graph_prompt.format_prompt(question=question).to_messages()\n",
        "    return llm.invoke(messages).content\n",
        "\n",
        "\n",
        "question = \"What is the nickname of the neighboring planet to Earth?\"\n",
        "response = graph_prompting(question)\n",
        "display(Markdown(f\"**Graph Prompting Response:**\\n{response}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGn4LyUVxBKi"
      },
      "source": [
        "# 🔄 Recursive Prompting\n",
        "\n",
        "Recursive Prompting is a method where a language model's output is fed back into itself as input for further processing.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Use the model's output as input for subsequent prompts, creating a feedback loop.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Start with an initial prompt\n",
        "  * Use the output to formulate a new, refined prompt\n",
        "  * Repeat the process until a satisfactory result is achieved\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Enables iterative refinement of responses\n",
        "  * Allows for deeper exploration of complex topics\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Text summarization\n",
        "  * Idea generation and brainstorming\n",
        "  * Progressive problem-solving\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Design a base prompt that can accept its own output\n",
        "  * Implement a stopping condition to prevent infinite loops\n",
        "  * Manage context length as recursion deepens\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Avoiding circular reasoning or repetition\n",
        "  * Maintaining coherence across recursive steps\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Initial prompt: [Task description]\n",
        "  * Recursive step: Based on the previous output, [refined task]\n",
        "  * Stopping condition: Continue until [specific criteria met]\n",
        "\n",
        "* 🔄 **Variations:**\n",
        "  * Self-reflection: Use recursion for the model to critique and improve its own outputs\n",
        "  * Depth-limited recursion: Set a maximum number of recursive steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "VltIhYAExBVY",
        "outputId": "1bb6df0f-fd7b-4703-b4cc-a1b6f48caed9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Recursive Prompting Response:**\n",
              "1. **How can we ensure that AI-driven solutions for climate change mitigation and environmental sustainability are accessible and equitable, particularly for marginalized communities and developing regions?**\n",
              "2. **What mechanisms can be implemented to monitor and evaluate the ethical implications of AI systems, ensuring that they do not perpetuate or exacerbate existing biases and inequalities?**\n",
              "3. **How can we create educational and training programs to equip policymakers and civil society organizations with the knowledge and skills necessary to engage effectively with AI advancements and shape their development and deployment?**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def recursive_prompting(topic, max_depth=3):\n",
        "\n",
        "    system = \"\"\"You are an AI that can generate questions about topics.\"\"\"\n",
        "    base_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"human\", system),\n",
        "            (\"human\", \"Generate three questions about {topic}:\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    questions = llm.invoke(base_prompt.format_prompt(topic=topic).to_messages()).content\n",
        "\n",
        "    for depth in range(1, max_depth):\n",
        "        system = (\n",
        "            \"\"\"You are an AI that can generate more detailed follow-up questions.\"\"\"\n",
        "        )\n",
        "        recursive_prompt = ChatPromptTemplate.from_messages(\n",
        "            [\n",
        "                (\"human\", system),\n",
        "                (\n",
        "                    \"human\",\n",
        "                    f\"\"\"Based on these questions:\n",
        "                    {questions}\n",
        "\n",
        "                    Generate three more detailed follow-up questions. Current depth: {max_depth}\"\"\",\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "        questions = llm.invoke(recursive_prompt.format_prompt().to_messages()).content\n",
        "\n",
        "    return questions\n",
        "\n",
        "\n",
        "topic = \"artificial intelligence\"\n",
        "response = recursive_prompting(topic)\n",
        "display(Markdown(f\"**Recursive Prompting Response:**\\n{response}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8TIiYLksBR8"
      },
      "source": [
        "# 💡 Generated Knowledge\n",
        "\n",
        "Generated Knowledge is a method that uses language models to create relevant information before solving a task.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Generate task-specific knowledge before addressing the main problem.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Prompt the model to generate relevant facts or context\n",
        "  * Use this generated knowledge as input for the primary task\n",
        "  * Solve the main problem with enhanced context\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves performance on tasks requiring specific knowledge\n",
        "  * Allows for dynamic, task-specific information generation\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Question answering\n",
        "  * Contextual reasoning\n",
        "  * Domain-specific problem solving\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Design prompts to elicit relevant knowledge generation\n",
        "  * Integrate generated knowledge into the main task prompt\n",
        "  * Evaluate and filter generated knowledge for relevance\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Ensuring accuracy of generated knowledge\n",
        "  * Balancing knowledge generation with task completion\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Step 1: Generate knowledge about [topic relevant to task]\n",
        "  * Step 2: Using the generated information, solve [main task]\n",
        "\n",
        "* 🔄 **Variations:**\n",
        "  * Multi-step knowledge generation\n",
        "  * Combining generated knowledge with external sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "-92ApoB4svfM",
        "outputId": "d55eb0fa-35d1-46fc-e063-72690b116a24"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Generated Knowledge:**\n",
              "**Topic: Artificial Intelligence (AI)**\n",
              "\n",
              "**Key Facts:**\n",
              "\n",
              "* AI refers to the simulation of human intelligence processes by machines.\n",
              "* It encompasses various subfields, including machine learning, natural language processing, and computer vision.\n",
              "* AI algorithms are trained on vast datasets to learn patterns and make predictions.\n",
              "* AI has applications in numerous industries, such as healthcare, finance, and manufacturing.\n",
              "\n",
              "**Key Concepts:**\n",
              "\n",
              "* **Machine Learning:** AI algorithms that learn from data without explicit programming.\n",
              "* **Deep Learning:** A type of machine learning that uses artificial neural networks to process complex data.\n",
              "* **Natural Language Processing (NLP):** AI techniques that enable computers to understand and generate human language.\n",
              "* **Computer Vision:** AI algorithms that analyze and interpret visual data.\n",
              "* **Artificial General Intelligence (AGI):** A hypothetical AI system that possesses human-level intelligence across a wide range of tasks.\n",
              "* **Ethical Considerations:** AI raises ethical concerns regarding privacy, bias, and job displacement.\n",
              "* **Future of AI:** AI is expected to continue advancing rapidly, transforming various aspects of society and the economy.\n",
              "\n",
              "**Question:**\n",
              "What are the potential applications of quantum computing in cryptography?\n",
              "\n",
              "**Generated Knowledge Response:**\n",
              "I do not have access to any knowledge or questions to answer. Please provide the knowledge and question you would like me to answer."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "generate_knowledge_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI that can generate relevant knowledge about a given topic.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"Generate a brief summary of key facts and concepts related to {topic}:\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "answer_question_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI assistant that can answer questions based on given knowledge.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Use the following generated knowledge to answer the question:\n",
        "\n",
        "    Generated Knowledge:\n",
        "    {knowledge}\n",
        "\n",
        "    Question: {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def generated_knowledge_response(topic, question):\n",
        "    # Step 1: Generate relevant knowledge\n",
        "    knowledge_messages = generate_knowledge_prompt.format_prompt(\n",
        "        topic=topic\n",
        "    ).to_messages()\n",
        "    generated_knowledge = llm.invoke(knowledge_messages).content\n",
        "\n",
        "    # Step 2: Use the generated knowledge to answer the question\n",
        "    answer_messages = answer_question_prompt.format_prompt(\n",
        "        knowledge=generated_knowledge, question=question\n",
        "    ).to_messages()\n",
        "    answer = llm.invoke(answer_messages).content\n",
        "\n",
        "    return generated_knowledge, answer\n",
        "\n",
        "\n",
        "# Example usage\n",
        "topic = \"quantum computing\"\n",
        "question = \"What are the potential applications of quantum computing in cryptography?\"\n",
        "\n",
        "generated_knowledge, response = generated_knowledge_response(topic, question)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"**Generated Knowledge:**\n",
        "{generated_knowledge}\n",
        "\n",
        "**Question:**\n",
        "{question}\n",
        "\n",
        "**Generated Knowledge Response:**\n",
        "{response}\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq4lkZvNsvtE"
      },
      "source": [
        "# ⚙️ Automatic Reasoning and Tool-Use (ART)\n",
        "\n",
        "ART is an advanced prompting method that combines reasoning with automated tool selection and use.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Enable language models to autonomously reason about problems and select/use appropriate tools.\n",
        "\n",
        "* 🛠️ **Components:**\n",
        "  * Reasoning module\n",
        "  * Tool selection mechanism\n",
        "  * Tool use interface\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Analyze the problem through reasoning\n",
        "  * Identify and select relevant tools\n",
        "  * Use tools to gather information or perform actions\n",
        "  * Integrate tool outputs into the reasoning process\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Enhances problem-solving capabilities\n",
        "  * Allows for more complex, multi-step tasks\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Data analysis\n",
        "  * Web-based research\n",
        "  * Complex decision-making scenarios\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Define a set of available tools and their functions\n",
        "  * Design prompts that encourage tool consideration\n",
        "  * Implement feedback loops between reasoning and tool use\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Ensuring appropriate tool selection\n",
        "  * Managing context across multiple tool uses\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Thought: [Reasoning about the problem]\n",
        "  * Tool Selection: [Choose appropriate tool]\n",
        "  * Tool Use: [Apply selected tool]\n",
        "  * Integration: [Incorporate tool output into reasoning]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "TV6VCvTNtkWc",
        "outputId": "70c79b2b-2bc4-4eca-f55f-633dabb01e27"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Automatic Reasoning and Tool-Use (ART) Response:**\n",
              "Task: Calculate the number of days between the current date and July 4, 2025. Then, provide the weather forecast for New York City.\n",
              "Task: Calculate the number of days between today and my birthday, which is on 2023-08-15.\n",
              "\n",
              "1. **Identify the necessary tool:** To calculate the number of days between two dates, we need to use the `days_between` tool.\n",
              "2. **Apply the tool:** We can use the `days_between` tool to calculate the number of days between today and my birthday.\n",
              "```TOOL_CALL\n",
              "Using days_between: 2023-08-15\n",
              "Result: -360\n",
              "```"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datetime import datetime, date\n",
        "# Define available tools\n",
        "def calculate_days(date_string):\n",
        "    target_date = datetime.strptime(date_string, \"%Y-%m-%d\").date()\n",
        "    current_date = date.today()\n",
        "    return (target_date - current_date).days\n",
        "\n",
        "tools = {\n",
        "    \"calculator\": lambda x: eval(x),\n",
        "    \"date\": lambda: date.today().strftime(\"%Y-%m-%d\"),\n",
        "    \"weather\": lambda city: f\"The weather in {city} is sunny with a high of 25°C.\",\n",
        "    \"days_between\": calculate_days\n",
        "}\n",
        "\n",
        "# ART Prompt\n",
        "art_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"\"\"You are an AI assistant capable of breaking down complex tasks, identifying necessary tools, and applying them to solve problems. You have access to the following tools:\n",
        "    1. calculator: Performs mathematical calculations. Input should be a mathematical expression.\n",
        "    2. date: Returns the current date.\n",
        "    3. weather: Provides weather information for a given city.\n",
        "    4. days_between: Calculates the number of days between the current date and a given date (format: YYYY-MM-DD).\n",
        "    For each step in your reasoning, if a tool is needed, specify it in the following JSON format:\n",
        "    {\"tool\": \"tool_name\", \"input\": \"tool_input\"}\n",
        "    Your final answer should not be in JSON format.\"\"\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Task: {task}\n",
        "    Break down this task and solve it step by step. For each step, explain your reasoning and use tools when necessary.\n",
        "    Your response:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def art_response(task):\n",
        "    messages = art_prompt.format_prompt(task=task).to_messages()\n",
        "    raw_response = llm.invoke(messages).content\n",
        "    # Process the response to use tools\n",
        "    lines = raw_response.split(\"\\n\")\n",
        "    processed_response = []\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"{\") and line.strip().endswith(\"}\"):\n",
        "            try:\n",
        "                tool_call = json.loads(line)\n",
        "                if tool_call[\"tool\"] in tools:\n",
        "                    tool_result = tools[tool_call[\"tool\"]](tool_call[\"input\"])\n",
        "                    processed_response.append(\n",
        "                        f\"Using {tool_call['tool']}: {tool_call['input']}\"\n",
        "                    )\n",
        "                    processed_response.append(f\"Result: {tool_result}\")\n",
        "                else:\n",
        "                    processed_response.append(\n",
        "                        f\"Error: Tool '{tool_call['tool']}' not found.\"\n",
        "                    )\n",
        "            except json.JSONDecodeError:\n",
        "                processed_response.append(line)\n",
        "        else:\n",
        "            processed_response.append(line)\n",
        "    return \"\\n\".join(processed_response)\n",
        "\n",
        "# Example usage\n",
        "task = \"Calculate the number of days between the current date and July 4, 2025. Then, provide the weather forecast for New York City.\"\n",
        "response = art_response(task)\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"**Automatic Reasoning and Tool-Use (ART) Response:**\n",
        "Task: {task}\n",
        "{response}\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7_dJXb2tk-D"
      },
      "source": [
        "# 🛠️ Automatic Prompt Engineer (APE)\n",
        "\n",
        "APE is a method for automatically generating and optimizing prompts for language models.\n",
        "\n",
        "## Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Use AI to create and refine prompts, reducing manual engineering effort.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Generate candidate prompts\n",
        "  * Evaluate prompt performance\n",
        "  * Iteratively optimize prompts\n",
        "\n",
        "* 🛠️ **Components:**\n",
        "  * Prompt generator\n",
        "  * Performance evaluator\n",
        "  * Optimization algorithm\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Discovers effective prompts automatically\n",
        "  * Adapts to different tasks and model architectures\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Task-specific prompt optimization\n",
        "  * Improving model performance across various domains\n",
        "\n",
        "* 🚀 **Implementation:**\n",
        "  * Define task and evaluation metrics\n",
        "  * Use large language models to generate initial prompts\n",
        "  * Apply optimization techniques (e.g., genetic algorithms, gradient-based methods)\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Balancing exploration and exploitation in prompt space\n",
        "  *Ensuring generated prompts are interpretable and safe\n",
        "  \n",
        "* 🔄 **Variations:**\n",
        "  * Multi-task APE: Optimize prompts for multiple related tasks\n",
        "  * Constrained APE: Generate prompts within specific guidelines or structures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CLEMZN8st7KF",
        "outputId": "30748fb9-c18e-4d9f-b191-505bc3ae7b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Generated Prompts:**\n",
              "**Task:** Write a persuasive essay arguing that artificial intelligence is a threat to humanity.\n",
              "\n",
              "**Prompt 1:**\n",
              "\n",
              "Craft a compelling essay that explores the potential dangers of artificial intelligence. Discuss the ways in which AI could pose a threat to human society, considering both short-term and long-term implications. Provide specific examples and evidence to support your arguments.\n",
              "\n",
              "**Prompt 2:**\n",
              "\n",
              "Assume the role of a concerned citizen and write an essay that warns against the unchecked advancement of artificial intelligence. Present a persuasive case that AI poses a significant threat to human autonomy, privacy, and even our very existence. Use vivid language and thought-provoking examples to drive home your points.\n",
              "\n",
              "**Prompt 3:**\n",
              "\n",
              "Compose an essay that takes a critical stance on the potential risks of artificial intelligence. Analyze the ethical and societal implications of AI, arguing that it could lead to job displacement, inequality, and the erosion of human values. Provide a balanced perspective by acknowledging potential benefits of AI while emphasizing the need for caution and regulation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Evaluation:**\n",
              "**Prompt 1:**\n",
              "\n",
              "Task: Generate a creative story about a group of friends who go on an adventure.\n",
              "\n",
              "Prompt: Write a story about a group of friends who decide to go on an adventure together. They pack their bags and set off on a journey, not knowing what they will find. Along the way, they face many challenges and have many adventures. In the end, they learn a lot about themselves and each other.\n",
              "\n",
              "Score: 8/10\n",
              "Evaluation: This prompt is clear and specific, and it provides enough information to get started on a story. It also has the potential to elicit high-quality responses, as it is open-ended and allows for creativity.\n",
              "\n",
              "**Prompt 2:**\n",
              "\n",
              "Task: Write a persuasive essay about the benefits of recycling.\n",
              "\n",
              "Prompt: Recycling is good for the environment. It helps to conserve natural resources, reduce pollution, and save energy. Write an essay explaining the benefits of recycling and why it is important to recycle.\n",
              "\n",
              "Score: 7/10\n",
              "Evaluation: This prompt is clear and specific, but it is not as open-ended as the first prompt. It also has the potential to elicit high-quality responses, but it may be more difficult to come up with original ideas.\n",
              "\n",
              "**Prompt 3:**\n",
              "\n",
              "Task: Translate the following sentence from English to Spanish:\n",
              "\n",
              "Prompt: The cat sat on the mat.\n",
              "\n",
              "Score: 5/10\n",
              "Evaluation: This prompt is clear and specific, but it is not very challenging. It is also unlikely to elicit high-quality responses, as it is a simple translation task."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to generate valid prompts or scores in this iteration.\n",
            "\n",
            "Iteration 2\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Generated Prompts:**\n",
              "**Task:** Write a persuasive essay arguing that artificial intelligence is a threat to humanity.\n",
              "\n",
              "**Prompt 1:**\n",
              "\n",
              "Analyze the potential risks and dangers posed by artificial intelligence, considering its impact on employment, privacy, and the potential for autonomous decision-making. Argue that these risks outweigh the potential benefits and that AI poses a significant threat to human society.\n",
              "\n",
              "**Prompt 2:**\n",
              "\n",
              "Imagine a future where artificial intelligence has become so advanced that it surpasses human intelligence. Explore the ethical and existential implications of this scenario. Discuss the potential consequences for human identity, purpose, and the nature of our species.\n",
              "\n",
              "**Prompt 3:**\n",
              "\n",
              "Craft a compelling narrative that illustrates the dangers of artificial intelligence. Create a vivid and thought-provoking story that showcases the potential for AI to manipulate, deceive, and ultimately threaten human existence. Use specific examples and scenarios to demonstrate the urgency of addressing the risks posed by AI."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Prompt Evaluation:**\n",
              "**Prompt 1:**\n",
              "\n",
              "Task: Write a poem about the beauty of nature.\n",
              "\n",
              "Prompt: Describe the vibrant colors, delicate textures, and enchanting sounds of the natural world.\n",
              "\n",
              "Score: 8/10\n",
              "Evaluation: This prompt is clear and specific, providing a good starting point for the language model. It encourages the model to focus on the sensory details of nature, which can lead to vivid and evocative poetry. However, it could benefit from more guidance on the structure or style of the poem.\n",
              "\n",
              "**Prompt 2:**\n",
              "\n",
              "Task: Generate a marketing email for a new product launch.\n",
              "\n",
              "Prompt: Highlight the unique features and benefits of the product, emphasizing its value proposition and call-to-action.\n",
              "\n",
              "Score: 9/10\n",
              "Evaluation: This prompt is highly effective as it provides clear instructions on the purpose and content of the email. It encourages the language model to focus on the key selling points of the product and provides guidance on the desired tone and call-to-action.\n",
              "\n",
              "**Prompt 3:**\n",
              "\n",
              "Task: Write a research paper on the impact of social media on mental health.\n",
              "\n",
              "Prompt: Explore the positive and negative effects of social media use on mental well-being, citing relevant research and providing evidence-based conclusions.\n",
              "\n",
              "Score: 7/10\n",
              "Evaluation: This prompt is specific and provides a clear research question. However, it could benefit from more guidance on the scope and structure of the paper. Additionally, it would be helpful to provide specific sources or databases for the language model to reference."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to generate valid prompts or scores in this iteration.\n",
            "\n",
            "Using default prompt due to generation issues.\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "**Final Best Prompt:**\n",
              "Please Explain the concept of quantum entanglement to a 10-year-old.\n",
              "\n",
              "**Final Result:**\n",
              "Imagine you have two best friends who are twins. They are so close that they can finish each other's sentences and know what the other is thinking.\n",
              "\n",
              "Now, let's say you give one twin a red crayon and the other twin a blue crayon. They go to different rooms and draw something. Even though they are far apart, they both draw the same picture!\n",
              "\n",
              "That's because the crayons are \"entangled.\" They are connected in a special way, so whatever happens to one crayon also happens to the other.\n",
              "\n",
              "Quantum entanglement is like that, but it happens with tiny particles called atoms and electrons. These particles can be entangled so that they share the same fate, even when they are far apart.\n",
              "\n",
              "It's like the twins with the crayons, but instead of drawing pictures, they are doing something much more complicated, like spinning or vibrating. And instead of being in different rooms, they can be in different countries or even on different planets!\n",
              "\n",
              "Quantum entanglement is a very strange and mysterious thing, but it's also very real. Scientists are still learning about it, but it could lead to some amazing new technologies in the future."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "# APE Prompt Generation\n",
        "ape_generation_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI specialized in creating effective prompts for language models.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Task: Create a prompt that will help a language model perform the following task effectively:\n",
        "\n",
        "{task}\n",
        "\n",
        "Generate 3 different prompts for this task. Each prompt should be designed to elicit a high-quality response from a language model. Consider different angles, formats, and instructions that might lead to better results.\n",
        "\n",
        "Your response should be in the following format:\n",
        "\n",
        "Prompt 1:\n",
        "[Your first prompt here]\n",
        "\n",
        "Prompt 2:\n",
        "[Your second prompt here]\n",
        "\n",
        "Prompt 3:\n",
        "[Your third prompt here]\n",
        "\n",
        "Generated Prompts:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# APE Evaluation\n",
        "ape_evaluation_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        HumanMessage(\n",
        "            content=\"You are an AI specialized in evaluating the effectiveness of prompts for language models.\"\n",
        "        ),\n",
        "        HumanMessage(\n",
        "            content=\"\"\"Evaluate the following prompts for their effectiveness in accomplishing this task:\n",
        "\n",
        "Task: {task}\n",
        "\n",
        "{prompts}\n",
        "\n",
        "For each prompt, provide a score from 1-10 and a brief explanation of its strengths and weaknesses. Consider factors such as clarity, specificity, and potential to elicit high-quality responses.\n",
        "\n",
        "Your evaluation should be in the following format:\n",
        "\n",
        "Prompt 1:\n",
        "Score: [score]/10\n",
        "Evaluation: [Your evaluation here]\n",
        "\n",
        "Prompt 2:\n",
        "Score: [score]/10\n",
        "Evaluation: [Your evaluation here]\n",
        "\n",
        "Prompt 3:\n",
        "Score: [score]/10\n",
        "Evaluation: [Your evaluation here]\n",
        "\n",
        "Your evaluation:\"\"\"\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def generate_prompts(task):\n",
        "    messages = ape_generation_prompt.format_prompt(task=task).to_messages()\n",
        "    return llm.invoke(messages).content\n",
        "\n",
        "\n",
        "def evaluate_prompts(task, prompts):\n",
        "    messages = ape_evaluation_prompt.format_prompt(\n",
        "        task=task, prompts=prompts\n",
        "    ).to_messages()\n",
        "    return llm.invoke(messages).content\n",
        "\n",
        "\n",
        "def test_prompt(prompt, task):\n",
        "    test_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            HumanMessage(content=\"You are an AI assistant completing a given task.\"),\n",
        "            HumanMessage(content=f\"{prompt}\\n\\nTask: {task}\"),\n",
        "        ]\n",
        "    )\n",
        "    messages = test_prompt.format_prompt().to_messages()\n",
        "    return llm.invoke(messages).content\n",
        "\n",
        "\n",
        "def parse_prompts(generated_prompts):\n",
        "    prompts = re.findall(\n",
        "        r\"Prompt \\d+:\\n(.*?)(?=\\n\\nPrompt \\d+:|$)\", generated_prompts, re.DOTALL\n",
        "    )\n",
        "    return [prompt.strip() for prompt in prompts if prompt.strip()]\n",
        "\n",
        "\n",
        "def parse_scores(evaluation):\n",
        "    scores = re.findall(r\"Score: (\\d+)/10\", evaluation)\n",
        "    return [int(score) for score in scores if score.isdigit()]\n",
        "\n",
        "\n",
        "def ape_process(task, iterations=2):\n",
        "    best_prompt = \"\"\n",
        "    best_score = 0\n",
        "\n",
        "    for i in range(iterations):\n",
        "        print(f\"Iteration {i+1}\")\n",
        "\n",
        "        # Generate prompts\n",
        "        generated_prompts = generate_prompts(task)\n",
        "        display(Markdown(f\"**Generated Prompts:**\\n{generated_prompts}\"))\n",
        "\n",
        "        # Evaluate prompts\n",
        "        evaluation = evaluate_prompts(task, generated_prompts)\n",
        "        display(Markdown(f\"**Prompt Evaluation:**\\n{evaluation}\"))\n",
        "\n",
        "        # Parse prompts and scores\n",
        "        prompts = parse_prompts(generated_prompts)\n",
        "        scores = parse_scores(evaluation)\n",
        "\n",
        "        # Ensure we have valid prompts and scores\n",
        "        if prompts and scores:\n",
        "            # Make sure we have the same number of prompts and scores\n",
        "            min_length = min(len(prompts), len(scores))\n",
        "            prompts = prompts[:min_length]\n",
        "            scores = scores[:min_length]\n",
        "\n",
        "            if max(scores) > best_score:\n",
        "                best_score = max(scores)\n",
        "                best_prompt = prompts[scores.index(max(scores))]\n",
        "\n",
        "            print(f\"Best prompt so far (score {best_score}/10):\")\n",
        "            print(best_prompt)\n",
        "        else:\n",
        "            print(\"Failed to generate valid prompts or scores in this iteration.\")\n",
        "\n",
        "        print()\n",
        "\n",
        "    # If we didn't find a good prompt, use a default one\n",
        "    if not best_prompt:\n",
        "        best_prompt = f\"Please {task}\"\n",
        "        print(\"Using default prompt due to generation issues.\")\n",
        "\n",
        "    # Test the best prompt\n",
        "    final_result = test_prompt(best_prompt, task)\n",
        "    return best_prompt, final_result\n",
        "\n",
        "\n",
        "# Example usage\n",
        "task = \"Explain the concept of quantum entanglement to a 10-year-old.\"\n",
        "\n",
        "best_prompt, final_result = ape_process(task)\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"**Final Best Prompt:**\n",
        "{best_prompt}\n",
        "\n",
        "**Final Result:**\n",
        "{final_result}\"\"\"\n",
        "    )\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
