{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1GbDDATVAeQ_kJ4MXiwzMmeWQlAAB8HCY?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "#\t What is Re-ranking?\n",
        "\n",
        "Re-ranking in RAG is a critical process that refines and reorders the initially retrieved information before it's fed into a generative AI model. It acts as a smart filter, ensuring that the most relevant and high-quality content is prioritized for the generation task.\n",
        "\n",
        "## Key aspects:\n",
        "  1. Relevance optimization: Improves the quality of information used by the LLM.\n",
        "  2. Intelligent sorting: Uses advanced algorithms to reassess and reorder retrieved passages.\n",
        "  3. Context consideration: Takes into account the query intent and user context.\n",
        "  4. Integration point: Sits between retrieval and generation components in the RAG pipeline.\n",
        "\n",
        "By effectively re-ranking retrieved information, RAG systems can significantly enhance the accuracy, relevance, and overall quality of the generated AI responses.\n",
        "\n",
        "#  Re-ranking RAG Implementation:\n",
        "\n",
        "1. **Initial Retrieval:** We use the Chroma vector store's retriever to get relevant documents.\n",
        "2. **Re-ranking:** We employ FlashRank (via FlashrankRerank) to re-rank the initially retrieved documents.\n",
        "3. **Context Formation:** We combine the top re-ranked documents into a single context string.\n",
        "4. **Response Generation:** Using the Gemini Pro model, we generate a final response based on the re-ranked context and the query.\n",
        "\n",
        "# 锔 Setup\n",
        "\n",
        "1. **[LLM](https://deepmind.google/technologies/gemini/pro/):** Google's free gemini-pro api endpoint ([Google's API Key](https://console.cloud.google.com/apis/credentials))\n",
        "2. **[Vector Store](https://www.pinecone.io/learn/vector-database/):** [ChromaDB](https://www.trychroma.com/)\n",
        "3. **[Embedding Model](https://qdrant.tech/articles/what-are-embeddings/):** [nomic-embed-text-v1.5](https://www.nomic.ai/blog/posts/nomic-embed-text-v1)\n",
        "4. **[LLM Framework](https://python.langchain.com/v0.2/docs/introduction/):** LangChain\n",
        "5. **[Huggingface API Key](https://huggingface.co/settings/tokens)**"
      ],
      "metadata": {
        "id": "HxxOWz9dpsYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-chroma==0.1.2 \\\n",
        "     langchain-community==0.2.10 \\\n",
        "     langchain-huggingface==0.0.3 \\\n",
        "     einops==0.8.0 \\\n",
        "     flashrank==0.2.8"
      ],
      "metadata": {
        "id": "ShxTNxM5gqtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6012244-c485-4c6e-eaba-d2e37be8821e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import related libraries related to Langchain, HuggingfaceEmbedding"
      ],
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors.flashrank_rerank import FlashrankRerank\n",
        "from langchain.schema import HumanMessage, SystemMessage"
      ],
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "GT55z5AkhyOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Google API Key. You can create Google API key at following lin\n",
        "\n",
        "[Google Gemini-Pro API Creation Link](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "[YouTube Video](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ],
      "metadata": {
        "id": "F6UeDlrgqI2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "2da4d12b-1472-438f-b294-f1fafcf59d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Huggingface API Key. You can create Huggingface API key at following lin\n",
        "\n",
        "[Higgingface API Creation Link](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1dLpYboqeIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6scBGZlhpG",
        "outputId": "02e9e725-4f64-4b15-9ecc-6801f8e1bde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    # Iterate through each document and format the output\n",
        "    for i, d in enumerate(docs):\n",
        "        print(f\"{'-' * 50}\\nDocument {i + 1}:\")\n",
        "        print(f\"Content:\\n{d.page_content}\\n\")\n",
        "        print(\"Metadata:\")\n",
        "        for key, value in d.metadata.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print(f\"{'-' * 50}\")  # Final separator for clarity\n",
        "\n",
        "# Example usage\n",
        "# Assuming `docs` is a list of Document objects"
      ],
      "metadata": {
        "id": "VC4pm6cPyhue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load and preprocess data code"
      ],
      "metadata": {
        "id": "ywIflM5huW6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_process_data(url):\n",
        "    loader = WebBaseLoader(url)\n",
        "    data = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "\n",
        "    for idx, chunk in enumerate(chunks):\n",
        "        chunk.metadata[\"id\"] = idx\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "TysZpDqRuXM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create vector store and BM25 retriever"
      ],
      "metadata": {
        "id": "_DCY2IUsucGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "    vectorstore = Chroma.from_documents(chunks, embeddings)\n",
        "    return vectorstore"
      ],
      "metadata": {
        "id": "Mr-l3FWKuccs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Re-ranking RAG related code\n",
        "\n",
        "1. **Initial Retrieval:** We use the Chroma vector store's retriever to get relevant documents.\n",
        "2. **Re-ranking:** We employ FlashRank (via FlashrankRerank) to re-rank the initially retrieved documents.\n",
        "3. **Context Formation:** We combine the top re-ranked documents into a single context string.\n",
        "4. **Response Generation:** Using the Gemini Pro model, we generate a final response based on the re-ranked context and the query."
      ],
      "metadata": {
        "id": "4J3zqczYwbig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reranking_rag(query, vectorstore, llm):\n",
        "    # Set up the document compressor using FlashRank\n",
        "    compressor = FlashrankRerank()\n",
        "\n",
        "    # Create a compression retriever\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "\n",
        "    # Retrieve and re-rank documents\n",
        "    docs = compression_retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "    # Generate response\n",
        "    prompt = f\"{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"final_answer\": response.content,\n",
        "        \"retrieval_method\": \"Re-ranking with FlashRank\"\n",
        "    }"
      ],
      "metadata": {
        "id": "dSFTMURrwbss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Create chunk of web data to Chroma Vector Store"
      ],
      "metadata": {
        "id": "Yz5W3MoNvQUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the gemini-pro language model with specified settings (Change temeprature  and other parameters as per your requirement)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, safety_settings={\n",
        "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        },)\n",
        "\n",
        "# Load and process data\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "chunks = load_and_process_data(url)\n",
        "\n",
        "# Create vector store\n",
        "vectorstore  = create_vector_store(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA02yiM7vQiD",
        "outputId": "a912ef05-8983-4aa7-ca1d-c30642cc5caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.e55a7d4324f65581af5f483e830b80f34680e8ff.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Run Re-ranking RAG\n",
        "\n",
        "This implementation shows the key parts of Re-ranking RAG:\n",
        "\n",
        "1. Initial broad retrieval of potentially relevant documents\n",
        "2. Re-ranking of retrieved documents to prioritize the most relevant ones\n",
        "3. Generation of a response using the re-ranked and refined context"
      ],
      "metadata": {
        "id": "R2cbajGgESjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example queries\n",
        "queries = [\n",
        "        \"What are the main applications of artificial intelligence in healthcare?\",\n",
        "        \"Explain the concept of machine learning and its relationship to AI.\",\n",
        "        \"Discuss the ethical implications of AI in decision-making processes.\"\n",
        "    ]\n",
        "\n",
        "# Run Re-ranking RAG for each query\n",
        "for query in queries:\n",
        "  print(f\"\\nQuery: {query}\")\n",
        "  result = reranking_rag(query, vectorstore, llm)\n",
        "  print(\"Final Answer:\")\n",
        "  print(result[\"final_answer\"])\n",
        "  print(\"\\nRetrieval Method:\")\n",
        "  print(result[\"retrieval_method\"])\n"
      ],
      "metadata": {
        "id": "LSjCJTQQ2mZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2305f6-baf9-4ea8-bc34-85d8ca430586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: What are the main applications of artificial intelligence in healthcare?\n",
            "Final Answer:\n",
            "The main applications of artificial intelligence in healthcare are to increase patient care and quality of life.\n",
            "\n",
            "Retrieval Method:\n",
            "Re-ranking with FlashRank\n",
            "\n",
            "Query: Explain the concept of machine learning and its relationship to AI.\n",
            "Final Answer:\n",
            "Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning. Machine learning is a subfield of AI that focuses on developing algorithms that can learn from data. These algorithms can be used to solve a wide variety of problems, such as image recognition, natural language processing, and speech recognition.\n",
            "\n",
            "Retrieval Method:\n",
            "Re-ranking with FlashRank\n",
            "\n",
            "Query: Discuss the ethical implications of AI in decision-making processes.\n",
            "Final Answer:\n",
            "The provided text does not discuss the ethical implications of AI in decision-making processes.\n",
            "\n",
            "Retrieval Method:\n",
            "Re-ranking with FlashRank\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstrate retrieval and re-ranking"
      ],
      "metadata": {
        "id": "owwiWOT-EcCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_query = \"Explain the concept of machine learning and its relationship to AI\"\n",
        "print(f\"\\nDemonstration Query: {demo_query}\")\n",
        "\n",
        "# Retrieve documents before re-ranking\n",
        "docs_before = vectorstore.similarity_search(demo_query)\n",
        "print(\"\\nDocuments before re-ranking:\")\n",
        "pretty_print_docs(docs_before)\n",
        "\n",
        "# Retrieve and re-rank documents\n",
        "compressor = FlashrankRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "docs_after = compression_retriever.get_relevant_documents(demo_query)\n",
        "print(\"\\nDocuments after re-ranking:\")\n",
        "pretty_print_docs(docs_after)"
      ],
      "metadata": {
        "id": "j1RCFCbS5KmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906e10c0-1842-44b8-e253-ba52cf3fde92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Demonstration Query: Explain the concept of machine learning and its relationship to AI\n",
            "\n",
            "Documents before re-ranking:\n",
            "--------------------------------------------------\n",
            "Document 1:\n",
            "Content:\n",
            "Learning\n",
            "Machine learning is the study of programs that can improve their performance on a given task automatically.[46] It has been a part of AI from the beginning.[e]\n",
            "\n",
            "Metadata:\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 2:\n",
            "Content:\n",
            "Learning\n",
            "Machine learning is the study of programs that can improve their performance on a given task automatically.[46] It has been a part of AI from the beginning.[e]\n",
            "\n",
            "Metadata:\n",
            "  id: 39\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 3:\n",
            "Content:\n",
            "No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI\n",
            "\n",
            "Metadata:\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "Document 4:\n",
            "Content:\n",
            "No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI\n",
            "\n",
            "Metadata:\n",
            "  id: 239\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "--------------------------------------------------\n",
            "\n",
            "Documents after re-ranking:\n",
            "--------------------------------------------------\n",
            "Document 1:\n",
            "Content:\n",
            "No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI\n",
            "\n",
            "Metadata:\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "  relevance_score: 0.9994165897369385\n",
            "--------------------------------------------------\n",
            "Document 2:\n",
            "Content:\n",
            "No established unifying theory or paradigm has guided AI research for most of its history.[z] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI\n",
            "\n",
            "Metadata:\n",
            "  id: 239\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "  relevance_score: 0.9994165897369385\n",
            "--------------------------------------------------\n",
            "Document 3:\n",
            "Content:\n",
            "Learning\n",
            "Machine learning is the study of programs that can improve their performance on a given task automatically.[46] It has been a part of AI from the beginning.[e]\n",
            "\n",
            "Metadata:\n",
            "  language: en\n",
            "  source: https://en.wikipedia.org/wiki/Artificial_intelligence\n",
            "  title: Artificial intelligence - Wikipedia\n",
            "  relevance_score: 0.9991251826286316\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}