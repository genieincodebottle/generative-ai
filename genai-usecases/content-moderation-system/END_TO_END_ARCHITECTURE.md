# Content Moderation System - Complete End-to-End Architecture

## ğŸ¯ System Overview

This document provides a comprehensive view of the entire content moderation platform, from user story submission to final resolution, including all components, databases, and workflows. It covers both the high-level system architecture and detailed backend module implementations.

---

## ğŸ“‘ Table of Contents

1. [ğŸ“Š High Level Architecture](#ğŸ“Š-high-level-architecture-diagram)
2. [ğŸ”§ Backend Module Overview](#ğŸ”§-backend-module-overview)
3. [ğŸ¤– AI/ML Models & Embedding Systems](#ğŸ¤–-aiml-models--embedding-systems)
4. [ğŸ”„ Complete User Story Submission Flow](#ğŸ”„-complete-user-story-submission-flow)
5. [ğŸ“ Sequence Diagrams](#ğŸ“-sequence-diagrams)
6. [ğŸ—„ï¸ Database Schema Details](#ğŸ—„ï¸-database-schema-details)
7. [ğŸ“ˆ System Performance Metrics](#ğŸ“ˆ-system-performance-metrics)
8. [ğŸ” Security & Privacy](#ğŸ”-security--privacy)
9. [ğŸš€ Deployment Architecture](#ğŸš€-deployment-architecture)
10. [âš™ï¸ Configuration](#âš™ï¸-configuration)
11. [ğŸ“Š Complete Flow Summary](#ğŸ“Š-complete-flow-summary)

---

## ğŸ“Š High Level Architecture Diagram

![End-to-End Architecture](images/end-to-end-architecture-content-moderation.png)
---

## ğŸ”§ Backend Module Overview

### ğŸ“ Backend Structure

```
backend/
â”œâ”€â”€ main.py                          # FastAPI application & REST endpoints
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ models.py                # Data models & type definitions
â”‚   â”‚   â””â”€â”€ llm_schemas.py           # Pydantic schemas for LLM responses
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ agents.py                # 6 AI agent implementations
â”‚   â”‚   â”œâ”€â”€ workflow.py              # LangGraph workflow orchestration
â”‚   â”‚   â”œâ”€â”€ reasoning.py             # ReAct decision loop logic
â”‚   â”‚   â””â”€â”€ tool_manager.py          # Tool management for agents
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ moderation_db.py         # Content moderation database operations
â”‚   â”‚   â””â”€â”€ auth_db.py               # User authentication database
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ memory.py                # ChromaDB memory manager
â”‚   â”‚   â”œâ”€â”€ agent_episodic_memory.py # Episode-level memory
â”‚   â”‚   â”œâ”€â”€ agent_semantic_memory.py # Pattern learning
â”‚   â”‚   â””â”€â”€ learning_tracker.py      # Decision outcome tracking
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ ml_classifier.py         # ML toxicity detection models
â”‚   â”‚   â”œâ”€â”€ keyword_detectors.py     # Keyword-based detection
â”‚   â”‚   â””â”€â”€ guardrails.py            # Safety guardrails
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ tools.py                 # Utility functions
â”‚       â”œâ”€â”€ evaluation.py            # Model evaluation metrics
â”‚       â””â”€â”€ observability.py         # Logging and monitoring
```

---

### 1. Core Modules (`src/core/`)

#### `models.py`
**Purpose**: Central data model definitions

**Key Components**:
- `ContentState` (TypedDict): Main state object passed through all agents
- `AgentDecision` (Dataclass): Individual agent decision record
- `UserProfile` (Dataclass): User reputation and history
- Enums: `ContentStatus`, `DecisionType`, `ToxicityLevel`, `PolicyCategory`

**Usage**: Every module imports types from here for type safety

---

#### `llm_schemas.py`
**Purpose**: Pydantic schemas for structured LLM outputs

**Key Components**:
- `TopicExtractionResponse`: Structured topic extraction
- `ToxicityAnalysisResponse`: Toxicity detection output
- `parse_llm_response()`: Parse and validate LLM JSON responses
- `create_structured_prompt()`: Format prompts for structured outputs

**Usage**: Agents use these to get reliable, parseable outputs from LLMs

---

### 2. Agent Modules (`src/agents/`)

#### `agents.py`
**Purpose**: Implementation of 6 specialized AI agents + Fast Mode agent

**Key Agents**:
1. **Content Analysis Agent**: Analyzes sentiment, topics, categories
2. **Toxicity Detection Agent**: Detects toxic language and hate speech
3. **Policy Violation Agent**: Checks community guidelines
4. **ReAct Decision Loop Agent**: Synthesizes all agent decisions
5. **HITL Checkpoint Agent**: Evaluates need for human review
6. **User Reputation Agent**: Scores user reputation and risk
7. **Action Enforcement Agent**: Executes moderation actions
8. **Appeal Review Agent**: Handles user appeals
9. **Fast Mode Agent**: Single-pass moderation for short comments

**Usage**: Called by [workflow.py](backend/src/agents/workflow.py) in sequence based on routing logic

---

#### `workflow.py`
**Purpose**: LangGraph workflow orchestration

**Key Functions**:
- `create_moderation_workflow()`: Builds the state graph
- `process_content()`: Executes workflow for content
- `resume_from_hitl()`: Resumes paused workflow after human review
- `should_use_fast_mode()`: Determines fast mode eligibility
- Routing functions: Direct flow between agents based on state

**Usage**: [main.py](backend/main.py) creates workflow on startup, calls it for each content submission

---

#### `reasoning.py`
**Purpose**: ReAct (Reason-Act-Observe) decision synthesis logic

**Key Functions**:
- Think phase: Analyzes all agent decisions
- Act phase: Makes final consolidated decision
- Observe phase: Evaluates HITL triggers and confidence
- Consensus calculation: Weights agent agreements

**Usage**: Called by ReAct Decision Loop Agent to synthesize multi-agent decisions

---

### 3. Database Modules (`src/database/`)

#### `moderation_db.py`
**Purpose**: All content moderation database operations

**Key Tables**:
- `content_submissions`: All moderated content
- `stories`: Story posts with visibility
- `story_comments`: Comments on stories
- `agent_executions`: Individual agent decision logs
- `policy_violations`: Policy violation records
- `user_actions`: Warnings, suspensions, bans

**Key Operations**:
- `store_content_submission()`: Save content and moderation results
- `get_content_by_id()`: Retrieve content details
- `update_story_moderation()`: Update story approval status
- `get_user_violations()`: Get user violation history

**Usage**: Called by agents and API endpoints to persist/retrieve data

---

#### `auth_db.py`
**Purpose**: User authentication and management

**Key Tables**:
- `users`: User accounts with credentials
- `user_profiles`: Extended profile information
- `sessions`: Active user sessions

**Key Operations**:
- `create_user()`: Register new user
- `authenticate_user()`: Verify credentials
- `update_user_role()`: Change user permissions
- `get_all_users()`: Admin user management

**Usage**: Called by authentication endpoints and user management APIs

---

### 4. Memory Modules (`src/memory/`)

#### `memory.py`
**Purpose**: ChromaDB-based memory management

**Key Collections**:
- `moderation_decisions`: Historical decisions
- `flagged_patterns`: Patterns of flagged content
- `user_violations`: User violation history

**Key Operations**:
- `store_moderation_decision()`: Save decision for learning
- `retrieve_similar_content()`: Find similar past content
- `get_user_violation_history()`: Get user's past violations

**Usage**: Agents store and retrieve decisions for pattern learning

---

#### `learning_tracker.py`
**Purpose**: Track decision outcomes and learn from appeals

**Key Functions**:
- `record_decision()`: Log agent decision with metadata
- `update_outcome()`: Update when decision is appealed
- `get_success_rate()`: Calculate agent accuracy
- `analyze_patterns()`: Identify learning opportunities

**Usage**: Called by agents to enable continuous improvement from feedback

---

### 5. ML Modules (`src/ml/`)

#### `ml_classifier.py`
**Purpose**: ML-based toxicity detection using transformer models

**Supported Models**:
- DistilBERT Toxic (default)
- HateBERT (hate speech specialist)
- Toxic BERT (multi-category)
- RoBERTa Hate (robust detection)

**Key Functions**:
- `load_models()`: Initialize transformer models
- `predict_toxicity()`: Get toxicity predictions
- `ensemble_predict()`: Combine multiple model predictions

**Usage**: Optional - used by Toxicity Detection Agent if `USE_ML_MODELS=true`

---

#### `keyword_detectors.py`
**Purpose**: Fast keyword-based toxicity detection (default)

**Key Functions**:
- `keyword_toxicity_detection()`: Pattern matching for toxic phrases
- `keyword_hate_speech_detection()`: Detect hate speech patterns
- Built-in pattern libraries for common violations

**Usage**: Default toxicity detection method (fast, no ML dependencies)

---

#### `guardrails.py`
**Purpose**: Safety guardrails for AI agent behavior

**Key Features**:
- Loop detection (max 10 iterations)
- Hallucination detection (contradiction checking)
- Cost budget tracking
- Execution time limits
- Consistency validation across agents

**Usage**: Wraps all agent functions to ensure safe, bounded execution

---

### 6. Main Application (`main.py`)

#### `main.py`
**Purpose**: FastAPI REST API server

**Key Endpoints**:

**Authentication**:
- `POST /api/auth/login`: User login
- `POST /api/auth/register`: New user registration
- `PUT /api/auth/password`: Update password

**Content Submission**:
- `POST /api/stories/submit`: Submit story for moderation
- `POST /api/stories/{id}/comments`: Submit comment

**Moderation**:
- `GET /api/hitl/queue`: Get HITL review queue
- `POST /api/hitl/review/{id}`: Submit human decision
- `POST /api/content/review`: Manual content review

**Appeals**:
- `POST /api/appeals/submit`: Submit appeal
- `POST /api/appeals/{id}/review`: Review appeal

**Analytics**:
- `GET /api/statistics`: System statistics
- `GET /api/analytics/overview`: Moderation analytics

**Usage**: Entry point for all frontend interactions

---

## ğŸ¤– AI/ML Models & Embedding Systems

This section clarifies the different AI models and embedding systems used throughout the platform.

### ğŸ§  Google Gemini LLM (Primary AI Engine)

**Model**: `gemini-1.5-flash` via Google Generative AI API

**Usage Locations**:
1. **Content Analysis Agent** ([agents.py:58](content-moderation-system/backend/src/agents/agents.py#L58))
   - Topic extraction
   - Sentiment analysis
   - Content categorization
   - Uses structured prompts with JSON output parsing

2. **Toxicity Detection Agent** ([agents.py:178](content-moderation-system/backend/src/agents/agents.py#L178))
   - Language-aware toxicity assessment
   - Contextual hate speech detection
   - Cultural nuance understanding

3. **Policy Compliance Agent** ([agents.py:298](content-moderation-system/backend/src/agents/agents.py#L298))
   - Community guidelines checking
   - Policy violation categorization
   - Severity level determination

4. **Context Analysis Agent** ([agents.py:418](content-moderation-system/backend/src/agents/agents.py#L418))
   - User history analysis
   - Behavioral pattern detection
   - Risk level assessment

5. **Decision Synthesis Agent** ([agents.py:538](content-moderation-system/backend/src/agents/agents.py#L538))
   - ReAct reasoning loop (Think-Act-Observe)
   - Multi-agent decision aggregation
   - Final decision with confidence scoring

6. **Appeal Review Agent** ([agents.py:658](content-moderation-system/backend/src/agents/agents.py#L658))
   - Original vs updated content comparison
   - Appeal justification evaluation
   - Decision reversal recommendations

**API Configuration**:
```python
# Environment variable
GOOGLE_API_KEY=your_api_key_here

# LLM Client initialization
from langchain_google_genai import ChatGoogleGenerativeAI
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.7,
    google_api_key=os.getenv("GOOGLE_API_KEY")
)
```

**Cost per Request**:
- Fast Mode (1 LLM call): ~$0.0002
- Full Pipeline (4 LLM calls): ~$0.0016
- Appeal Review (2 LLM calls): ~$0.0005

---

### ğŸ” ChromaDB Vector Embeddings (Memory System)

**Embedding Function**: ChromaDB Default (sentence-transformers/all-MiniLM-L6-v2)

**NOT using Google Embedding API** - The memory system uses ChromaDB's built-in embedding function for:
- Fast local embedding generation
- No API call overhead
- Cost-free vector operations
- Consistent semantic search

**Implementation** ([memory.py:45-60](content-moderation-system/backend/src/memory/memory.py#L45-L60)):
```python
from chromadb import PersistentClient, Settings

# Initialize ChromaDB with default embedding function
self.client = chromadb.PersistentClient(
    path=persist_directory,
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# Collections use ChromaDB default embeddings
self.decisions_collection = self.client.get_or_create_collection(
    name="moderation_decisions",
    metadata={"description": "Historical moderation decisions"}
    # No embedding_function specified = uses ChromaDB default
)
```

**Vector Collections**:

1. **moderation_decisions**
   - **Documents**: Historical moderation decisions and content
   - **Embeddings**: 384-dim vectors (sentence-transformers)
   - **Purpose**: Find similar past content for consistency
   - **Query Example**: "Find content similar to current submission"

2. **flagged_patterns**
   - **Documents**: Patterns of flagged/violating content
   - **Embeddings**: 384-dim vectors
   - **Purpose**: Pattern matching and learning
   - **Query Example**: "Check if content matches known violation patterns"

3. **user_violations**
   - **Documents**: User violation history and context
   - **Embeddings**: 384-dim vectors
   - **Purpose**: User risk scoring and repeat offender detection
   - **Query Example**: "Retrieve user's historical violations"

**Embedding Dimensions**: 384 (all-MiniLM-L6-v2) or 768 (depending on ChromaDB version)

**Performance**:
- Embedding Generation: <10ms locally
- Similarity Search: <50ms for typical queries
- No API latency or costs

---

### ğŸ›¡ï¸ ML Toxicity Detection Models (Optional)

**Framework**: HuggingFace Transformers

**Activation**: Set `USE_ML_MODELS=true` in environment variables

**Supported Models** ([ml_classifier.py:25-45](content-moderation-system/backend/src/ml/ml_classifier.py#L25-L45)):

1. **DistilBERT Toxic** (default)
   - Model: `unitary/toxic-bert`
   - Categories: toxic, severe_toxic, obscene, threat, insult, identity_hate
   - Speed: Fast (~50-100ms inference)

2. **HateBERT**
   - Model: `GroNLP/hateBERT`
   - Specialization: Hate speech detection
   - Performance: High precision on hate speech

3. **Toxic BERT**
   - Model: `unitary/multilingual-toxic-xlm-roberta`
   - Features: Multi-language support
   - Languages: 100+ languages

4. **RoBERTa Hate**
   - Model: `facebook/roberta-hate-speech-dynabench-r4-target`
   - Features: Robust against adversarial examples
   - Use case: Production-grade hate detection

**Default Mode**: Keyword-based detection ([keyword_detectors.py](content-moderation-system/backend/src/ml/keyword_detectors.py))
- No ML model loading required
- Fast pattern matching (~1-5ms)
- Built-in toxic phrase libraries
- Good for basic toxicity screening

---

### ğŸ“Š AI/ML Architecture Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTENT SUBMISSION                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   Fast Mode Check     â”‚
         â”‚  (length < 200 chars) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚
        â–¼                 â–¼
   [Fast Mode]      [Full Pipeline]
        â”‚                 â”‚
        â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Gemini â”‚  â”‚  6 Agent Pipeline    â”‚
â”‚ 1 LLM call    â”‚  â”‚  Google Gemini LLM   â”‚
â”‚ Quick decisionâ”‚  â”‚  4 LLM calls total   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â”‚                 â–¼
        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚ ChromaDB Query      â”‚
        â”‚          â”‚ (Default Embeddings)â”‚
        â”‚          â”‚ - Similar content   â”‚
        â”‚          â”‚ - Pattern matching  â”‚
        â”‚          â”‚ - User violations   â”‚
        â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â”‚                 â–¼
        â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚          â”‚ ML Toxicity         â”‚
        â”‚          â”‚ (Optional)          â”‚
        â”‚          â”‚ HuggingFace Models  â”‚
        â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Final Decisionâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Store in ChromaDB â”‚
         â”‚ (Memory Learning) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Distinctions**:
- **Google Gemini**: Used for reasoning, analysis, and decision-making (LLM tasks)
- **ChromaDB Embeddings**: Used for vector storage and similarity search (memory tasks)
- **HuggingFace Models**: Optional ML-based toxicity detection (classification tasks)
- **Keyword Detection**: Default fast toxicity screening (pattern matching)

---

### Component Interaction Diagrams

#### Database Interactions

```
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Agents     â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ Read/Write
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ moderation_db.py                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ Operations:                            â”‚
â”‚ â€¢ store_content_submission()           â”‚
â”‚ â€¢ update_story_moderation()            â”‚
â”‚ â€¢ get_user_violations()                â”‚
â”‚ â€¢ record_agent_execution()             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ SQL Operations
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQLite Database                        â”‚
â”‚ databases/moderation_data.db           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ Tables:                                â”‚
â”‚ â€¢ content_submissions                  â”‚
â”‚ â€¢ stories                              â”‚
â”‚ â€¢ story_comments                       â”‚
â”‚ â€¢ agent_executions                     â”‚
â”‚ â€¢ policy_violations                    â”‚
â”‚ â€¢ user_actions                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Memory System Interactions

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Agents     â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ Store/Retrieve Decisions
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ memory.py (ModerationMemoryManager)    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ Methods:                               â”‚
â”‚ â€¢ store_moderation_decision()          â”‚
â”‚ â€¢ retrieve_similar_content()           â”‚
â”‚ â€¢ get_user_violation_history()         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ Vector Operations
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ChromaDB                               â”‚
â”‚ databases/chroma_moderation_db/        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ Collections:                           â”‚
â”‚ â€¢ moderation_decisions                 â”‚
â”‚ â€¢ flagged_patterns                     â”‚
â”‚ â€¢ user_violations                      â”‚
â”‚                                        â”‚
â”‚ Used for:                              â”‚
â”‚ â€¢ Finding similar past content         â”‚
â”‚ â€¢ Learning from patterns               â”‚
â”‚ â€¢ Improving agent decisions            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### LLM Interactions

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Agents     â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â”‚ LLM Calls
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Gemini API                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚ Used for:                              â”‚
â”‚ â€¢ Content analysis                     â”‚
â”‚ â€¢ Topic extraction                     â”‚
â”‚ â€¢ Policy checking                      â”‚
â”‚ â€¢ ReAct synthesis                      â”‚
â”‚ â€¢ Appeal review                        â”‚
â”‚ â€¢ Action reason generation             â”‚
â”‚                                        â”‚
â”‚ Full Pipeline: 6-8 LLM calls           â”‚
â”‚ Fast Mode: 1 LLM call                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Complete User Story Submission Flow

### Story 1: User Submits a Story (Happy Path - Auto-Approved)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: USER SUBMISSION                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User Login
   â”œâ”€> User enters credentials on frontend
   â”œâ”€> POST /api/auth/login
   â”‚   Body: { username: "john_doe", password: "****" }
   â”‚
   â”œâ”€> Auth Database Query
   â”‚   â””â”€> SELECT * FROM users WHERE username=? AND password_hash=?
   â”‚
   â””â”€> Response: { session_token: "abc123...", user_info: {...} }

2. Story Submission
   â”œâ”€> User writes story in text editor
   â”œâ”€> User clicks "Submit Story"
   â”œâ”€> POST /api/stories/submit
   â”‚   Headers: { Authorization: "Bearer abc123..." }
   â”‚   Body: {
   â”‚     title: "My Amazing Day at the Park",
   â”‚     content: "I had a wonderful time at the park today...",
   â”‚     user_id: "user_12345",
   â”‚     author_name: "John Doe"
   â”‚   }
   â”‚
   â””â”€> API validates token and user

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: CONTENT PROCESSING INITIALIZATION                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Workflow Initialization
   â”œâ”€> Generate content_id: "story_1733334567_abc"
   â”‚
   â”œâ”€> Create ContentState object:
   â”‚   {
   â”‚     content_id: "story_1733334567_abc",
   â”‚     content_type: "story",
   â”‚     content_text: "I had a wonderful time...",
   â”‚     user_id: "user_12345",
   â”‚     author_name: "John Doe",
   â”‚     metadata: {
   â”‚       title: "My Amazing Day at the Park",
   â”‚       submission_timestamp: "2025-12-04T10:30:00Z"
   â”‚     },
   â”‚     user_profile: {
   â”‚       reputation_score: 0.85,
   â”‚       total_violations: 0,
   â”‚       account_age_days: 365
   â”‚     },
   â”‚     status: "pending",
   â”‚     decisions: []
   â”‚   }
   â”‚
   â””â”€> Invoke LangGraph Workflow: process_content(state)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: MULTI-AGENT AI ANALYSIS                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Entry Router Decision
   â”œâ”€> Check: content_type == "story"
   â”œâ”€> Check: ENABLE_FAST_MODE = true
   â”œâ”€> Check: len(content) = 450 chars > 200 (FAST_MODE_MAX_LENGTH)
   â”‚
   â””â”€> Route: "content_analysis" (Full Pipeline)
       âš ï¸  Story too long for fast mode - using full analysis

5. Agent 1: Content Analysis Agent
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~2 seconds                             â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1. Call Google Gemini API (LLM)                        â”‚
   â”‚    Prompt: "Analyze this content for sentiment..."     â”‚
   â”‚    Response: {                                         â”‚
   â”‚      sentiment: "positive",                            â”‚
   â”‚      topics: ["leisure", "outdoor activities"],        â”‚
   â”‚      category: "personal_story"                        â”‚
   â”‚    }                                                   â”‚
   â”‚                                                        â”‚
   â”‚ 2. Query ChromaDB for Similar Content                  â”‚
   â”‚    â””â”€> retrieve_similar_content(text)                  â”‚
   â”‚        Found: 3 similar approved stories               â”‚
   â”‚                                                        â”‚
   â”‚ 3. Initial Toxicity Check                              â”‚
   â”‚    â””â”€> keyword_toxicity_detection(text)                â”‚
   â”‚        Result: toxicity_score = 0.05 (very low)        â”‚
   â”‚                                                        â”‚
   â”‚ 4. Make Decision                                       â”‚
   â”‚    Decision: APPROVE                                   â”‚
   â”‚    Confidence: 0.92                                    â”‚
   â”‚    Reasoning: "Positive personal story, no red flags"  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â””â”€> Update ContentState.decisions[]
       â””â”€> Add AgentDecision record

6. Agent 2: Toxicity Detection Agent
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~1.5 seconds                           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1. Keyword-Based Detection (Default)                   â”‚
   â”‚    â””â”€> keyword_toxicity_detection(text)                â”‚
   â”‚        â€¢ Profanity score: 0.0                          â”‚
   â”‚        â€¢ Hate speech: None detected                    â”‚
   â”‚        â€¢ Harassment patterns: None                     â”‚
   â”‚                                                        â”‚
   â”‚ 2. Calculate Overall Toxicity                          â”‚
   â”‚    toxicity_score = 0.03                               â”‚
   â”‚    toxicity_level = "none"                             â”‚
   â”‚                                                        â”‚
   â”‚ 3. Make Decision                                       â”‚
   â”‚    Decision: APPROVE                                   â”‚
   â”‚    Confidence: 0.95                                    â”‚
   â”‚    Reasoning: "No toxic content detected"              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â””â”€> Update ContentState

7. Agent 3: Policy Violation Agent
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~2 seconds                             â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1. Call Google Gemini API                              â”‚
   â”‚    Prompt: "Check for policy violations..."            â”‚
   â”‚    Community Guidelines:                               â”‚
   â”‚    â€¢ No hate speech                                    â”‚
   â”‚    â€¢ No harassment                                     â”‚
   â”‚    â€¢ No spam/misinformation                            â”‚
   â”‚    â€¢ No sexual/violent content                         â”‚
   â”‚    â€¢ No illegal activity                               â”‚
   â”‚                                                        â”‚
   â”‚ 2. LLM Analysis Result                                 â”‚
   â”‚    {                                                   â”‚
   â”‚      violations: [],                                   â”‚
   â”‚      violation_severity: "none",                       â”‚
   â”‚      explanation: "Content follows all guidelines"     â”‚
   â”‚    }                                                   â”‚
   â”‚                                                        â”‚
   â”‚ 3. Make Decision                                       â”‚
   â”‚    Decision: APPROVE                                   â”‚
   â”‚    Confidence: 0.88                                    â”‚
   â”‚    Reasoning: "No policy violations found"             â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â””â”€> Update ContentState

8. Agent 4: ReAct Decision Loop (Synthesis)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~2.5 seconds                           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ THINK PHASE:                                           â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ 1. Gather all agent decisions                          â”‚
   â”‚    â€¢ Agent 1: APPROVE (conf: 0.92)                     â”‚
   â”‚    â€¢ Agent 2: APPROVE (conf: 0.95)                     â”‚
   â”‚    â€¢ Agent 3: APPROVE (conf: 0.88)                     â”‚
   â”‚                                                        â”‚
   â”‚ 2. Calculate consensus                                 â”‚
   â”‚    Consensus = 100% (all agree on APPROVE)             â”‚
   â”‚    Avg Confidence = (0.92 + 0.95 + 0.88) / 3 = 0.917   â”‚
   â”‚                                                        â”‚
   â”‚ 3. Identify conflicts                                  â”‚
   â”‚    Conflicts: None                                     â”‚
   â”‚                                                        â”‚
   â”‚ ACT PHASE:                                             â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ 4. Call LLM for synthesis                              â”‚
   â”‚    Prompt: "Synthesize final decision..."              â”‚
   â”‚    All agents recommend approval                       â”‚
   â”‚                                                        â”‚
   â”‚ 5. Generate final decision                             â”‚
   â”‚    Final Decision: APPROVE                             â”‚
   â”‚    Final Confidence: 0.92                              â”‚
   â”‚                                                        â”‚
   â”‚ OBSERVE PHASE:                                         â”‚
   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
   â”‚ 6. Evaluate HITL triggers (8 conditions)               â”‚
   â”‚    âœ— Low confidence (<70%)? No (92%)                  â”‚
   â”‚    âœ— High severity violation? No                      â”‚
   â”‚    âœ— Conflicting decisions (<60% consensus)? No       â”‚
   â”‚    âœ— High-profile user? No                            â”‚
   â”‚    âœ— Sensitive content? No                            â”‚
   â”‚    âœ— Potential false positive? No                     â”‚
   â”‚    âœ— First offense + severe? No                       â”‚
   â”‚    âœ— Legal concern? No                                â”‚
   â”‚                                                        â”‚
   â”‚ 7. HITL decision                                       â”‚
   â”‚    hitl_required = False                               â”‚
   â”‚    hitl_trigger_reasons = []                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚
   â””â”€> Update ContentState

9. Route Decision: HITL Not Required
   â””â”€> Proceed to: user_reputation_scoring

10. Agent 5b: User Reputation Scoring Agent
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Execution Time: ~1 second                              â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Query User History from Database                    â”‚
    â”‚    â””â”€> get_user_violations(user_id="user_12345")       â”‚
    â”‚        Result: violations = []                         â”‚
    â”‚                                                        â”‚
    â”‚ 2. Calculate Reputation Metrics                        â”‚
    â”‚    â€¢ Account age: 365 days                             â”‚
    â”‚    â€¢ Total submissions: 24                             â”‚
    â”‚    â€¢ Approved: 24, Removed: 0                          â”‚
    â”‚    â€¢ Approval rate: 100%                               â”‚
    â”‚    â€¢ Previous violations: 0                            â”‚
    â”‚                                                        â”‚
    â”‚ 3. Compute Reputation Score                            â”‚
    â”‚    reputation_score = 0.85 (Good standing)             â”‚
    â”‚    user_risk_score = 0.05 (Very low risk)              â”‚
    â”‚                                                        â”‚
    â”‚ 4. Make Decision                                       â”‚
    â”‚    Decision: APPROVE                                   â”‚
    â”‚    Confidence: 0.90                                    â”‚
    â”‚    Reasoning: "Trusted user with clean history"        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â””â”€> Update ContentState

11. Agent 6: Action Enforcement Agent
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Execution Time: ~1.5 seconds                           â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ 1. Determine final action                              â”‚
    â”‚    final_decision = "approve"                          â”‚
    â”‚    final_confidence = 0.92                             â”‚
    â”‚                                                        â”‚
    â”‚ 2. Generate user-friendly reason (LLM)                 â”‚
    â”‚    Prompt: "Generate approval message..."              â”‚
    â”‚    Response: "Your story has been approved!"           â”‚
    â”‚                                                        â”‚
    â”‚ 3. Execute Actions:                                    â”‚
    â”‚    â”œâ”€> Update story visibility = "public"              â”‚
    â”‚    â”œâ”€> Set moderation_status = "approved"              â”‚
    â”‚    â”œâ”€> Generate notification message                   â”‚
    â”‚    â””â”€> Record action timestamp                         â”‚
    â”‚                                                        â”‚
    â”‚ 4. Store decision in ChromaDB memory                   â”‚
    â”‚    â””â”€> store_moderation_decision(state)                â”‚
    â”‚        Stored for future pattern learning              â”‚
    â”‚                                                        â”‚
    â”‚ 5. Update databases                                    â”‚
    â”‚    â””â”€> See Phase 4 below                               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: DATABASE PERSISTENCE                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

12. Database Operations (All executed together)

    A. Moderation Database (moderation_data.db)

       Table: content_submissions
       INSERT INTO content_submissions VALUES (
         content_id: "story_1733334567_abc",
         content_type: "story",
         content_text: "I had a wonderful time...",
         user_id: "user_12345",
         author_name: "John Doe",
         final_decision: "approve",
         confidence_score: 0.92,
         processing_time_ms: 10500,
         hitl_required: false,
         created_at: "2025-12-04T10:30:00Z",
         updated_at: "2025-12-04T10:30:11Z"
       )

       Table: stories
       INSERT INTO stories VALUES (
         story_id: "story_1733334567_abc",
         title: "My Amazing Day at the Park",
         content: "I had a wonderful time...",
         author_id: "user_12345",
         author_name: "John Doe",
         moderation_status: "approved",
         visibility: "public",
         created_at: "2025-12-04T10:30:00Z",
         approved_at: "2025-12-04T10:30:11Z"
       )

       Table: agent_executions (6 records inserted)
       For each agent:
       INSERT INTO agent_executions VALUES (
         execution_id: "exec_...",
         content_id: "story_1733334567_abc",
         agent_name: "content_analysis_agent",
         decision: "approve",
         confidence: 0.92,
         reasoning: "Positive personal story...",
         execution_time_ms: 2000,
         timestamp: "2025-12-04T10:30:01Z"
       )

    B. ChromaDB Vector Memory

       Collection: moderation_decisions
       ADD document:
       {
         id: "story_1733334567_abc",
         text: "I had a wonderful time...",
         metadata: {
           decision: "approve",
           confidence: 0.92,
           topics: ["leisure", "outdoor activities"],
           toxicity_score: 0.03,
           user_reputation: 0.85
         },
         embedding: [0.234, -0.145, ...] (768-dim vector)
       }

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: USER NOTIFICATION & RESPONSE                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

13. API Response to Frontend
    â””â”€> POST /api/stories/submit
        Status: 200 OK
        Body: {
          success: true,
          content_id: "story_1733334567_abc",
          status: "approved",
          message: "Your story has been approved and is now public!",
          moderation_result: {
            decision: "approve",
            confidence: 0.92,
            processing_time_ms: 10500,
            reviewed_by: "AI_System"
          }
        }

14. Frontend Updates
    â”œâ”€> Display success notification
    â”œâ”€> Redirect to story page
    â””â”€> Story is immediately visible to all users

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SUMMARY: Happy Path - Auto-Approved Story                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Processing Time: ~10.5 seconds
LLM API Calls: 4 (Content Analysis, Policy Check, ReAct Synthesis, Action Reason)
Agents Executed: 6 agents
Database Writes: 8 operations
Final Status: Approved âœ…
User Impact: Story published immediately
```

---

### ğŸ”´ Story 2: User Submits Toxic Comment (Removal Path)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO: User posts a toxic comment that violates community guidelines  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Comment Submission
   POST /api/stories/story_123/comments
   Body: {
     content: "This is stupid. You're an idiot for posting this garbage.",
     user_id: "user_99999",
     story_id: "story_123"
   }

2. Fast Mode Check
   â”œâ”€> len(content) = 68 chars < 200 âœ…
   â”œâ”€> content_type = "story_comment" âœ…
   â””â”€> Route: "fast_mode" (Single-agent processing)

3. Fast Mode Agent Execution
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~1.2 seconds                           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1. Single LLM call with combined analysis              â”‚
   â”‚                                                        â”‚
   â”‚ 2. Toxicity Detection                                  â”‚
   â”‚    â””â”€> keyword_toxicity_detection()                    â”‚
   â”‚        Detected: "stupid", "idiot", "garbage"          â”‚
   â”‚        toxicity_score = 0.72 (High)                    â”‚
   â”‚                                                        â”‚
   â”‚ 3. Policy Check                                        â”‚
   â”‚    Violation: Harassment/Bullying                      â”‚
   â”‚    Severity: Medium                                    â”‚
   â”‚                                                        â”‚
   â”‚ 4. User History Check                                  â”‚
   â”‚    get_user_violations("user_99999")                   â”‚
   â”‚    Found: 2 previous warnings                          â”‚
   â”‚                                                        â”‚
   â”‚ 5. Decision                                            â”‚
   â”‚    Decision: REMOVE                                    â”‚
   â”‚    Confidence: 0.88                                    â”‚
   â”‚    Reasoning: "Contains harassment. Repeat offender."  â”‚
   â”‚                                                        â”‚
   â”‚ 6. User Action                                         â”‚
   â”‚    â””â”€> Apply 7-day suspension (3rd violation)          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. Action Enforcement
   â”œâ”€> Delete comment from database
   â”œâ”€> Update user: suspended = true, suspended_until = +7 days
   â”œâ”€> Generate notification
   â””â”€> Store in memory for learning

5. Database Updates

   Table: story_comments
   INSERT ... moderation_status = "removed"

   Table: policy_violations
   INSERT (user_id="user_99999", violation_type="harassment", severity="medium")

   Table: user_actions
   INSERT (user_id="user_99999", action="suspend", duration_days=7, reason="Repeated harassment")

6. API Response
   Status: 200 OK
   Body: {
     success: false,
     status: "removed",
     message: "Your comment violates our harassment policy and has been removed.",
     user_action: {
       type: "suspension",
       duration_days: 7,
       reason: "Repeated policy violations",
       appeal_allowed: true
     }
   }

Total Processing Time: ~1.2 seconds âš¡
Decision: Removed + 7-day suspension âŒ
```

---

### â¸ï¸ Story 3: Content Requires Human Review (HITL Path)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO: Borderline case requiring human judgment                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Story Submission
   POST /api/stories/submit
   Body: {
     title: "My Views on Recent Elections",
     content: "I think the recent election was unfair. Many people believe..."
   }

2. Multi-Agent Analysis
   â€¢ Agent 1 (Content Analysis): FLAG (conf: 0.68) - Sensitive political content
   â€¢ Agent 2 (Toxicity): APPROVE (conf: 0.75) - Low toxicity
   â€¢ Agent 3 (Policy): WARN (conf: 0.62) - Potential misinformation

3. ReAct Decision Loop
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ THINK: Agents disagree (consensus = 33%)               â”‚
   â”‚ ACT: Synthesize â†’ FLAG                                 â”‚
   â”‚ OBSERVE: Check HITL triggers                           â”‚
   â”‚                                                        â”‚
   â”‚ âœ“ Low confidence: 0.68 < 0.70                          â”‚
   â”‚ âœ“ Conflicting decisions: consensus 33% < 60%           â”‚
   â”‚ âœ“ Sensitive content: political topic                   â”‚
   â”‚                                                        â”‚
   â”‚ Result: hitl_required = TRUE                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. HITL Checkpoint Agent
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ 1. Calculate priority score                            â”‚
   â”‚    â€¢ Low confidence: +30                               â”‚
   â”‚    â€¢ Conflicting decisions: +40                        â”‚
   â”‚    â€¢ Sensitive content: +50                            â”‚
   â”‚    Total: 120 points â†’ CRITICAL priority               â”‚
   â”‚                                                        â”‚
   â”‚ 2. Prepare review packet                               â”‚
   â”‚    â€¢ Content text                                      â”‚
   â”‚    â€¢ All agent decisions + reasoning                   â”‚
   â”‚    â€¢ User context                                      â”‚
   â”‚    â€¢ Similar past cases                                â”‚
   â”‚                                                        â”‚
   â”‚ 3. Add to HITL queue in database                       â”‚
   â”‚    INSERT INTO hitl_queue VALUES (...)                 â”‚
   â”‚                                                        â”‚
   â”‚ 4. Pause workflow using LangGraph checkpointer         â”‚
   â”‚    â””â”€> Workflow state saved, execution paused          â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. Workflow Paused - Wait for Human
   Status: PENDING_HUMAN_REVIEW

   â””â”€> Store state in memory:
       hitl_pending_reviews["story_1733334567_xyz"] = ContentState

6. API Response (Immediate)
   Status: 202 Accepted
   Body: {
     success: true,
     status: "pending_review",
     message: "Your story is under review by our moderation team.",
     estimated_review_time: "< 1 hour"
   }

7. Moderator Reviews (Separate Session)

   A. Moderator Login
      â””â”€> GET /api/hitl/queue
          Returns: [
            {
              content_id: "story_1733334567_xyz",
              priority: "critical",
              submitted_at: "2025-12-04T14:30:00Z",
              ai_recommendation: "flag",
              reason: "Conflicting AI decisions, sensitive content"
            }
          ]

   B. Moderator Reviews Content
      â””â”€> GET /api/hitl/review/story_1733334567_xyz
          Returns full review packet with all AI analysis

   C. Moderator Makes Decision
      â””â”€> POST /api/hitl/review/story_1733334567_xyz
          Body: {
            decision: "approve",
            reviewer_name: "Moderator Sarah",
            notes: "Factual discussion, no misinformation detected",
            confidence_override: 0.95
          }

8. Resume Workflow
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ resume_from_hitl() called                              â”‚
   â”‚                                                        â”‚
   â”‚ 1. Retrieve saved state from checkpointer              â”‚
   â”‚ 2. Update state with human decision                    â”‚
   â”‚    â€¢ hitl_human_decision = "approve"                   â”‚
   â”‚    â€¢ reviewer_name = "Moderator Sarah"                 â”‚
   â”‚    â€¢ hitl_resolution_timestamp = now()                 â”‚
   â”‚                                                        â”‚
   â”‚ 3. Route to action_enforcement                         â”‚
   â”‚ 4. Workflow continues from pause point                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

9. Action Enforcement
   â””â”€> Apply human decision (approve)
   â””â”€> Publish story
   â””â”€> Notify user: "Your story has been reviewed and approved"

10. Learning Update
    â””â”€> Store in memory: Human overrode AI flag â†’ approve
    â””â”€> System learns: Similar political content may be acceptable

Total Time: Variable (depends on moderator availability)
HITL ensures quality on edge cases âœ…
```

---

### ğŸ“ Story 4: User Appeals a Removal Decision

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SCENARIO: User appeals removed content                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User Submits Appeal
   POST /api/appeals/submit
   Body: {
     content_id: "comment_12345",
     user_id: "user_99999",
     appeal_reason: "This was sarcasm, not actual harassment. Context was missed."
   }

2. Appeal Review Agent Execution
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Execution Time: ~2.5 seconds                           â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ 1. Retrieve original content and decision              â”‚
   â”‚    â””â”€> get_content_by_id("comment_12345")              â”‚
   â”‚        Original decision: REMOVE (toxicity detected)   â”‚
   â”‚                                                        â”‚
   â”‚ 2. Get user violation history                          â”‚
   â”‚    â””â”€> get_user_violations("user_99999")               â”‚
   â”‚        Found: 2 previous warnings, 1 suspension        â”‚
   â”‚                                                        â”‚
   â”‚ 3. Analyze appeal with LLM                             â”‚
   â”‚    Prompt: "Review this appeal considering context"    â”‚
   â”‚    LLM Response: {                                     â”‚
   â”‚      appeal_valid: true,                               â”‚
   â”‚      confidence: 0.78,                                 â”‚
   â”‚      reasoning: "Sarcasm indicators present,           â”‚
   â”‚                  context supports user claim"          â”‚
   â”‚    }                                                   â”‚
   â”‚                                                        â”‚
   â”‚ 4. Make appeal decision                                â”‚
   â”‚    Decision: UPHOLD_APPEAL (restore content)           â”‚
   â”‚    Confidence: 0.78                                    â”‚
   â”‚                                                        â”‚
   â”‚ 5. Update databases                                    â”‚
   â”‚    â€¢ Restore comment visibility                        â”‚
   â”‚    â€¢ Reverse suspension                                â”‚
   â”‚    â€¢ Update violation count (-1)                       â”‚
   â”‚    â€¢ Record appeal outcome                             â”‚
   â”‚                                                        â”‚
   â”‚ 6. Learning update                                     â”‚
   â”‚    â””â”€> Update patterns: Sarcasm detection improved     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Database Updates

   Table: appeals
   UPDATE appeals SET
     status = "upheld",
     reviewed_at = now(),
     reviewer = "appeal_review_agent"
   WHERE appeal_id = "appeal_789"

   Table: story_comments
   UPDATE story_comments SET
     moderation_status = "approved",
     visibility = "visible"
   WHERE comment_id = "comment_12345"

   Table: user_actions
   UPDATE user_actions SET
     reversed = true,
     reversal_reason = "Appeal upheld - context misunderstood"
   WHERE action_id = "action_456"

4. User Notification
   â””â”€> Email/In-app: "Your appeal has been approved. Your comment has been restored."

5. System Learning
   â””â”€> ChromaDB: Store appeal outcome for future reference
   â””â”€> Update sarcasm detection patterns

Total Processing Time: ~2.5 seconds
Outcome: Appeal upheld, content restored âœ…
Learning: System improved for similar cases
```

---

## ğŸ“ Sequence Diagrams

### 1. Story Submission Flow

User â†’ Frontend â†’ API â†’ Workflow â†’ Agents â†’ Database â†’ Frontend â†’ User

![Story Submission Sequence Diagram](images/story-submission-seq-diagram.png)

- **Processing Time:** 6-12 seconds (Full Pipeline)
- **LLM Calls:** 6-8 calls

---

### 2. Comment Submission Flow (Fast Mode)

User â†’ Frontend â†’ API â†’ Workflow â†’ Fast Mode Agent â†’ Database â†’ User

![Comment Submission Sequence Diagram](images/comment-submission-seq-diagram.png)

- **Processing Time:** 1-2 seconds (Fast Mode)
- **LLM Calls:** 1 call
- **Cost Savings:** 87.5% reduction

---

### 3. Full Multi-Agent Pipeline

```
Entry â†’ Content Analysis â†’ Toxicity â†’ Policy â†’ ReAct â†’ Reputation â†’ Action â†’ END

                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚ Entry Router â”‚
                            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ content_type = "story"
                                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT 1: Content Analysis Agent                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Sentiment analysis (positive/negative/neutral)                     â”‚
â”‚ â€¢ Topic extraction (using LLM)                                       â”‚
â”‚ â€¢ Category detection                                                 â”‚
â”‚ â€¢ Explicit content check                                             â”‚
â”‚ â€¢ Retrieve similar historical content from ChromaDB                  â”‚
â”‚ â€¢ Initial toxicity detection                                         â”‚
â”‚                                                                      â”‚
â”‚ Output: content_category, content_topics, sentiment                  â”‚
â”‚ Decision: APPROVE â†’ continue | FLAG â†’ END                            â”‚
â”‚ Confidence: 0.85                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT 2: Toxicity Detection Agent                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Calculate toxicity score (0.0-1.0)                                 â”‚
â”‚ â€¢ Detect profanity patterns                                          â”‚
â”‚ â€¢ Identify hate speech indicators                                    â”‚
â”‚ â€¢ Check for harassment/bullying                                      â”‚
â”‚ â€¢ Use ML models (if enabled) or keyword detection                    â”‚
â”‚                                                                      â”‚
â”‚ Thresholds:                                                          â”‚
â”‚   0.0-0.2: None â†’ APPROVE                                            â”‚
â”‚   0.2-0.4: Low â†’ MONITOR                                             â”‚
â”‚   0.4-0.6: Medium â†’ FLAG                                             â”‚
â”‚   0.6-0.8: High â†’ REMOVE                                             â”‚
â”‚   0.8-1.0: Severe â†’ REMOVE + User Action                             â”‚
â”‚                                                                      â”‚
â”‚ Output: toxicity_score, toxicity_level, categories                   â”‚
â”‚ Decision: APPROVE | FLAG | REMOVE                                    â”‚
â”‚ Confidence: 0.80                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT 3: Policy Violation Agent                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Check against community guidelines                                 â”‚
â”‚ â€¢ Identify specific violations:                                      â”‚
â”‚   - Hate speech, harassment, bullying                                â”‚
â”‚   - Spam, misinformation                                             â”‚
â”‚   - Sexual content, violence                                         â”‚
â”‚   - Self-harm, illegal activity                                      â”‚
â”‚ â€¢ Assess severity: low/medium/high/critical                          â”‚
â”‚                                                                      â”‚
â”‚ Output: policy_violations[], violation_severity                      â”‚
â”‚ Decision: APPROVE | WARN | REMOVE                                    â”‚
â”‚ Confidence: 0.75                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT 4: ReAct Decision Loop (Think-Act-Observe)                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ THINK Phase:                                                         â”‚
â”‚ â€¢ Analyze all 3 agent decisions                                      â”‚
â”‚ â€¢ Calculate consensus level (% agreement)                            â”‚
â”‚ â€¢ Compute average confidence                                         â”‚
â”‚ â€¢ Identify conflicts in recommendations                              â”‚
â”‚ â€¢ Run LLM synthesis on combined analysis                             â”‚
â”‚                                                                      â”‚
â”‚ ACT Phase:                                                           â”‚
â”‚ â€¢ Synthesize final decision                                          â”‚
â”‚ â€¢ Map to: APPROVE | WARN | REMOVE | SUSPEND | BAN                    â”‚
â”‚ â€¢ Calculate final confidence score                                   â”‚
â”‚                                                                      â”‚
â”‚ OBSERVE Phase:                                                       â”‚
â”‚ â€¢ Evaluate 8 HITL trigger conditions:                                â”‚
â”‚   1. Low confidence (<70%)                                           â”‚
â”‚   2. High severity violation (critical/high)                         â”‚
â”‚   3. Conflicting decisions (consensus <60%)                          â”‚
â”‚   4. High-profile user (10k+ followers)                              â”‚
â”‚   5. Sensitive content (politics/religion)                           â”‚
â”‚   6. Potential false positive                                        â”‚
â”‚   7. First offense + severe                                          â”‚
â”‚   8. Legal concern                                                   â”‚
â”‚                                                                      â”‚
â”‚ Output: react_act_decision, react_confidence, hitl_required          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚ HITL Needed? â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ YES                      NOâ”‚
              â†“                            â†“
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ AGENT 5a:          â”‚      â”‚ AGENT 5b:          â”‚
       â”‚ HITL Checkpoint    â”‚      â”‚ User Reputation    â”‚
       â”‚                    â”‚      â”‚ Scoring Agent      â”‚
       â”‚ â€¢ Pause workflow   â”‚      â”‚                    â”‚
       â”‚ â€¢ Calculate        â”‚      â”‚ â€¢ Get user history â”‚
       â”‚   priority         â”‚      â”‚ â€¢ Count violations â”‚
       â”‚ â€¢ Add to queue     â”‚      â”‚ â€¢ Identify repeat  â”‚
       â”‚ â€¢ Wait for human   â”‚      â”‚   offenders        â”‚
       â”‚                    â”‚      â”‚ â€¢ Calculate risk   â”‚
       â”‚ Status: PENDING    â”‚      â”‚ â€¢ Update rep score â”‚
       â”‚                    â”‚      â”‚                    â”‚
       â”‚ [Workflow pauses   â”‚      â”‚ Output: user_      â”‚
       â”‚  until human       â”‚      â”‚ reputation_score,  â”‚
       â”‚  provides          â”‚      â”‚ user_risk_score    â”‚
       â”‚  decision via      â”‚      â”‚                    â”‚
       â”‚  /api/hitl/review] â”‚      â”‚ Decision: APPROVE  â”‚
       â”‚                    â”‚      â”‚ | WARN | REMOVE    â”‚
       â”‚                    â”‚      â”‚ | SUSPEND | BAN    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                           â”‚
                â”‚ Human Decision            â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT 6: Action Enforcement Agent                                    â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Execute final moderation action                                    â”‚
â”‚ â€¢ Generate user-friendly reason (via LLM)                            â”‚
â”‚ â€¢ Update content visibility                                          â”‚
â”‚ â€¢ Apply user penalties if needed:                                    â”‚
â”‚   - WARN: Notify user, keep content                                  â”‚
â”‚   - REMOVE: Delete content, notify user                              â”‚
â”‚   - SUSPEND: Calculate duration, notify user                         â”‚
â”‚   - BAN: Permanent ban, notify user                                  â”‚
â”‚ â€¢ Store decision in ChromaDB memory                                  â”‚
â”‚ â€¢ Record audit log in database                                       â”‚
â”‚ â€¢ Update user profile (violations count, reputation)                 â”‚
â”‚                                                                      â”‚
â”‚ Output: action_timestamp, user_notified, content_removed             â”‚
â”‚ Decision: Always APPROVE (action completed)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  END  â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4. HITL Review Process

```
Agent Pause â†’ Queue â†’ Moderator â†’ Decision â†’ Resume â†’ Complete

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Content triggers HITL (from ReAct Loop)                      â”‚
â”‚ â€¢ Low confidence: 65% (below 70% threshold)                  â”‚
â”‚ â€¢ Conflicting decisions: Agents disagree                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HITL Checkpoint Agent                                        â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Calculate Priority Score:                                 â”‚
â”‚    â€¢ Low confidence: +30 points                              â”‚
â”‚    â€¢ High severity: +80 points                               â”‚
â”‚    â€¢ Conflicting decisions: +40 points                       â”‚
â”‚    â€¢ High-profile user: +60 points                           â”‚
â”‚    â€¢ Sensitive content: +50 points                           â”‚
â”‚    â€¢ Potential false positive: +45 points                    â”‚
â”‚    â€¢ First offense severe: +70 points                        â”‚
â”‚    â€¢ Legal concern: +100 points                              â”‚
â”‚                                                              â”‚
â”‚ 2. Assign Priority Level:                                    â”‚
â”‚    â€¢ Critical: 100+ points â†’ Immediate review                â”‚
â”‚    â€¢ High: 75-99 points â†’ <1 hour SLA                        â”‚
â”‚    â€¢ Medium: 50-74 points â†’ <4 hours SLA                     â”‚
â”‚    â€¢ Low: 0-49 points â†’ <24 hours SLA                        â”‚
â”‚                                                              â”‚
â”‚ 3. Prepare Review Packet:                                    â”‚
â”‚    â€¢ Content text                                            â”‚
â”‚    â€¢ All agent decisions + reasoning                         â”‚
â”‚    â€¢ Toxicity scores                                         â”‚
â”‚    â€¢ Policy violations                                       â”‚
â”‚    â€¢ User context (reputation, history)                      â”‚
â”‚    â€¢ ReAct synthesis                                         â”‚
â”‚                                                              â”‚
â”‚ 4. Add to HITL Queue (in database)                           â”‚
â”‚                                                              â”‚
â”‚ 5. Pause Workflow (using LangGraph checkpointer)             â”‚
â”‚                                                              â”‚
â”‚ Status: PENDING_HUMAN_REVIEW                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ [Workflow is saved and paused]
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Moderator Dashboard                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Moderator logs in                                         â”‚
â”‚                                                              â”‚
â”‚ 2. GET /api/hitl/queue                                       â”‚
â”‚    â€¢ Returns items sorted by priority                        â”‚
â”‚    â€¢ Shows pending count by priority level                   â”‚
â”‚                                                              â”‚
â”‚ 3. Moderator selects item                                    â”‚
â”‚                                                              â”‚
â”‚ 4. GET /api/hitl/review/{content_id}                         â”‚
â”‚    â€¢ Returns detailed review packet:                         â”‚
â”‚      - Content details                                       â”‚
â”‚      - AI analysis (all agents)                              â”‚
â”‚      - User context                                          â”‚
â”‚      - Suggested actions                                     â”‚
â”‚                                                              â”‚
â”‚ 5. Moderator Reviews:                                        â”‚
â”‚    â€¢ Reads content                                           â”‚
â”‚    â€¢ Reviews AI reasoning                                    â”‚
â”‚    â€¢ Checks user history                                     â”‚
â”‚    â€¢ Considers context                                       â”‚
â”‚                                                              â”‚
â”‚ 6. Moderator Makes Decision:                                 â”‚
â”‚    â€¢ APPROVE: Content is acceptable                          â”‚
â”‚    â€¢ WARN: Minor violation, warn user                        â”‚
â”‚    â€¢ REMOVE: Violates policy, remove                         â”‚
â”‚    â€¢ SUSPEND_USER: Suspend for X days                        â”‚
â”‚    â€¢ BAN_USER: Permanent ban                                 â”‚
â”‚    â€¢ ESCALATE: Needs senior review                           â”‚
â”‚                                                              â”‚
â”‚ 7. POST /api/hitl/review/{content_id}                        â”‚
â”‚    Body: {                                                   â”‚
â”‚      decision: "remove",                                     â”‚
â”‚      reviewer_name: "Moderator Jane",                        â”‚
â”‚      notes: "Clear harassment",                              â”‚
â”‚      confidence_override: 0.95                               â”‚
â”‚    }                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Resume Workflow (resume_from_hitl)                           â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ 1. Retrieve saved state from checkpointer                    â”‚
â”‚                                                              â”‚
â”‚ 2. Update state with human decision:                         â”‚
â”‚    â€¢ hitl_human_decision = "remove"                          â”‚
â”‚    â€¢ hitl_human_notes = "Clear harassment"                   â”‚
â”‚    â€¢ reviewer_name = "Moderator Jane"                        â”‚
â”‚    â€¢ hitl_resolution_timestamp = now()                       â”‚
â”‚                                                              â”‚
â”‚ 3. Route based on human decision:                            â”‚
â”‚    â€¢ APPROVE â†’ Action Enforcement â†’ END                      â”‚
â”‚    â€¢ WARN/REMOVE â†’ Action Enforcement â†’ END                  â”‚
â”‚    â€¢ SUSPEND/BAN â†’ Reputation Scoring â†’ Action Enforcement   â”‚
â”‚    â€¢ ESCALATE â†’ END (handled externally)                     â”‚
â”‚                                                              â”‚
â”‚ 4. Workflow continues from pause point                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Action Enforcement Agent                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Uses human decision as final authority                     â”‚
â”‚ â€¢ Executes action (remove content, notify user, etc.)        â”‚
â”‚ â€¢ Records in audit log: "Human Override"                     â”‚
â”‚ â€¢ Stores decision in memory for learning                     â”‚
â”‚ â€¢ If overturned AI decision, updates learning patterns       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  END  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”˜

Total Time: Variable (depends on moderator availability)
HITL adds human judgment to edge cases and low-confidence decisions
```

---

### 5. Appeal Workflow

User Appeal â†’ Review Agent â†’ Decision â†’ Database â†’ Notify User

![Appeal Workflow](images/appeal-workflow-seq-diagram.png)

- **Appeal Processing Time:** 2-3 seconds
- **Outcome:** Content restored if appeal successful
- **Learning:** System learns from overturned decisions to improve future accuracy

---

## ğŸ—„ï¸ Database Schema Details

### Database 1: Auth Database (moderation_auth.db)

```sql
-- User Management
CREATE TABLE users (
    user_id INTEGER PRIMARY KEY AUTOINCREMENT,
    username TEXT UNIQUE NOT NULL,
    password_hash TEXT NOT NULL,
    full_name TEXT NOT NULL,
    role TEXT NOT NULL,  -- 'moderator', 'senior_moderator', 'admin'
    email TEXT,
    phone TEXT,
    is_active INTEGER DEFAULT 1,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    last_login TEXT
);

-- Session Management
CREATE TABLE user_sessions (
    session_id TEXT PRIMARY KEY,
    user_id INTEGER NOT NULL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    expires_at TEXT,
    ip_address TEXT,
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);

-- Audit Trail
CREATE TABLE audit_log (
    log_id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id INTEGER,
    action TEXT NOT NULL,
    content_id TEXT,
    details TEXT,
    timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
    ip_address TEXT,
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);

-- Moderator Performance
CREATE TABLE moderator_stats (
    user_id INTEGER PRIMARY KEY,
    total_reviews INTEGER DEFAULT 0,
    approved_count INTEGER DEFAULT 0,
    removed_count INTEGER DEFAULT 0,
    warned_count INTEGER DEFAULT 0,
    escalated_count INTEGER DEFAULT 0,
    avg_response_time_seconds REAL DEFAULT 0,
    accuracy_score REAL DEFAULT 0,
    last_updated TEXT,
    FOREIGN KEY (user_id) REFERENCES users(user_id)
);
```

### Database 2: Moderation Database (moderation_data.db)

```sql
-- Content Submissions (All content processed)
CREATE TABLE content_submissions (
    content_id TEXT PRIMARY KEY,
    content_type TEXT NOT NULL,  -- 'story', 'story_comment'
    content_text TEXT NOT NULL,
    user_id TEXT NOT NULL,
    author_name TEXT,
    final_decision TEXT,  -- 'approve', 'warn', 'remove', 'suspend', 'ban'
    confidence_score REAL,
    processing_time_ms INTEGER,
    hitl_required INTEGER DEFAULT 0,
    hitl_reviewer TEXT,
    hitl_notes TEXT,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    updated_at TEXT,
    metadata TEXT  -- JSON blob
);

-- Stories
CREATE TABLE stories (
    story_id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    content TEXT NOT NULL,
    author_id TEXT NOT NULL,
    author_name TEXT,
    moderation_status TEXT,  -- 'pending', 'approved', 'removed'
    visibility TEXT,  -- 'public', 'hidden', 'deleted'
    toxicity_score REAL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    approved_at TEXT,
    removed_at TEXT
);

-- Comments
CREATE TABLE story_comments (
    comment_id TEXT PRIMARY KEY,
    story_id TEXT NOT NULL,
    content TEXT NOT NULL,
    author_id TEXT NOT NULL,
    author_name TEXT,
    moderation_status TEXT,
    visibility TEXT,
    toxicity_score REAL,
    created_at TEXT DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (story_id) REFERENCES stories(story_id)
);

-- Agent Execution Logs
CREATE TABLE agent_executions (
    execution_id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_id TEXT NOT NULL,
    agent_name TEXT NOT NULL,
    decision TEXT,
    confidence REAL,
    reasoning TEXT,
    execution_time_ms INTEGER,
    timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
    metadata TEXT,
    FOREIGN KEY (content_id) REFERENCES content_submissions(content_id)
);

-- Policy Violations
CREATE TABLE policy_violations (
    violation_id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_id TEXT NOT NULL,
    user_id TEXT NOT NULL,
    violation_type TEXT NOT NULL,  -- 'hate_speech', 'harassment', 'spam', etc.
    severity TEXT,  -- 'low', 'medium', 'high', 'critical'
    detected_by TEXT,  -- 'ai_agent' or moderator name
    timestamp TEXT DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (content_id) REFERENCES content_submissions(content_id)
);

-- User Actions (Warnings, Suspensions, Bans)
CREATE TABLE user_actions (
    action_id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    action_type TEXT NOT NULL,  -- 'warn', 'suspend', 'ban'
    reason TEXT,
    duration_days INTEGER,  -- NULL for permanent bans
    applied_by TEXT,  -- Agent or moderator name
    applied_at TEXT DEFAULT CURRENT_TIMESTAMP,
    expires_at TEXT,
    reversed INTEGER DEFAULT 0,
    reversal_reason TEXT
);

-- Appeals
CREATE TABLE appeals (
    appeal_id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_id TEXT NOT NULL,
    user_id TEXT NOT NULL,
    appeal_reason TEXT NOT NULL,
    status TEXT DEFAULT 'pending',  -- 'pending', 'upheld', 'denied'
    reviewed_by TEXT,
    review_notes TEXT,
    submitted_at TEXT DEFAULT CURRENT_TIMESTAMP,
    reviewed_at TEXT,
    FOREIGN KEY (content_id) REFERENCES content_submissions(content_id)
);

-- HITL Queue
CREATE TABLE hitl_queue (
    queue_id INTEGER PRIMARY KEY AUTOINCREMENT,
    content_id TEXT NOT NULL,
    priority TEXT,  -- 'critical', 'high', 'medium', 'low'
    priority_score INTEGER,
    trigger_reasons TEXT,  -- JSON array
    ai_recommendation TEXT,
    status TEXT DEFAULT 'pending',  -- 'pending', 'in_progress', 'completed'
    assigned_to TEXT,
    added_at TEXT DEFAULT CURRENT_TIMESTAMP,
    completed_at TEXT,
    FOREIGN KEY (content_id) REFERENCES content_submissions(content_id)
);
```

### Database 3: ChromaDB Vector Memory

```python
# Collections in ChromaDB

Collection: "moderation_decisions"
â”œâ”€ Documents: Historical moderation decisions
â”œâ”€ Embeddings: 768-dimensional vectors (ChromaDB default embedding function)
â”œâ”€ Metadata: {
â”‚    decision: "approve/remove/warn",
â”‚    confidence: 0.0-1.0,
â”‚    topics: ["topic1", "topic2"],
â”‚    toxicity_score: 0.0-1.0,
â”‚    user_reputation: 0.0-1.0,
â”‚    timestamp: "ISO8601"
â”‚  }
â””â”€ Used for: Finding similar past content decisions

Collection: "flagged_patterns"
â”œâ”€ Documents: Patterns of flagged content
â”œâ”€ Metadata: {
â”‚    pattern_type: "hate_speech/harassment/etc",
â”‚    severity: "low/medium/high",
â”‚    frequency: count
â”‚  }
â””â”€ Used for: Pattern learning and detection

Collection: "user_violations"
â”œâ”€ Documents: User violation history
â”œâ”€ Metadata: {
â”‚    user_id: "user_12345",
â”‚    violation_type: "harassment",
â”‚    timestamp: "ISO8601"
â”‚  }
â””â”€ Used for: User risk scoring
```

**Embedding Model Details:**
- **Embedding Function**: ChromaDB default (typically sentence-transformers/all-MiniLM-L6-v2)
- **Vector Dimensions**: 384 or 768 dimensions (depending on ChromaDB version)
- **Purpose**: Semantic similarity search for historical content retrieval
- **Performance**: Fast local embedding generation without API calls

---

## ğŸ“ˆ System Performance Metrics

### Processing Time Comparison

| Scenario | Mode | Agents | LLM Calls | Time | Cost/Request |
|----------|------|--------|-----------|------|--------------|
| Short comment (<200 chars) | Fast Mode | 1 | 1 | 1-2s | $0.0002 |
| Long comment (>200 chars) | Full Pipeline | 6 | 4 | 6-12s | $0.0016 |
| Story submission | Full Pipeline | 6 | 4 | 6-12s | $0.0016 |
| HITL case | Full + Human | 6 + Human | 4 | Variable | $0.0016 + Human time |
| Appeal review | Appeal Agent | 1 | 2 | 2-3s | $0.0005 |

### Throughput Estimates

- **Fast Mode**: ~500-1000 comments/minute
- **Full Pipeline**: ~100-200 stories/minute
- **HITL Reviews**: Depends on moderator availability
- **Appeals**: ~400-600 appeals/minute

---

## ğŸ” Security & Privacy

### Authentication Flow
```
User â†’ Login â†’ Auth DB â†’ Session Token â†’ Stored in memory â†’ Used for all API calls
```

### Authorization Levels
- **Regular Users**: Submit content, view own content, file appeals
- **Moderators**: Access HITL queue, review content, view analytics
- **Senior Moderators**: Override decisions, manage moderators
- **Admins**: Full system access, user management, configuration

### Data Privacy
- User passwords: SHA-256 hashed (stored in Auth DB)
- Session tokens: 32-byte random URL-safe tokens
- Content data: Encrypted at rest (database level)
- API communication: HTTPS only (production)

---

## ğŸš€ Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Load Balancer                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Server 1   â”‚    â”‚   API Server 2   â”‚
â”‚   (FastAPI)      â”‚    â”‚   (FastAPI)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚               â”‚             â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ SQLite   â”‚  â”‚ ChromaDB â”‚  â”‚ Google     â”‚
â”‚ DBs      â”‚  â”‚ Vector   â”‚  â”‚ Gemini API â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### Environment Variables Impact

```
ENABLE_FAST_MODE=true
â”œâ”€> workflow.py: enable_fast_mode parameter
â”œâ”€> should_use_fast_mode(): Check eligibility
â””â”€> Entry Router: Route to fast_mode or content_analysis

FAST_MODE_MAX_LENGTH=200
â””â”€> should_use_fast_mode(): len(content) <= 200

FAST_MODE_CONTENT_TYPES=story_comment
â””â”€> should_use_fast_mode(): content_type in eligible_types

USE_ML_MODELS=true
â”œâ”€> ml_classifier.py: Load transformer models
â””â”€> Toxicity Detection Agent: Use ML predictions

GOOGLE_API_KEY=your_key
â”œâ”€> All agents: Initialize ChatGoogleGenerativeAI (LLM reasoning)
â””â”€> Used for: Content analysis, toxicity detection, policy compliance, decision synthesis
    NOTE: NOT used for embeddings - ChromaDB uses local sentence-transformers

CHROMADB_PERSIST_DIR=./chroma_data
â”œâ”€> memory.py: PersistentClient storage location
â””â”€> Stores: moderation_decisions, flagged_patterns, user_violations collections
    Embeddings: ChromaDB default (sentence-transformers/all-MiniLM-L6-v2)
```

### Key Takeaways

1. **Modular Design**: Each component has a single responsibility
2. **State-Driven**: ContentState flows through all agents, accumulating data
3. **Flexible Routing**: Workflow adapts based on content type and agent decisions
4. **Human Oversight**: HITL provides safety net for edge cases
5. **Learning System**: Memory and learning tracker enable continuous improvement
6. **Performance Options**: Fast Mode for comments, Full Pipeline for critical content

### Processing Paths

| Content Type | Length | Mode | Agents | Time | Cost |
|--------------|--------|------|--------|------|------|
| Story | Any | Full | 6 agents | 6-12s | High |
| Comment | >200 chars | Full | 6 agents | 6-12s | High |
| Comment | â‰¤200 chars | Fast | 1 agent | 1-2s | Low |

### Agent Decision Flow

```
Entry â†’ Analysis â†’ Toxicity â†’ Policy â†’ ReAct
                                          â†“
                                    HITL? (if needed)
                                          â†“
                                    Reputation
                                          â†“
                                    Action â†’ END
```

---

## ğŸ“Š Complete Flow Summary

### Key Metrics
- **6 AI Agents** working in orchestrated pipeline
- **3 Databases**: Auth SQLite, Moderation SQLite, ChromaDB Vector Memory
- **2 AI Systems**: Google Gemini LLM (reasoning) + ChromaDB Embeddings (memory)
- **8 HITL Triggers** for edge case detection
- **87.5% Cost Reduction** with Fast Mode for comments
- **~10 second** average processing time (full pipeline)
- **~1 second** average processing time (fast mode)

### Decision Paths
1. **Auto-Approve**: Clean content â†’ 6 agents â†’ Approved
2. **Auto-Remove**: Toxic content â†’ Fast/Full mode â†’ Removed + User action
3. **HITL Review**: Edge cases â†’ Pause â†’ Human review â†’ Resume
4. **Appeal**: User contests â†’ Appeal agent â†’ Restore/Deny

This architecture ensures:
- âœ… Scalable multi-agent processing
- âœ… Human oversight for edge cases
- âœ… Continuous learning from decisions
- âœ… Fast processing with cost optimization
- âœ… Complete audit trail
- âœ… User appeal mechanism
