{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1aaU4YZC-fswSImo1fV-w67FXPQg5Ictm?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
      ],
      "metadata": {
        "id": "AWgDx0D0ED7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“Š What is Vector Embeddings?\n",
        "\n",
        "Vector embedding is a way to represent words, phrases, or texts as numerical vectors in a multi-dimensional space. This helps the model understand language better by capturing meanings and relationships between words.\n",
        "\n",
        "![Vector Embedding](https://qdrant.tech/articles_data/what-are-embeddings/BERT-model.jpg)\n",
        "Source: [Qdrant Blog](https://qdrant.tech/articles/what-are-embeddings/)\n",
        "\n",
        "\n",
        "\n",
        "#### Embedding Models â€¢ Vector Stores â€¢ Vector Embeddings (Guide) â†’ [PDF](https://github.com/genieincodebottle/generative-ai/blob/main/docs/vector-embeddings-guide.pdf)"
      ],
      "metadata": {
        "id": "HxxOWz9dpsYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required libraries"
      ],
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "     langchain \\\n",
        "     langchain-chroma \\\n",
        "     langchain-community \\\n",
        "     einops"
      ],
      "metadata": {
        "id": "ShxTNxM5gqtr",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import related libraries"
      ],
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Model Decision Flow\n",
        "\n",
        "<br>\n",
        "\n",
        "![MTEB Areana](https://raw.githubusercontent.com/genieincodebottle/generative-ai/main/images/embedding.png)"
      ],
      "metadata": {
        "id": "UxSeVL9rKoyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Google's Embedding Model\n",
        "\n",
        "### Provide Google API Key.\n",
        "\n",
        "It can be used both for Gemini LLM  & Google Embedding Model. You can create Google API key using following link\n",
        "\n",
        "- [Google API Key](https://aistudio.google.com/apikey)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6UeDlrgqI2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
      ],
      "metadata": {
        "id": "Zqf1sMFbKE_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "74b3435b-af88-4d1f-d196-b2522cffe4d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”“ Free-Tier: Google Gemini Embeddings  \n",
        "\n",
        "**Model:** `text-embedding-004`  \n",
        "\n",
        "**Pros:** Free for experimentation, multilingual (100+), long context (3,072 tokens), multiple dimensions (768/1024), high quality, easy Google Cloud integration.  \n",
        "\n",
        "**Cons:** Usage limits, closed-source, vendor lock-in, limited customization, infra dependency, possible service changes.  \n",
        "\n",
        "ðŸ”— [Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api#generative-ai-get-text-embedding-python_vertex_ai_sdk)  \n"
      ],
      "metadata": {
        "id": "wY2uCq0XBeXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SfvFZd4y-962"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Basic RAG with Key Components\n",
        "\n",
        "1. **Chroma:** Vector store for efficient similarity search  \n",
        "2. **Embedding Model:** Googleâ€™s text-embedding model  \n",
        "3. **ChatGoogleGenerativeAI:** Gemini LLM for response generation  \n",
        "4. **Cosine Similarity:** For evaluating queryâ€“responseâ€“context relevance  \n",
        "\n",
        "## Step 1: RAG in Action with Evaluation  \n",
        "\n",
        "This implementation demonstrates the core workflow of a Basic RAG system:  \n",
        "\n",
        "1. Chunking and embedding source documents  \n",
        "2. Retrieving relevant documents via similarity search  \n",
        "3. Generating responses using the retrieved context  \n",
        "4. Evaluating response quality using similarity scores  \n",
        "\n",
        "ðŸ”— References:  \n",
        "- [LangChain Chunking Strategies](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)  \n",
        "- [LangChain Vectorstores](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)  \n"
      ],
      "metadata": {
        "id": "D5cQdItbNMsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Gemini language model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    temperature=0.3  # Adjust temperature or other parameters as needed\n",
        ")\n",
        "\n",
        "# Step 2: Load documents from a web URL\n",
        "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
        "loader = WebBaseLoader(url)\n",
        "data = loader.load()\n",
        "\n",
        "# Step 3: Split text into chunks\n",
        "# (Experiment with chunk_size and chunk_overlap for optimal results)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "chunks = text_splitter.split_documents(data)\n",
        "\n",
        "# Add unique IDs to each text chunk\n",
        "for idx, chunk in enumerate(chunks):\n",
        "    chunk.metadata[\"id\"] = idx\n",
        "\n",
        "# Step 4: Get embedding model\n",
        "gemini_embeddings = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\"\n",
        ")\n",
        "\n",
        "# Step 5: Create vector store using embeddings\n",
        "vectorstore = Chroma.from_documents(chunks, gemini_embeddings)\n",
        "\n",
        "# Step 6: Define query\n",
        "query = \"What are the main applications of artificial intelligence in healthcare?\"\n",
        "\n",
        "# Step 7: Retrieve relevant documents\n",
        "docs = vectorstore.similarity_search(query, k=5)\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "retrieval_method = \"Basic similarity search\"\n",
        "\n",
        "# Step 8: Generate response\n",
        "prompt = f\"{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "final_response = llm.invoke(prompt).content\n",
        "\n",
        "# Step 9: Print results\n",
        "print(f\"Query: {query}\")\n",
        "print(\"=========================\")\n",
        "print(f\"Final Answer: {final_response}\")\n",
        "print(\"=========================\")\n",
        "print(f\"Retrieval Method: {retrieval_method}\")"
      ],
      "metadata": {
        "id": "B5I5PCsKNP2N"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: RAG Evaluation  \n",
        "\n",
        "1. Generate embeddings for **query**, **response**, and **context**  \n",
        "2. Measure **cosine similarity** between queryâ€“response and responseâ€“context  \n",
        "3. Derive an **overall relevance score** as the average of these similarities  \n"
      ],
      "metadata": {
        "id": "C23HOdS0IO_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Step 10: Define evaluation function\n",
        "def evaluate_response(query, embeddings, response, context):\n",
        "    \"\"\"\n",
        "    Evaluate the relevance of the model's response by comparing embeddings.\n",
        "\n",
        "    - Computes embeddings for query, response, and context\n",
        "    - Calculates cosine similarities\n",
        "    - Returns an average relevance score\n",
        "    \"\"\"\n",
        "    # Compute embeddings\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    response_embedding = embeddings.embed_query(response)\n",
        "    context_embedding = embeddings.embed_query(context)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    query_response_similarity = cosine_similarity(\n",
        "        [query_embedding], [response_embedding]\n",
        "    )[0][0]\n",
        "\n",
        "    response_context_similarity = cosine_similarity(\n",
        "        [response_embedding], [context_embedding]\n",
        "    )[0][0]\n",
        "\n",
        "    # Compute overall relevance score (average)\n",
        "    relevance_score = (\n",
        "        query_response_similarity + response_context_similarity\n",
        "    ) / 2\n",
        "\n",
        "    return {\n",
        "        \"query_response_similarity\": query_response_similarity,\n",
        "        \"response_context_similarity\": response_context_similarity,\n",
        "        \"relevance_score\": relevance_score,\n",
        "    }\n",
        "\n",
        "# Step 11: Evaluate the response\n",
        "evaluation = evaluate_response(query, gemini_embeddings, final_response, context)\n",
        "\n",
        "# Step 12: Print evaluation results\n",
        "print(\"\\nEvaluation Results\")\n",
        "print(\"=========================\")\n",
        "print(f\"Query-Response Similarity   : {evaluation['query_response_similarity']:.4f}\")\n",
        "print(f\"Response-Context Similarity : {evaluation['response_context_similarity']:.4f}\")\n",
        "print(f\"Overall Relevance Score     : {evaluation['relevance_score']:.4f}\")"
      ],
      "metadata": {
        "id": "aIXlHaWqgcza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similalry OpenAI and Huggingface Embedding models can be used as following"
      ],
      "metadata": {
        "id": "8vnE9a3jN1U6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. OpenAI Embedding Model\n",
        "\n",
        "### Provide OpenAI API Key.\n",
        "\n",
        "If you want to use OpenAI Embedding. You can create OpenAI API key using following link\n",
        "\n",
        "- [OpenAI API Key](https://platform.openai.com/settings/organization/api-keys)"
      ],
      "metadata": {
        "id": "dVOq9sU--Vj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "BluoWUGsHxes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "e3TOKmRoCzzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’° Paid: OpenAI Embedding Models  \n",
        "\n",
        "**Models:** `text-embedding-3-small`, `text-embedding-3-large`, `ada v2`  \n",
        "\n",
        "**Pros:** High-quality embeddings, multiple model sizes, seamless API integration, batch processing for cost efficiency, regularly updated, versatile across NLP tasks.  \n",
        "\n",
        "**Cons:** Paid (costs can scale), closed-source, requires API key + internet, limited customization, data privacy considerations, subject to OpenAI policies.  \n",
        "\n",
        "ðŸ”— [Docs](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings)  \n"
      ],
      "metadata": {
        "id": "gCE5wgpGDTD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ],
      "metadata": {
        "id": "QJESKKcjDhgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Huggingface Embedding Model\n",
        "\n",
        "### Provide Huggingface API Key.\n",
        "\n",
        "If you want to use Huggingface Embedding Models. You can create Huggingface API key using following link\n",
        "\n",
        "- [Huggingface API Key](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mbH7Tl5fI5qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU Sentence-transformers \\\n",
        "                 langchain-huggingface"
      ],
      "metadata": {
        "id": "ILNgaQmfJH79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "iOoRf7vrDArL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”“ Free: Hugging Face Open-Source Embeddings  \n",
        "\n",
        "**Models:** gte-large-en-v1.5, bge-multilingual-gemma2, snowflake-arctic-embed-l, nomic-embed-text-v1.5, e5-mistral-7b-instruct, etc. â†’ [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)  \n",
        "\n",
        "**Pros:** Open-source, customizable, community-backed, Hugging Face integration, supports fine-tuning, broad NLP use cases.  \n",
        "**Cons:** May underperform vs. commercial models, variable quality, limited support, high compute needs, community-depende\n"
      ],
      "metadata": {
        "id": "VhVYI4olDrCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face: Nomic AI Embedding Model  \n",
        "\n",
        "You can choose from various Hugging Face open-source embedding models depending on your use case, performance needs, and system constraints. Model rankings and benchmarks are available on the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).  \n",
        "\n",
        "**Popular Models:**  \n",
        "1. `nomic-ai/nomic-embed-text-v1.5`  \n",
        "2. `nomic-ai/nomic-embed-text-v1`  \n",
        "3. `sentence-transformers/all-MiniLM-L12-v2`  \n",
        "4. `sentence-transformers/all-MiniLM-L6-v2`  \n"
      ],
      "metadata": {
        "id": "Co0T5mEdDx5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Change model_name as per your choosen huggingface embedding model\n",
        "nomic_embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})"
      ],
      "metadata": {
        "id": "_m3Ff9WTDrkE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}