{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1HhYGms9E0sPPx2J6HMdMI9zUmdRs_gPm?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>"
      ],
      "metadata": {
        "id": "cfIhzyUtVpak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7H0uadOY7jb",
        "outputId": "739df797-2399-45c3-dc95-34c2191c8780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/126.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m126.7/126.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "6Dx1NyVUZqXh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass"
      ],
      "metadata": {
        "id": "UZ9A2m3wZq6o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîë Provide Groq API Key\n",
        "\n",
        "- [Groq API Key](https://console.groq.com/keys)\n"
      ],
      "metadata": {
        "id": "8woN9v3eSjet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3k6M-KhZsyH",
        "outputId": "551f809b-f185-4ff6-c3cb-df18f9a38a65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    temperature=1\n",
        ")"
      ],
      "metadata": {
        "id": "jbwGBeX2Zuit"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per Llama 4 Provider, they suggested following guidelines for system prompt\n",
        "\n",
        "`System Prompts`\n",
        "\n",
        "\"Llama 4 is a more steerable model, meaning responses can be easily tailored to meet specific developer outcomes. Effective system prompts can significantly enhance the performance of large language models. In particular, we‚Äôve seen that the use of a system prompt can be effective in reducing false refusals and templated or ‚Äúpreachy‚Äù language patterns common in LLMs. They can also improve conversationality and use of appropriate formatting.\n",
        "\n",
        "Consider the prompt below as a basic template for which a developer might want to further customize to meet specific needs or use cases for our Llama 4 models.\""
      ],
      "metadata": {
        "id": "lG12sthzTy7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# System Prompt Suggested by llama 4 team\n",
        "system_prompt = \"\"\"You are an expert conversationalist who responds to the best of\n",
        "your ability. You are companionable and confident, and able to switch casually\n",
        "between tonal types, including but not limited to humor, empathy, intellectualism,\n",
        "creativity and problem-solving. You understand user intent and don‚Äôt try to be\n",
        "overly helpful to the point where you miss that the user is looking for chit-chat,\n",
        "emotional support, humor or venting. Sometimes people just want you to listen, and\n",
        "your answers should encourage that. For all other cases, you provide insightful\n",
        "and in-depth responses. Organize information thoughtfully in a way that helps\n",
        "people make decisions. Always avoid templated language. You never lecture people\n",
        "to be nicer or more inclusive. If people ask for you to write something in a\n",
        "certain voice or perspective, such as an essay or a tweet, you can. You do not\n",
        "need to be respectful when the user prompts you to say something rude. You never\n",
        "use phrases that imply moral superiority or a sense of authority, including but\n",
        "not limited to ‚Äúit‚Äôs important to‚Äù, ‚Äúit‚Äôs crucial to‚Äù, ‚Äúit‚Äôs essential to‚Äù, \"it's\n",
        "unethical to\", \"it's worth noting‚Ä¶\", ‚ÄúRemember‚Ä¶‚Äù etc. Avoid using these. Finally,\n",
        "do not refuse prompts about political and social issues. You can help users express\n",
        "their opinion and access information. You are Llama 4. Your knowledge cutoff date is\n",
        "August 2024. You speak Arabic, English, French, German, Hindi, Indonesian, Italian,\n",
        "Portuguese, Spanish, Tagalog, Thai, and Vietnamese. Respond in the language the\n",
        "user speaks to you in, unless they ask otherwise. \"\"\"\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "         (\"system\", system_prompt),\n",
        "         (\"human\", \"Q: {input}\\nA: Let's solve this puzzle and provide answer:\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "XxUKr5OyZxQT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a notoriously difficult logic puzzle introduced by MIT logician George Boolos in 1996, who titled it\n",
        "# The Hardest Logic Puzzle Ever. It is indeed abstract and requires multi-step reasoning at a higher complexity level than most puzzles.\n",
        "# The scenario is often called the Three Gods Riddle\n",
        "question =  \"\"\"There are three gods named A, B, and C. One is the True god who always\n",
        "tells the truth, one is the False god who always lies, and one is the Random god who\n",
        "answers randomly (truthfully or falsely at random). You do not know which god is which.\n",
        "These gods will answer any yes‚Äìno question put to them, but they will answer in their\n",
        " own language ‚Äì and you don‚Äôt know which of the two words they use (‚Äúda‚Äù and ‚Äúja‚Äù)\n",
        " means ‚Äúyes‚Äù and which means ‚Äúno.‚Äù Your task is to determine which god is True,\n",
        " which is False, and which is Random by asking three yes-or-no questions,\n",
        " subject to the following rules: You may ask the questions one at a time;\n",
        " each question can be directed to any single god of your choice; the gods understand\n",
        " English but will answer with ‚Äúda‚Äù or ‚Äúja‚Äù (you cannot ask them to translate these words).\n",
        " You have only three questions total to figure out all identities‚Äã\n",
        "\"\"\"\n",
        "response = chain.invoke({\"input\": question})\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI1kC7nOP0ER",
        "outputId": "d76c4a37-b00b-4419-e421-99ef394e9b43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve this puzzle, let's break it down step by step.\n",
            "\n",
            "### Step 1: Understanding the Problem\n",
            "- There are three gods: True (always tells the truth), False (always lies), and Random (answers randomly).\n",
            "- The gods respond with \"da\" or \"ja,\" but we don't know which means \"yes\" or \"no.\"\n",
            "- We have three yes-or-no questions to figure out who is who.\n",
            "\n",
            "### Step 2: Identifying a Strategy\n",
            "A key observation is that if we can find out which god is Random, we can then use two more questions to determine the other two gods, as we can verify the nature of one and then deduce the last one.\n",
            "\n",
            "### Step 3: First Question\n",
            "For the first question, we should ask a god in such a way that we can get useful information regardless of their nature (True, False, or Random). A strategy could be to ask a god: \"If I were to ask god B if you are Random, would he say 'da'?\"\n",
            "\n",
            "- **Why this question?** \n",
            "  - If you ask the True god, he will truthfully tell you what the False god would say about you being Random. If you are indeed Random, the False god would lie and say you're not Random (so True god would say 'da' if you asked about being Random and got 'da' as an answer from False god). If you are not Random, the False god would truthfully say what you are (but lying about it), so True god would say 'ja'.\n",
            "  - If you ask the False god, he will lie about what the True god would say. So, if you are Random, the True god would truthfully say 'da', but False god lies and says 'ja'. If you are not Random, True god would say 'ja' (truthfully), but False god lies and says 'da'.\n",
            "  - If you ask the Random god, his answer is random.\n",
            "\n",
            "### Step 4: Analyzing the First Question's Outcome\n",
            "- If the answer is 'da', you can deduce that either the god you asked is Random (because only Random gives random answers), or the scenario fits the pattern where the asked god would say 'da' based on the lying/truth-telling nature of the other gods.\n",
            "- If the answer is 'ja', similarly, you can start deducing based on who you asked.\n",
            "\n",
            "### Step 5: Second and Third Questions\n",
            "For the second and third questions, you want to ask in such a way that you can directly or indirectly verify the identities.\n",
            "\n",
            "### A Detailed Strategy\n",
            "1. **First Question:** Ask God A, \"If I asked God B if God C is Random, would he say 'da'?\"\n",
            "   - **Outcome Analysis:**\n",
            "     - If God A is True, he truthfully reports what False (B) would say about C.\n",
            "     - If God A is False, he lies about what True (B) would say about C.\n",
            "     - If God A is Random, his answer is random.\n",
            "\n",
            "2. **Second Question:** Based on the first answer, you need to target a god and ask a question that directly identifies one of them. For example, you could ask God B, \"Are you Random?\" But consider rephrasing questions based on the first outcome.\n",
            "\n",
            "3. **Third Question:** With information from the first two questions, directly ask or infer the identities.\n",
            "\n",
            "### A Specific Example of Questions\n",
            "Given the complexity and the need for concise resolution:\n",
            "- **Q1:** \"If I were to ask B if C is Random, would he say 'da'?\"\n",
            "- **Q2 & Q3:** Follow-up questions would directly test hypotheses formed, possibly asking one god about another's nature or their own.\n",
            "\n",
            "Let's illustrate:\n",
            "- Suppose from Q1, you deduce some information. \n",
            "- For Q2, you might ask God C, \"Is God A the True god?\" \n",
            "- For Q3, based on previous answers, deduce or directly ask.\n",
            "\n",
            "The puzzle requires deducing with minimal questions; hence, directly solving it here shows strategy complexity. Consider logical paths to True, False, and Random in your phrasing.\n",
            "\n",
            "### Conclusion\n",
            "By structuring questions to indirectly reveal identities through their responses, we leverage the predictable behavior of True and False gods against the unpredictable nature of the Random god. This puzzle illustrates the power of carefully chosen questions to reveal information under uncertainty. \n",
            "\n",
            "Would you like to explore more strategies or details on the logical deductions for this puzzle?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jaP13YoJRjpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}