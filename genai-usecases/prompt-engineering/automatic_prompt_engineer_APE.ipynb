{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1Ib3QcE9TgUZHuYXgepwk9FS-eSbDniQ8?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "## What is Automatic Prompt Engineer (APE)?\n",
        "\n",
        "**Automatic Prompt Engineer (APE)** is an AI based system that automatically generates and optimizes prompts for language models. Instead of manually crafting prompts through trial and error, APE uses AI to create, test and select the best prompts for your specific tasks.\n",
        "\n",
        "### üéØ Key Concepts:\n",
        "\n",
        "**üîÑ How APE Works:**\n",
        "1. **Generate**: Creates multiple prompt candidates using different techniques\n",
        "2. **Evaluate**: Scores each prompt based on clarity, effectiveness, and completeness  \n",
        "3. **Optimize**: Selects the best-performing prompt for your task\n",
        "4. **Test**: Runs the optimal prompt to produce your desired output\n",
        "\n",
        "**üß† Core Benefits:**\n",
        "- **Time-Saving**: Eliminates hours of manual prompt tweaking\n",
        "- **Quality**: Often produces better prompts than human engineers\n",
        "- **Consistency**: Provides reliable, repeatable results\n",
        "- **Learning**: Shows you different prompting approaches and techniques\n",
        "\n",
        "**üî¨ Advanced Techniques Included:**\n",
        "- **Meta-prompting**: AI creates prompts for other AI systems\n",
        "- **Chain-of-thought**: Step-by-step reasoning optimization\n",
        "- **Persona-based**: Expert role assignments for specialized tasks\n",
        "- **Multi-dimensional evaluation**: Comprehensive prompt quality assessment\n",
        "- **Temperature optimization**: Automatic creativity/precision balancing\n",
        "\n",
        "### üìä Real-World Impact:\n",
        "- Up to 40% improvement in AI response accuracy\n",
        "- Better task completion rates across diverse applications\n"
      ],
      "metadata": {
        "id": "Y2eqHW0TK87s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U langchain-groq>=0.2.4\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNrPX8MZJm9e",
        "outputId": "a62e0a34-e26a-4b54-9f73-84fc59752829"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dependencies installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import getpass\n",
        "from typing import List, Dict, Tuple\n",
        "from dataclasses import dataclass\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "gOLahJCEJv6_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîë Provide Groq API Key\n",
        "\n",
        "- [Groq API Key](https://console.groq.com/keys)\n"
      ],
      "metadata": {
        "id": "KIxcSinElhRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter Groq API Key: \")\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ],
      "metadata": {
        "id": "I6qvHWQsJxWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43842acb-9c26-46cc-eed3-566c92f32ccf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Groq API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class SimplePrompt:\n",
        "    \"\"\"Simple prompt container\"\"\"\n",
        "    text: str\n",
        "    score: float = 0.0\n",
        "    technique: str = \"\"\n",
        "    temperature: float = 0.5\n",
        "\n",
        "    def show(self):\n",
        "        \"\"\"Display the prompt nicely\"\"\"\n",
        "        display(Markdown(f\"\"\"\n",
        "        **Technique:** {self.technique} | **Score:** {self.score:.1f}/10 | **Temperature:** {self.temperature}\n",
        "\n",
        "        ```\n",
        "        {self.text}\n",
        "        ```\n",
        "        \"\"\"))"
      ],
      "metadata": {
        "id": "RzCcry2GJ2Z1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class APE:\n",
        "    \"\"\"Automatic Prompt Engineer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.5)\n",
        "        print(\"APE ready!\")\n",
        "\n",
        "    def generate_prompts(self, task: str) -> List[SimplePrompt]:\n",
        "        \"\"\"Generate different types of optimized prompts\"\"\"\n",
        "        print(\"üîÑ Generating optimized prompts...\")\n",
        "\n",
        "        # Single comprehensive prompt to generate multiple types\n",
        "        generator = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are an expert prompt engineer. Create 5 different high-quality prompts for the given task using these techniques:\n",
        "\n",
        "            1. ZERO_SHOT: Direct instruction with clear expectations\n",
        "            2. EXPERT_PERSONA: Assign relevant expertise and professional approach\n",
        "            3. STEP_BY_STEP: Chain-of-thought with structured reasoning\n",
        "            4. EXAMPLE_BASED: Few-shot with examples and patterns\n",
        "            5. COMPREHENSIVE: Detailed instructions with verification\n",
        "\n",
        "            Make each prompt effective and unique.\"\"\"),\n",
        "\n",
        "            (\"human\", \"\"\"Task: {task}\n",
        "\n",
        "            Create 5 optimized prompts using the techniques above. Format exactly as:\n",
        "\n",
        "            ZERO_SHOT:\n",
        "            [prompt text]\n",
        "\n",
        "            EXPERT_PERSONA:\n",
        "            [prompt text]\n",
        "\n",
        "            STEP_BY_STEP:\n",
        "            [prompt text]\n",
        "\n",
        "            EXAMPLE_BASED:\n",
        "            [prompt text]\n",
        "\n",
        "            COMPREHENSIVE:\n",
        "            [prompt text]\"\"\")\n",
        "        ])\n",
        "\n",
        "        result = self.llm.invoke(generator.format_prompt(task=task))\n",
        "        return self._parse_prompts(result.content)\n",
        "\n",
        "    def _parse_prompts(self, response: str) -> List[SimplePrompt]:\n",
        "        \"\"\"Parse generated prompts\"\"\"\n",
        "        prompts = []\n",
        "        techniques = [\"ZERO_SHOT\", \"EXPERT_PERSONA\", \"STEP_BY_STEP\", \"EXAMPLE_BASED\", \"COMPREHENSIVE\"]\n",
        "        temperatures = [0.3, 0.4, 0.2, 0.5, 0.3]  # Optimal temps for each technique\n",
        "\n",
        "        for i, technique in enumerate(techniques):\n",
        "            pattern = f\"{technique}:\\\\s*(.*?)(?={techniques[i+1] if i+1 < len(techniques) else '$'})\"\n",
        "            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "            if match:\n",
        "                prompt_text = match.group(1).strip()\n",
        "                prompt = SimplePrompt(\n",
        "                    text=prompt_text,\n",
        "                    technique=technique.lower().replace('_', '-'),\n",
        "                    temperature=temperatures[i]\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "\n",
        "        print(f\"‚úÖ Generated {len(prompts)} prompts\")\n",
        "        return prompts\n",
        "\n",
        "    def evaluate_prompts(self, task: str, prompts: List[SimplePrompt]) -> List[SimplePrompt]:\n",
        "        \"\"\"Evaluate and score prompts\"\"\"\n",
        "        print(\"üìä Evaluating prompts...\")\n",
        "\n",
        "        evaluator = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"\"\"You are a prompt evaluation expert. Score prompts 1-10 based on:\n",
        "            - Clarity and specificity\n",
        "            - Likelihood of good results\n",
        "            - Completeness for the task\n",
        "            - Technical effectiveness\n",
        "\n",
        "            Be strict but fair in scoring.\"\"\"),\n",
        "\n",
        "            (\"human\", \"\"\"Task: {task}\n",
        "\n",
        "            Evaluate this prompt and give a single score 1-10:\n",
        "\n",
        "            Technique: {technique}\n",
        "            Prompt: {prompt_text}\n",
        "\n",
        "            Response format: Score: [number]/10\"\"\")\n",
        "        ])\n",
        "\n",
        "        for prompt in prompts:\n",
        "            result = self.llm.invoke(evaluator.format_prompt(\n",
        "                task=task,\n",
        "                technique=prompt.technique,\n",
        "                prompt_text=prompt.text\n",
        "            ))\n",
        "\n",
        "            # Extract score\n",
        "            score_match = re.search(r\"Score:\\s*(\\d+(?:\\.\\d+)?)/10\", result.content)\n",
        "            if score_match:\n",
        "                prompt.score = float(score_match.group(1))\n",
        "            else:\n",
        "                prompt.score = 5.0  # Default score\n",
        "\n",
        "        # Sort by score\n",
        "        sorted_prompts = sorted(prompts, key=lambda x: x.score, reverse=True)\n",
        "        print(f\"‚úÖ Evaluation complete! Best score: {sorted_prompts[0].score:.1f}/10\")\n",
        "\n",
        "        return sorted_prompts\n",
        "\n",
        "    def test_best_prompt(self, task: str, best_prompt: SimplePrompt) -> str:\n",
        "        \"\"\"Test the best prompt and generate result\"\"\"\n",
        "        print(\"üß™ Testing best prompt...\")\n",
        "\n",
        "        # Use the best prompt with its optimal temperature\n",
        "        test_llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=best_prompt.temperature)\n",
        "\n",
        "        tester = ChatPromptTemplate.from_messages([\n",
        "            (\"system\", \"You are an AI assistant following the given prompt instructions carefully.\"),\n",
        "            (\"human\", \"{prompt}\\n\\nTask: {task}\")\n",
        "        ])\n",
        "\n",
        "        result = test_llm.invoke(tester.format_prompt(prompt=best_prompt.text, task=task))\n",
        "        return result.content\n",
        "\n",
        "    def run_ape(self, task: str) -> Tuple[SimplePrompt, str]:\n",
        "        \"\"\"Run the complete APE process\"\"\"\n",
        "        print(\"üöÄ\" + \"=\"*50)\n",
        "        print(\"=\"*52)\n",
        "        print(f\"üìã Task: {task}\")\n",
        "        print(\"=\"*52 + \"\\n\")\n",
        "\n",
        "        # Generate prompts\n",
        "        prompts = self.generate_prompts(task)\n",
        "\n",
        "        # Evaluate prompts\n",
        "        evaluated_prompts = self.evaluate_prompts(task, prompts)\n",
        "\n",
        "        # Get best prompt\n",
        "        best_prompt = evaluated_prompts[0]\n",
        "\n",
        "        # Test best prompt\n",
        "        result = self.test_best_prompt(task, best_prompt)\n",
        "\n",
        "        print(\"‚úÖ APE process complete!\\n\")\n",
        "\n",
        "        return best_prompt, result"
      ],
      "metadata": {
        "id": "lgfrnWAylw5Y"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_prompt(task: str):\n",
        "    \"\"\"\n",
        "    One-function prompt optimization!\n",
        "\n",
        "    Args:\n",
        "        task (str): The task you want to optimize a prompt for\n",
        "\n",
        "    Returns:\n",
        "        Displays the best prompt and its result\n",
        "    \"\"\"\n",
        "    ape = APE()\n",
        "    best_prompt, result = ape.run_ape(task)\n",
        "\n",
        "    # Display results nicely\n",
        "    display(Markdown(f\"\"\"\n",
        "    ## üéØ Optimization Results\n",
        "\n",
        "    **Your Task:** {task}\n",
        "\n",
        "    ### üèÜ Best Prompt Found:\n",
        "    **Technique:** {best_prompt.technique}\n",
        "    **Score:** {best_prompt.score:.1f}/10\n",
        "    **Temperature:** {best_prompt.temperature}\n",
        "\n",
        "    ```\n",
        "    {best_prompt.text}\n",
        "    ```\n",
        "\n",
        "    ### üìù Generated Result:\n",
        "    {result}\n",
        "    \"\"\"))\n",
        "\n",
        "    return best_prompt, result\n",
        "\n",
        "def compare_prompts(task: str):\n",
        "    \"\"\"\n",
        "    Generate and compare multiple prompts\n",
        "\n",
        "    Args:\n",
        "        task (str): The task to generate prompts for\n",
        "\n",
        "    Returns:\n",
        "        Shows all generated prompts with scores\n",
        "    \"\"\"\n",
        "    ape = APE()\n",
        "\n",
        "    print(\"üîÑ Generating and comparing prompts...\\n\")\n",
        "\n",
        "    # Generate and evaluate\n",
        "    prompts = ape.generate_prompts(task)\n",
        "    evaluated_prompts = ape.evaluate_prompts(task, prompts)\n",
        "\n",
        "    # Display all prompts\n",
        "    display(Markdown(f\"## üìä All Generated Prompts for: {task}\\n\"))\n",
        "\n",
        "    for i, prompt in enumerate(evaluated_prompts, 1):\n",
        "        display(Markdown(f\"### #{i} - {prompt.technique.title()} (Score: {prompt.score:.1f}/10)\"))\n",
        "        prompt.show()\n",
        "        print()\n",
        "\n",
        "    return evaluated_prompts"
      ],
      "metadata": {
        "id": "RCjbg-xkJ4cY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Quick optimization\n",
        "print(\"üéØ EXAMPLE 1: Quick Prompt Optimization\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "task1 = \"Write a professional email to decline a job offer politely\"\n",
        "best_prompt, result = optimize_prompt(task1)"
      ],
      "metadata": {
        "id": "YCnIpHUSKWsT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "outputId": "46d48fc1-b7b7-4288-85ee-d155ac837b68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ EXAMPLE 1: Quick Prompt Optimization\n",
            "----------------------------------------\n",
            "APE ready!\n",
            "üöÄ==================================================\n",
            "====================================================\n",
            "üìã Task: Write a professional email to decline a job offer politely\n",
            "====================================================\n",
            "\n",
            "üîÑ Generating optimized prompts...\n",
            "‚úÖ Generated 5 prompts\n",
            "üìä Evaluating prompts...\n",
            "‚úÖ Evaluation complete! Best score: 9.0/10\n",
            "üß™ Testing best prompt...\n",
            "‚úÖ APE process complete!\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n    ## üéØ Optimization Results\n    \n    **Your Task:** Write a professional email to decline a job offer politely\n    \n    ### üèÜ Best Prompt Found:\n    **Technique:** zero-shot  \n    **Score:** 9.0/10  \n    **Temperature:** 0.3\n    \n    ```\n    Write a polite professional email declining a job offer with a standard format and phrases.\n    ```\n    \n    ### üìù Generated Result:\n    Subject: Declining Job Offer for [Position]\n\nDear [Hiring Manager's Name],\n\nI wanted to express my sincere gratitude for extending the offer to me for the [Position] role at [Company Name]. I was truly impressed by the company culture and the team's passion for their work during my interview process.\n\nAfter careful consideration, I regret to inform you that I have decided to decline the offer. Although I am excited about the opportunities that [Company Name] presents, I have decided to pursue another opportunity that aligns more closely with my long-term career goals.\n\nPlease know that my decision is not a reflection on the company or the role, but rather a personal decision that I believe is in the best interest of my career. I appreciate the time and effort that you and your team invested in the hiring process, and I am grateful for the experience.\n\nThank you again for the opportunity, and I wish the company continued success in the future.\n\nBest regards,\n\n[Your Name]\n\n[Your Contact Information]\n\nThis email follows a standard format and includes polite phrases to decline the job offer professionally.\n    "
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 2: Compare different prompts\n",
        "print(\"\\nüìä EXAMPLE 2: Compare Multiple Prompts\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "task2 = \"Explain quantum computing to a 12-year-old\"\n",
        "all_prompts = compare_prompts(task2)"
      ],
      "metadata": {
        "id": "kY1AoektWzh_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41394952-5517-4fb9-a92b-d0e19c98afec"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä EXAMPLE 2: Compare Multiple Prompts\n",
            "----------------------------------------\n",
            "APE ready!\n",
            "üîÑ Generating and comparing prompts...\n",
            "\n",
            "üîÑ Generating optimized prompts...\n",
            "‚úÖ Generated 5 prompts\n",
            "üìä Evaluating prompts...\n",
            "‚úÖ Evaluation complete! Best score: 9.0/10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## üìä All Generated Prompts for: Explain quantum computing to a 12-year-old\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### #1 - Zero-Shot (Score: 9.0/10)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Technique:** zero-shot | **Score:** 9.0/10 | **Temperature:** 0.3\n        \n        ```\n        Explain quantum computing to a 12-year-old in 100 words or less, using simple terms and examples.\n        ```\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### #2 - Expert-Persona (Score: 9.0/10)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Technique:** expert-persona | **Score:** 9.0/10 | **Temperature:** 0.4\n        \n        ```\n        Dr. Rachel Thompson, a quantum computing expert, here to explain quantum computing to a 12-year-old. Please provide a clear and concise explanation of the concept, its principles, and how it differs from classical computing.\n        ```\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### #3 - Step-By-Step (Score: 9.0/10)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Technique:** step-by-step | **Score:** 9.0/10 | **Temperature:** 0.2\n        \n        ```\n        Imagine you have a combination lock with 10 numbers. To open it, you need to try all the combinations one by one. Now, imagine a quantum computer can try all the combinations simultaneously. Explain how this works, using the concept of superposition and entanglement. Break down the process into steps, and provide an example of how a quantum computer would solve this problem.\n        ```\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### #4 - Comprehensive (Score: 9.0/10)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Technique:** comprehensive | **Score:** 9.0/10 | **Temperature:** 0.3\n        \n        ```\n        Explain quantum computing to a 12-year-old, covering the following topics:\n\n1. What is quantum computing, and how does it differ from classical computing?\n2. Explain the concept of superposition and entanglement, and how they enable quantum computers to process multiple possibilities simultaneously.\n3. Describe the process of quantum computing, including the use of qubits, quantum gates, and quantum algorithms.\n4. Provide examples of real-world applications of quantum computing, such as cryptography and simulations.\n5. Verify your explanation by answering the following questions: What are the benefits and limitations of quantum computing? How does it compare to classical computing in terms of processing power and speed?\n        ```\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### #5 - Example-Based (Score: 8.0/10)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n        **Technique:** example-based | **Score:** 8.0/10 | **Temperature:** 0.5\n        \n        ```\n        Explain quantum computing using the following examples:\n\n- A coin can be either heads or tails. A classical computer would have to choose one or the other. A quantum computer can exist in both states at the same time.\n- A quantum computer can process multiple possibilities simultaneously, like trying all the combinations of a lock at once.\n- Explain how quantum computing is used in real-world applications, such as cryptography and simulations.\n        ```\n        "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you can use it for your own tasks!\n",
        "print(\"\\nYOUR TURN!\")\n",
        "print(\"-\" * 40)\n",
        "print(\"Use these functions for your own tasks:\")\n",
        "print()\n",
        "print(\"# For quick optimization:\")\n",
        "print(\"optimize_prompt('your task here')\")\n",
        "print()\n",
        "print(\"# To compare multiple approaches:\")\n",
        "print(\"compare_prompts('your task here')\")\n",
        "print()\n",
        "\n",
        "# Uncomment and modify these lines for your own tasks:\n",
        "# my_task = \"Your task description here\"\n",
        "# my_best_prompt, my_result = optimize_prompt(my_task)"
      ],
      "metadata": {
        "id": "U5I1nLSwmO5f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}