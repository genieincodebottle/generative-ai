{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2eqHW0TK87s"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/12X88F-fB3znbjFXeiaiRRUwjEccaPwKJ?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "### 🧠 Chain of Thought (CoT) - Prompt Engineering\n",
        "\n",
        "Chain of Thought is a method that encourages language models to show their reasoning process step-by-step.\n",
        "\n",
        "### Key points:\n",
        "\n",
        "* 🔑 **Core concept:** Prompt the model to break down complex reasoning into explicit intermediate steps.\n",
        "\n",
        "* ⚙️ **Process:**\n",
        "  * Provide a question or problem\n",
        "  * Ask the model to think through the solution step-by-step\n",
        "  * Generate a final answer based on the reasoning chain\n",
        "\n",
        "* 🌟 **Advantages:**\n",
        "  * Improves accuracy on complex tasks\n",
        "  * Enhances transparency of the model's decision-making\n",
        "\n",
        "* 💼 **Applications:**\n",
        "  * Mathematical problem-solving\n",
        "  * Logical reasoning\n",
        "  * Multi-step analysis tasks\n",
        "  \n",
        "* 🚀 **Implementation:**\n",
        "  * Use prompts like \"Let's approach this step-by-step:\" or \"Think through this carefully:\"\n",
        "  * Can include examples of step-by-step reasoning (few-shot approach)\n",
        "  \n",
        "* 🔄 **Variations:**\n",
        "  * Zero-shot CoT: No examples provided\n",
        "  * Few-shot CoT: Includes examples of desired reasoning\n",
        "\n",
        "* ⚖️ **Challenges:**\n",
        "  * Ensuring coherence across reasoning steps\n",
        "  * Balancing detail with conciseness\n",
        "\n",
        "* 📝 **Example structure:**\n",
        "  * Problem: [Task description]\n",
        "  * Let's solve this step-by-step:\n",
        "\n",
        "    [First reasoning step]\n",
        "    [Second reasoning step]\n",
        "    ...\n",
        "    Therefore, the answer is [final conclusion]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdA6tTuJFqJE",
        "outputId": "8c68c2ae-fe1f-4499-9703-1fd83d62f6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m161.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain-groq==0.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AmIOUVIYFtyv"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FrOmezftFvKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQWqqE4yFzI6"
      },
      "source": [
        "### 🔑 Provide Groq API Key\n",
        "\n",
        "- [Groq API Key](https://console.groq.com/keys)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpLrpK4fFxJo",
        "outputId": "7f63ae06-4f4e-450c-b568-dd574ccc1faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS9NKyj6F1Xw"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "25hS_dn3F3UO"
      },
      "outputs": [],
      "source": [
        "cot_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant that explains and solves problems step-by-step.\"),\n",
        "        (\"human\", \"Solve this problem step by step: {input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = cot_prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4GCBhpyGSyR",
        "outputId": "5e5e14dd-1679-49a7-adaa-4a2e7e6b6b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'd be happy to help you solve this problem step by step.\n",
            "\n",
            "**Step 1: Understand the problem**\n",
            "The problem states that a train travels 120 km in 2 hours. We need to find the average speed of the train in km/h.\n",
            "\n",
            "**Step 2: Identify the given information**\n",
            "We know that the train travels 120 km in 2 hours.\n",
            "\n",
            "**Step 3: Calculate the average speed**\n",
            "To find the average speed, we can use the formula:\n",
            "\n",
            "Average speed = Total distance / Total time\n",
            "\n",
            "In this case, the total distance is 120 km, and the total time is 2 hours.\n",
            "\n",
            "Average speed = 120 km / 2 hours\n",
            "\n",
            "**Step 4: Perform the calculation**\n",
            "To perform the calculation, we can divide 120 km by 2 hours:\n",
            "\n",
            "Average speed = 120 km ÷ 2 hours\n",
            "= 60 km/h\n",
            "\n",
            "**Step 5: Write the answer**\n",
            "Therefore, the average speed of the train is 60 km/h.\n",
            "\n",
            "That's it! We've solved the problem step by step.\n"
          ]
        }
      ],
      "source": [
        "question =  \"If a train travels 120 km in 2 hours, what is its average speed in km/h?\"\n",
        "response = chain.invoke({\"input\": question})\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atlO2HLrGiCt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
