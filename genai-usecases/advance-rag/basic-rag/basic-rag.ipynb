{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1cA3yryEXdhIHwWRfAMleWPj0pv6izRXl?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\"Open In Colab\\\"></a>\n",
        "\n",
        "# üìöüîçü§ñ What is Basic RAG?\n",
        "\n",
        "Basic RAG is the standard, straightforward implementation of **Retrieval-Augmented Generation**. It involves retrieving relevant information from a knowledge base in response to a query, then using this information to generate an answer using a language model.\n",
        "\n",
        "# ‚ùì Why we need RAG?\n",
        "\n",
        "1. Combines the broad knowledge of language models with specific, up-to-date information\n",
        "2. Improves accuracy of responses by grounding them in retrieved facts\n",
        "3. Reduces hallucinations common in standalone language models\n",
        "4. Allows for easy updates to the knowledge base without retraining the entire model\n",
        "\n"
      ],
      "metadata": {
        "id": "HxxOWz9dpsYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ],
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-community==0.2.10 \\\n",
        "     langchain-huggingface==0.0.3 \\\n",
        "     einops==0.8.0 \\\n",
        "     faiss_cpu==1.8.0.post1"
      ],
      "metadata": {
        "id": "ShxTNxM5gqtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import related libraries related to Langchain, HuggingfaceEmbedding"
      ],
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "GT55z5AkhyOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Google API Key. You can create Google API key at following lin\n",
        "\n",
        "[Google Gemini-Pro API Creation Link](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "[YouTube Video](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ],
      "metadata": {
        "id": "F6UeDlrgqI2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "5223a3be-72d7-480e-e75d-644fedf1bb42"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Provide Huggingface API Key. You can create Huggingface API key at following lin\n",
        "\n",
        "[Higgingface API Creation Link](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1dLpYboqeIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6scBGZlhpG",
        "outputId": "5956fd3b-6b5c-4eb5-f7ab-d7e1f234c6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for printing docs\n",
        "def pretty_print_docs(docs):\n",
        "    # Iterate through each document and format the output\n",
        "    for i, d in enumerate(docs):\n",
        "        print(f\"{'-' * 50}\\nDocument {i + 1}:\")\n",
        "        print(f\"Content:\\n{d.page_content}\\n\")\n",
        "        print(\"Metadata:\")\n",
        "        for key, value in d.metadata.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print(f\"{'-' * 50}\")  # Final separator for clarity\n",
        "\n",
        "# Example usage\n",
        "# Assuming `docs` is a list of Document objects\n"
      ],
      "metadata": {
        "id": "VC4pm6cPyhue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG in Action"
      ],
      "metadata": {
        "id": "D5cQdItbNMsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Load documents from a web URL\n",
        "documents = WebBaseLoader(\"https://github.com/hwchase17/chroma-langchain/blob/master/state_of_the_union.txt\").load()\n",
        "\n",
        "# Split documents into chunks of 500 characters with 100 characters overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Add unique IDs to each text chunk\n",
        "for idx, text in enumerate(texts):\n",
        "    text.metadata[\"id\"] = idx"
      ],
      "metadata": {
        "id": "B5I5PCsKNP2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings for the text chunks\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "\n",
        "# Initialize a FAISS retriever with the text embeddings\n",
        "retriever = FAISS.from_documents(texts, embedding).as_retriever(search_kwargs={\"k\": 20})\n",
        "\n",
        "# Define a query and retrieve relevant documents\n",
        "query = \"What did the president say about Ketanji Brown Jackson\"\n",
        "docs = retriever.invoke(query)\n",
        "# Print the retrieved documents\n",
        "pretty_print_docs(docs)"
      ],
      "metadata": {
        "id": "ur_WY3cygkxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the RetrievalQA chain for question-answering tasks\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Create a RetrievalQA chain using the language model and the retriever\n",
        "chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "\n",
        "# Invoke the chain with a specific query to get a response\n",
        "reponse = chain.invoke(query)\n",
        "\n",
        "# Print the result of the response\n",
        "print(reponse[\"result\"])"
      ],
      "metadata": {
        "id": "aIXlHaWqgcza"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}