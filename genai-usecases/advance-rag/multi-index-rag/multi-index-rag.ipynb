{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxOWz9dpsYj"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1PBvBa2Gt9NtNIZGnfr-7uYui_fRSCyJI?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "#\t What is Multi-index?\n",
        "\n",
        "Multi-index is a RAG technique that involves using multiple separate indexes or databases to store and retrieve information. Instead of having a single, monolithic index, data is distributed across several specialized indexes. Each index can be optimized for different types of data or query patterns, allowing for more efficient and targeted.\n",
        "\n",
        "#  Multi-index RAG Implementation:\n",
        "\n",
        "1. **Retrieval from Multiple Indices:** We retrieve relevant documents from each vector store.\n",
        "2. **Context Combination:** We combine the retrieved documents from all sources into a single context.\n",
        "3. **Response Generation:** Using the combined context, we generate a final response to the original query.\n",
        "4. **Source Usage Explanation:** We generate an explanation of how information from different sources was used to answer the question.\n",
        "\n",
        "# 锔 Setup\n",
        "\n",
        "1. **[LLM](https://deepmind.google/technologies/gemini/pro/):** Google's free gemini-pro api endpoint ([Google's API Key](https://console.cloud.google.com/apis/credentials))\n",
        "2. **[Vector Store](https://www.pinecone.io/learn/vector-database/):** [ChromaDB](https://www.trychroma.com/)\n",
        "3. **[Embedding Model](https://qdrant.tech/articles/what-are-embeddings/):** [nomic-embed-text-v1.5](https://www.nomic.ai/blog/posts/nomic-embed-text-v1)\n",
        "4. **[LLM Framework](https://python.langchain.com/v0.2/docs/introduction/):** LangChain\n",
        "5. **[Huggingface API Key](https://huggingface.co/settings/tokens)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      },
      "source": [
        "# Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShxTNxM5gqtr",
        "outputId": "cf0b7fa6-d19c-4088-a63e-b413b3e1be1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m584.3/584.3 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m557.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \\\n",
        "     Sentence-transformers==3.0.1 \\\n",
        "     langchain==0.2.11 \\\n",
        "     langchain-google-genai==1.0.7 \\\n",
        "     langchain-chroma==0.1.2 \\\n",
        "     langchain-community==0.2.10 \\\n",
        "     langchain-huggingface==0.0.3 \\\n",
        "     einops==0.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      },
      "source": [
        "# Import related libraries related to Langchain, HuggingfaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RL-3LsYogoH5",
        "outputId": "7a165c5e-e784-4552-cbe0-75c2a1c9c072"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import (\n",
        "    ChatGoogleGenerativeAI,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        ")\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.document_loaders import WebBaseLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GT55z5AkhyOW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6UeDlrgqI2A"
      },
      "source": [
        "# Provide Google API Key. You can create Google API key at following link\n",
        "\n",
        "[Google Gemini-Pro API Creation Link](https://console.cloud.google.com/apis/credentials)\n",
        "\n",
        "[YouTube Video](https://www.youtube.com/watch?v=ZHX7zxvDfoc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "38b6366d-d2f8-4bf0-870d-4bcf143f5809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1dLpYboqeIK"
      },
      "source": [
        "# Provide Huggingface API Key. You can create Huggingface API key at following link\n",
        "\n",
        "[Huggingface API Creation Link](https://huggingface.co/settings/tokens)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6scBGZlhpG",
        "outputId": "7734f35a-3710-4d28-fd0e-5ca46aa757eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "路路路路路路路路路路\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"HF_TOKEN\"] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaB23jlCpQhd"
      },
      "source": [
        "# Step 1: Load and preprocess data code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azSUjUBugUz3"
      },
      "outputs": [],
      "source": [
        "def load_and_process_data(url):\n",
        "    # Load data from web\n",
        "    loader = WebBaseLoader(url)\n",
        "    data = loader.load()\n",
        "\n",
        "    # Split text into chunks (Experiment with Chunk Size and Chunk Overlap to get optimal chunking)\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JkZFt0wxYQj"
      },
      "source": [
        "# Step 2: Create multiple vector stores code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJBNL80WxUMe"
      },
      "outputs": [],
      "source": [
        "def create_vector_stores(chunks_list):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"nomic-ai/nomic-embed-text-v1.5\", model_kwargs = {'trust_remote_code': True})\n",
        "    vector_stores = []\n",
        "    for chunks in chunks_list:\n",
        "        vector_store = Chroma.from_documents(chunks, embeddings)\n",
        "        vector_stores.append(vector_store)\n",
        "    return vector_stores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGe14v-pxaj_"
      },
      "source": [
        "# Step 3: Mult-index-RAG related code\n",
        "\n",
        "1. **Retrieval from Multiple Indices:** We retrieve relevant documents from each vector store.\n",
        "2. **Context Combination:** We combine the retrieved documents from all sources into a single context.\n",
        "3. **Response Generation:** Using the combined context, we generate a final response to the original query.\n",
        "4. **Source Usage Explanation:** We generate an explanation of how information from different sources was used to answer the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gAL7mH5xaBM"
      },
      "outputs": [],
      "source": [
        "def multi_index_rag(query, vector_stores, llm):\n",
        "    # Retrieve documents from each vector store\n",
        "    all_docs = []\n",
        "    for i, vector_store in enumerate(vector_stores):\n",
        "        docs = vector_store.similarity_search(query, k=3)  # Increased k to 3\n",
        "        all_docs.extend([f\"Source {i+1}: \" + doc.page_content for doc in docs])\n",
        "\n",
        "    # Combine retrieved documents\n",
        "    context = \"\\n\\n\".join(all_docs)\n",
        "\n",
        "    # Generate response using combined context\n",
        "    response_prompt = ChatPromptTemplate.from_template(\n",
        "        \"You are an AI assistant tasked with answering questions based on the provided context. \"\n",
        "        \"The context contains information from multiple sources related to AI, machine learning, and NLP. \"\n",
        "        \"Please analyze the context carefully and provide a comprehensive answer to the question. \"\n",
        "        \"If the context doesn't contain enough information, use your general knowledge to supplement the answer, \"\n",
        "        \"but prioritize information from the context when available.\\n\\n\"\n",
        "        \"Context:\\n{context}\\n\\n\"\n",
        "        \"Question: {query}\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "    response_chain = response_prompt | llm\n",
        "    try:\n",
        "        response = response_chain.invoke({\"context\": context, \"query\": query})\n",
        "        final_answer = response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        final_answer = \"I apologize, but I encountered an error while generating the response.\"\n",
        "\n",
        "    # Generate explanation of source usage\n",
        "    explanation_prompt = ChatPromptTemplate.from_template(\n",
        "        \"Based on the answer you provided, explain how information from different sources was used to answer the question. \"\n",
        "        \"If general knowledge was used to supplement the answer, mention that as well.\\n\\n\"\n",
        "        \"Context: {context}\\n\"\n",
        "        \"Question: {query}\\n\"\n",
        "        \"Answer: {answer}\\n\"\n",
        "        \"Explanation:\"\n",
        "    )\n",
        "    explanation_chain = explanation_prompt | llm\n",
        "    try:\n",
        "        explanation = explanation_chain.invoke({\"context\": context, \"query\": query, \"answer\": final_answer})\n",
        "        source_explanation = explanation.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating explanation: {e}\")\n",
        "        source_explanation = \"Unable to generate explanation due to an error.\"\n",
        "\n",
        "    return {\n",
        "        \"final_answer\": final_answer,\n",
        "        \"source_explanation\": source_explanation,\n",
        "        \"retrieved_context\": context\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFcan5TryUca"
      },
      "source": [
        "# Step 4: Create chunk of web data to Chroma Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrAQVOhXw6QH",
        "outputId": "5d0620c9-867b-4f85-be18-e664602b7a59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.e55a7d4324f65581af5f483e830b80f34680e8ff.modeling_hf_nomic_bert:<All keys matched successfully>\n"
          ]
        }
      ],
      "source": [
        "# Initialize the gemini-pro language model with specified settings (Change temeprature  and other parameters as per your requirement)\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, safety_settings={\n",
        "          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        },)\n",
        "\n",
        "# Load and process data\n",
        "urls = [\n",
        "        \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
        "        \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
        "        \"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
        "      ]\n",
        "chunks_list = [load_and_process_data(url) for url in urls]\n",
        "\n",
        "# Create multiple vector stores\n",
        "vector_stores = create_vector_stores(chunks_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FShzccEf2Z2t"
      },
      "source": [
        "# Step 5: Run Multi-index RAG\n",
        "\n",
        "This implementation shows the key parts of Multi-index RAG:\n",
        "\n",
        "1. Use of multiple vector stores representing different sources or types of information\n",
        "2. Retrieval and combination of information from multiple sources\n",
        "3. Generation of a comprehensive response using the combined information\n",
        "Explanation of how different sources contributed to the answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXZ1eC7i2UKm",
        "outputId": "54aac285-aa4e-426c-fd24-1fe8e9b78d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How do AI, machine learning, and NLP relate to each other in modern applications?\n",
            "\n",
            "Final Answer:\n",
            "Machine learning is a subfield of AI that allows programs to improve their performance on a given task automatically. NLP, in turn, is a subfield of AI and computer science that focuses on providing computers the ability to process data encoded in natural language. In modern applications, AI, machine learning, and NLP are often used together to create systems that can understand and respond to human language. For example, AI-powered chatbots use machine learning to improve their ability to understand and respond to user queries, and NLP to process and interpret the user's input.\n",
            "\n",
            "Source Usage Explanation:\n",
            "The answer draws upon information from all three sources to provide a comprehensive explanation of the relationship between AI, machine learning, and NLP in modern applications.\n",
            "\n",
            "- Source 1 defines machine learning as a subfield of AI and NLP as a subfield of AI and computer science.\n",
            "- Source 2 provides a similar definition of machine learning and NLP.\n",
            "- Source 3 provides a more detailed definition of NLP and its relationship to information retrieval, knowledge representation, and computational linguistics.\n",
            "\n",
            "The answer combines these definitions to explain that AI, machine learning, and NLP are often used together in modern applications to create systems that can understand and respond to human language. For example, AI-powered chatbots use machine learning to improve their ability to understand and respond to user queries, and NLP to process and interpret the user's input.\n",
            "\n",
            "In addition to the information from the sources, the answer also draws upon general knowledge about the use of AI, machine learning, and NLP in modern applications. For example, the answer mentions that AI-powered chatbots are one example of how these technologies are used together.\n"
          ]
        }
      ],
      "source": [
        "# Example query\n",
        "query = \"How do AI, machine learning, and NLP relate to each other in modern applications?\"\n",
        "\n",
        "# Run Multi-index RAG\n",
        "result = multi_index_rag(query, vector_stores, llm)\n",
        "\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nFinal Answer:\")\n",
        "print(result[\"final_answer\"])\n",
        "print(\"\\nSource Usage Explanation:\")\n",
        "print(result[\"source_explanation\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
