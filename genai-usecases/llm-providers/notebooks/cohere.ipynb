{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1_CAy-bNBSxtRrdlHx8cNs5FMj4ko9Y9b?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "\n",
        "### Quick Start: Using Cohere LLM API with LangChain\n",
        "Easily connect Cohere with LangChain AI framework to build and run AI based applications.\n",
        "\n",
        "1. [Get Started with Cohere API](https://cohere.com/)\n",
        "2. [Learn LangChain Basics](https://python.langchain.com/docs/tutorials/)"
      ],
      "metadata": {
        "id": "HxxOWz9dpsYj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install required libraries"
      ],
      "metadata": {
        "id": "Y3axTI0sp5Hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-cohere>=0.4.5"
      ],
      "metadata": {
        "id": "ShxTNxM5gqtr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b8cde1d-cb5a-4661-d2d9-d879736a5552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import related libraries"
      ],
      "metadata": {
        "id": "9jJ1vqs-p_Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_cohere import ChatCohere\n",
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "RL-3LsYogoH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Provide a Cohere API key (Traial & Paid one)\n",
        "\n",
        "[Cohere API Key Creation Link](https://dashboard.cohere.com/api-keys)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F6UeDlrgqI2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobvrD3glfd4",
        "outputId": "3c1fac2a-ce55-4322-9916-6ed36b76d6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the Program\n",
        "1. [LangChain Doc](https://python.langchain.com/docs/integrations/chat/cohere/)\n",
        "2. [Cohere Models](https://docs.cohere.com/docs/models)\n",
        "\n",
        "#### Understanding the Parameters\n",
        "\n",
        "1. `temperature` → Think of this as creativity level.\n",
        "    - 0 = very strict, almost always gives the same answer.\n",
        "    - 1 = more random, tries different ways of answering.\n",
        "\n",
        "\n",
        "2. `top_p` → Imagine the model has a bag of possible next words.\n",
        "    - 0.9 means: only keep the words that together cover 90% of the likelihood, then pick from them.\n",
        "    - Helps cut off very unlikely/weird words.\n",
        "\n",
        "\n",
        "3. `top_k` → Another way of limiting choices.\n",
        "    - 40 means: only look at the 40 most likely words before picking one.\n",
        "    - Smaller k = safer, larger k = more variety.\n",
        "\n",
        "\n",
        "4. `max_output_tokens` → Maximum length of the answer.\n",
        "    - 512 = the model will stop once it has generated about 512 tokens (roughly 350-400 words)."
      ],
      "metadata": {
        "id": "NTwQumZZZpzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatCohere(\n",
        "    model=\"command-r-plus-08-2024\",\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "HHXlnTcxpi-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A. Basic LLM Call"
      ],
      "metadata": {
        "id": "w4UstBlvTXtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = llm.invoke(\"Provide me python code of sudoku\")\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "f4OZQ-kvqNl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### B. Prompt Template Example with System and Human Roles"
      ],
      "metadata": {
        "id": "-44mZEW_TnFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg =from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant that explains problems step-by-step.\"),\n",
        "    (\"human\", \"Solve this problem step by step: {problem}\"),\n",
        "])\n",
        "\n",
        "# Chaining Prompt Template with LLM\n",
        "chain = prompt | llm\n",
        "\n",
        "ai_msg = chain.invoke({\"problem\": \"Provide python code of sudoku\"})\n",
        "print(ai_msg.content)"
      ],
      "metadata": {
        "id": "6S4n9GOir1Us"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}